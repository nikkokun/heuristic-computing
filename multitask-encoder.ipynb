{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:34.099968Z",
     "start_time": "2019-01-13T03:43:33.088554Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicoroble/anaconda3/envs/tensorflow-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras, keras.layers as L, keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from keras.models import save_model\n",
    "import keras\n",
    "from keras.datasets import cifar10, fashion_mnist\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import UpSampling2D, Reshape, Dense,Activation, Flatten, Dropout, BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, InputLayer\n",
    "from keras.layers import Merge\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:34.328871Z",
     "start_time": "2019-01-13T03:43:34.101518Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bit_size = 32\n",
    "num_epochs = 500\n",
    "batch_size = 500\n",
    "autoencoder_id='multitaskfinal'\n",
    "model_path = 'fashion-models/'\n",
    "log_path = F'fashion-logs/autoencoder-{autoencoder_id}'\n",
    "tensorboard = TensorBoard(log_dir=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:34.344358Z",
     "start_time": "2019-01-13T03:43:34.330706Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yagmail\n",
    "email = os.environ.get('GMAIL')\n",
    "pswd = os.environ.get('GMAILPASS')\n",
    "yag = yagmail.SMTP(email, pswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:34.349496Z",
     "start_time": "2019-01-13T03:43:34.345404Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSaveCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        super(ModelSaveCallback, self).__init__()\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model_filename = self.file_name.format(epoch)\n",
    "        save_model(self.model, model_filename)\n",
    "        print(\"Model saved in {}\".format(model_filename))\n",
    "\n",
    "\n",
    "# !!! remember to clear session/graph if you rebuild your graph to avoid out-of-memory errors !!!\n",
    "def reset_tf_session():\n",
    "    curr_session = tf.get_default_session()\n",
    "    # close current session\n",
    "    if curr_session is not None:\n",
    "        curr_session.close()\n",
    "    # reset graph\n",
    "    K.clear_session()\n",
    "    # create new session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    s = tf.InteractiveSession(config=config)\n",
    "    K.set_session(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:34.626326Z",
     "start_time": "2019-01-13T03:43:34.350606Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:34.629846Z",
     "start_time": "2019-01-13T03:43:34.627455Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: (60000, 28, 28) (60000,)\n",
      "Test samples: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train samples:\", x_train.shape, y_train.shape)\n",
    "print(\"Test samples:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:34.637539Z",
     "start_time": "2019-01-13T03:43:34.631713Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "cifar10_classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "                   \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.156605Z",
     "start_time": "2019-01-13T03:43:34.639520Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAADyCAYAAAA89ja4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzsnXeAHFeZ7W91dZruyUkzoxlplEaSrWhZtuQoHHDC4AVsYB/JS15YkhcesAs8Mm+JJpicvBgMxsCCMU44gG3ZliXLsnJOo8l5OndXvz943PPdctdoZMlWjXx+f309Xbequm6oqrnnO9cqFouKEEIIIYQQQoh/CJzsEyCEEEIIIYQQYsIXNUIIIYQQQgjxGXxRI4QQQgghhBCfwRc1QgghhBBCCPEZfFEjhBBCCCGEEJ/BFzVCCCGEEEII8Rl8USOEEEIIIYQQn8EXNUIIIYQQQgjxGXxRI4QQQgghhBCfEXwhD3Zp4NripDa0LMTFyRXxIrB0oY4zDTEdl23rMrZzhoZ1PPiqpTruX47jxzvxXjv93kGjfL6qTMcDp0d13PQ3bFfYsuOYzv25cq9zm3X0rSZm0nUlmUy9BWzzs1Moudnon+d4HiaZCes4HET5OTX9JbcfSseMz+c37NbxoXSNjveflSp9QGuSl/M5tNWTVlceWEEMCcV8flJl7A7U1d7XNxrfzfzE2mM6/u6vrzI+z33/Y8dUfrLt67lwvHV1IuuJeOO3PmXgHks8xoyDty3Wcd2t5vgVv/3xYzvmqiXGx8MfQp+Y+ZbDOi4Mj2Aj2Y9OYB9y4+u6IgZTta5yl6zQ8bzPb9XxvdsXGtst+OI4PvQO6NAqw7Pdjve36vgdl91rlP/5jy7TcdPXHn3uJ3wCmKp19WJkMnXFGTVCCCGEEEII8Rkv6IzapDnGmYn9n11tfG45+4iOex/AjMl//cuPdWwr8xi7Mk06nhdZr+Mme1THWfFe+/i/zDXKv7pii45/NHymjn92AWYIzptdoeONP8d/TJVSqvFbJ/c/MCcEWW8es2tWyGxyxQz+W3vgU+fo+EPtv9XxoyPmtW6O4j+/uxMNOi6zczquCKZ1HA6YM0N/OIRr/+WFt+n4I699J8rfipmcgPiPmlJKOSmPmTe/M4kZz4lm0Qbehn4WeVWPjq9q2azj71evN8p88qWX6/jJW/Gf/by4pG/+X3fr+BOxm4zyP1x1oY4f3ID/gJ72BfTx/CHMCrj/+2+FMPtazKN9HO9MPSHPBcs2Z3xlfwvOmqnj+Y29On76pTOMMgv/UqVjYxZMYFdW6njHa8wZuXIbZZy5bfjiydL7ImSqIZUZ85Ycwt9H8bxww5nmjFjm1yEdf/OJi3Q8vQWqqE+0/0bHt3Sa6o/RxVkdV909S8cVb8bzQr6re3I/gBABZ9QIIYQQQgghxGfwRY0QQgghhBBCfIY/pY8eBOJxHR+5BXKQ/Hja2O7AkTodhyP4+wd/eb2Of/H6G40yl9dkdDziYKo6V3R07Ijtc2X7jPLfHzpLx7//3hodF1bh3B4/DGnL9H/CdLxSSu1etVzHc1//lJryeMggi5lMiY3/TqEMZaaHhnR8dtVeY7uOMOQDc6P1Oh7Ml+v4inLI8RpsU+b2vy0k/fblIRGys6XlcE4yaf5hsuYifkNKUD1MQ/reBXnjW/7tDqP4iui3dNxbgIx3fRIyj0fT040yP5rxMD58+GFVCtnfvj6wwvhuddUeHb/j8gd1vP9i1Ptnt1yp4+mvhARZKaWKOchRnotRyoueE2jsJLHr64zPzqwWHGbdM0c/FzengpQ1hza5ubNZxy1/MeWS1X/C/1cHzi29K+f36J/Tvmd+1xXBd9OTMNEyRMNFebcjxN9EHmoyPs/Od+p4/CYYgAzPQV+y32TK9EcK0OOv7MDz3cIKPG9cGtuv4+/lLjDKh49AOploQPzX9b/T8WUty7x/BCEecEaNEEIIIYQQQnwGX9QIIYQQQgghxGfwRY0QQgghhBBCfMaUylHb8V+LdFzsg4beSpoafkuK7ReO6dDZixym99/wb0aZ3jPwzpptEjbeIi3CSuA44SHzmOUHkSMxvBp5MaEQTqZQwDEO9mPZAKWUaqqHNfLgHR06rn3ZTjUlmWRui30afmt0Hq5BLIBcttMjh40y61Kzdbw3BbvdRXHo0telkcPYk4edtVJKnV+NaxoNoK4OX4E2NX/f6Tj9p8y8J+P3yIVhi8/fwrAngsnkaL3zvf+j44wTMr77wwjyKMtt1E9NMKHjfRlzweuDUdRJQVy2eADtY2sWOTP1IfRXpZTqzSGH8Hdp5K8l8kg+vXHprTp+28/eZJSf96YN+GC9SP4vNZkcSnefFGWkhbxXOwlObzE+b/1P5IHM+2+0jVC3WErjLci7ylWZOVCzfids6j3O61TIQ5soNzLfiSUnGmqiOi7/9UZju/WLkUcaezeuT2QY17SvE3Uw91fmovE9V6MfF7Z63F9kX/H5uEbI0NdnGp/7l2AMS1+CPleOlGf1jVteYZQpioHHEt1019B8Hd9RQF6a5UrjjIvyw7FaHc9/+l06nrka+e7W2qcVIZPhRfLkQgghhBBCCCFTB76oEUIIIYQQQojP8L30UUrjisJmPTBul/y7UsqQy2R7YyjTBqv8rjazSHA3rFkrnwnrWKq/0vU4TqYNkjmllCrMw1y5lcf7bz6L82xsGNVxIoNjKKXU0DjOc0kzJDDDyyeQ4E1Bdn37bOPzly79pY4PZGG5/tfxBTqeFekzytTa4zoOxSDL+d7u83Q8shPSg89fjWMopdRwAcs8/HUMx/ngOffouG3NgI6/c3CN+SMuFlJMR8iCfC7T8pI+HvjUOTqeHvqxjjenzE5SFYSNfn8OMuLaIOpjv6hDN7VCUhdSiP80slTHc6K9RpmxAiRgIaFpjgch7ZJt5eWLNhnlt4lYWvUbklXnFJN2eSyNMaGcTZSRbSP1Ciw7Mv4WyBgty2zfSysP6PjKH8Fe/yeffrmO59yMfjy6yLTnD/7FtMoudV6nAlbIHPdlm7Q75uh47dLf6PgyZVp6t398rY6v34HrXm1DVvW1uQuxX9dSCE+v+Y6Or42u0bGTxv3REtJkOvUTP7L766t0vPqs7cZ3vfej/ccO4L6XWo4+4gxEjDJKDDXFMD7I26AVxbjZ3IQlhJRS6lWtkCjftAkSyVcthPz+z4tP03GjqbwkxBPOqBFCCCGEEEKIz+CLGiGEEEIIIYT4DN9LH/vOFrKNACQ5xaCcpzbLGG48WUg4nGHITtxyyUIrJCjZdrEDaaDmiA9p0/UxnxMaSXFutpgqLzh4L87lzPJ1lXDOG0pDBrnzHZDpdbxT+RovaV2wdbqOG2cNGGVu6ztTx6uq9+o4JtwY+/IVRhnpRhgJwKFzZhWkCE9VwulxWwrHV0qpXBHXvjWMMgczkEtuTsDV7vrWR4zy33nZtTqO3vGEjq2waF+ZjPIbUtokqV4JuWHawW8IWaY8riA6Q0BI35IOJCQry/cZZX4pnCL/d90uHX91EM6dcl9RUZ9KKeUUcUw7gH6ZKqANjIvjnxaDbFgppXYsgQTF2QR5jBUSbTVzikkfJVI6OIF7X/5iOGpG1u/WcfYdgzqeEYcj59ZHUH9KKbU9j3H6rGv2Y79lwk0yjT7ReZmpp4ueDvlt+29xTGezkDQFzDHT0ORNFYnkBDpCaxQS4ll3vVXHHepJzzKPjs3Vcb4orw/6eqHfHHMX3/FeHc8vlnaeKzpT5HpOUUZfB9le5nW4B7klxdMrkS4RtTE2bu+Hu25qe7WOK6CEVUopVbsdfS44jNjuxTGl2+hUInYYz1NPtZn3+M9c9wvEW67SccdH8FwxcFaZUSaHRy0lb0PhBOqkEBby/T5TUnzLjMt0fNOHfqjjn/YgJSOzrlYRcqxwRo0QQgghhBBCfAZf1AghhBBCCCHEZ/he+ti/Wqw8KGWI0mAv6JJp5ITcpiA2FKZvxTKzjCWljPnSl0VKKt1ryjpRsQC3WOS6LAa5gVSTxMtMaVwshLn2/iSkj6sXQy5mClimDgNrYJvUGN9vfLepCxLDuXG4wi2JHdTx7nSTUSYp5HlSxniBkNa9/SV/1fGerLkIc38OUso9aSyYbSvUoVzQeaAAh0OllBp+KyRgTXfg736UO06G6ijcHOVvTbsWvJayRHmtpAOkjJVSanoEEptvDmFR0qVl0OhstbBY8pakKWGR8suRPKQq9SHIxGQbkOellFKdl0Jq0mwaQp66eLiPBmIYVzLnnSZLqHQNxrz+6+E0O9YnnACFLCsybA6AQqmsYqLvOP+EUWtI1G241yw/+weQPe97C2SVbZvFRm7Z4FSROwqKhQnkp13dOl46B7+t844OY7ugjX3cuw91atu4PomfwC3VCpjXaXE7xtbMFB2zTjrH6fDb/RLU4Y0L/6jjQzlTTtcexj2xUMT/1Re04e8NK6Qk3exXtpSri//L5xSO/+mec40y/1qPe+flt35ogl9xcmn58qOe3/3wnGt0XP5x3Cs+fOfvdPxoYp5R5kAa94qISLORkuJWcT97d625EP0DKTxLfPU11+k4sOuQjttGvc+ZEC84o0YIIYQQQgghPoMvaoQQQgghhBDiM3wvfZzWiqnmnoMejjku6aMjflUgh+8M6WLOpV2Ur6xyf0KpUhT7tfJmeSmxtIM4UDSEKfRMHlPopzX0GOWl02NWSC/bYvj9w66FS91uXicbL1lPP4z/VKPru5pyLEB5ywYshr13PhZOXi3cIJVSam4UEqF9GeyxKwv3q93JaTqWiyMrpVRGNBAprZNyut4s5JHjYtFlpZSaU9uv44Sa+pxdt1/HUlbqZjAPWyx5DVOizNwyc8HqigBklVtSkDgujkIOsmFsho7by8w23ZOt1HFcSOoCwuo15wgnLktIpZVSqRVJVYqpKlOdFF4LW8/FdY4eGDaKhO6B02PP1+BIF+hH3V68dKeOf+tyTGurg1OjdGxteif6VL4T5Qc+t9ooL2V/hegsNSlO4UXLd/Vj/KuOp4zvaoRUWUrmxzJwP81G0T/zWdMtc08/7iOtAUjoTrVr+LxynLLbskOQlW8WKyrvSTYY2w3FMObKe9UTiTmqFLZlyoMjYjyU0nXp4Ftmm067/zO2RMdhl8R5qmA9CjfT8s9hwfjPht6s46JLJpqrRD9xbJE+I6r6adGVbpt2sVE+MoRrX7X+MR0bveoUHrNOOBPIi62QcNjOZVUpMleu1HHkznUn9tQ8HM69tlFKqZ53nKXjxm8fmwSWM2qEEEIIIYQQ4jP4okYIIYQQQgghPoMvaoQQQgghhBDiM3yfozYwLOy+w0J/nYPW10q73jdtoWfN4jsnhL8XQy6NuShjiTLKQ6LtlDmlv1BKFYtC/y1yCPIF7Lc2bObO7BlCTkJBbPd4XztO5UxTvx6+y185al65MS2LkI+3f6jGKLKyCblK3X1VOl67DRr8tZapx4/XIEfj3FbkrzWEkQ8T8bCSV0qpvMhpyivEu3PId6sMmnkhkhlx5A3uakIuXL67p9TmvsSuRO7XjDByh/Zl0MbkEglKmXb9zySRVxEUuRPSKl8ppdrDyOfryqHuu/Oo6wurd+j4zv7FRvn6CLIAHdGv3l6zQcc3j6BM0kGejlJKXTAbuVeH1YsEj9wHZ9N2HQebzSUvEq9GfmjzIyL/rwx9+pY48soCKXPMLb4R/fD2pMxEPVLyXMq6Xfb+UeSBhkc8Bl13XlDx1M3xCAhL/VTWXCZjcAx5S3UV6B+D4l4Zj2NZhbGsmU+YHEQ+dCCMfTtpcT2ZP/O8UtaL+s2I5HeZ+6uUacnfGkYeaE1wctnRMuc4I8Zvme+WKpjtqzWG54qZP8DYrD43qUOeFGTOklJm3tKe1+KecO/VX9HxN/vXGGUSeWwn7zXy+uRFfXyyVazNo5S6Ye+rcfxbxBfMS3tuyPHelU/olZd25HdYdqZK5Lt3rTZzots/vvboxxfHtGzzucYrL22ibYJX4FlovG+Ve/MJ4YwaIYQQQgghhPgMvqgRQgghhBBCiM/wpfRx53dgYxkNQYLmdEMeIy1T7VZTRphNYBrcSuBdVJZRBddUqvwglXJe7rQTuPMWPVSRQRtfSKtxpZRyxP7S4vyXtHfquPfjo0aZobu8z+GkIKb1g0ISOJSE9CadMiUK0yL4TdXVQsbTCav9QNy0D04ehsTnvuQCHb9/+f063pxtQXnLrKyZZZiCHspBRrQ3WS+2wbR5wGV5XBNEexs5b4WO47+ZOtLH3FLISeuCkBF25XDdbVcjt8V1bAyj3qLCAvrVFVuMMj8fXarjkbxoByHISc6O7tfxvBbYtCul1H1ji3Qsrfo/euSlOl5WAYmmW/roeHbgUxgPW+NgO+z5t3/GXOojshPSDqEaVqKpq453PuF5SG8heGmCKdeSKmlI9ZrWll5SwU1gCfq+lHVOVaSdc1UZrke2YMpuHAf1O5ZGey+ksN14EffKYsH8f+zSDvSX7Izp+GLnHnEy3tbY5PjJXj6i4zkR3DekJFEppRpDGGcH87jvyXtS1EKHjQVcS9EojLPlNtqUIyR888rM+9aCMD67Lcb9ipcUTimlOm7YqOM3/vXfdRztN58r0vW4VpkKtP9CBLFQj6prmjuM8tPWiec7hec2yh0nxsvq3q7DclyFgUHlxc4fwIZ/biWeH/ZsxthWbDJliHu/CCnk7I94yCDFuDeR1HHozdiXyKpRA+e52qRYCaXj11i+Qd3quWsNZ9QIIYQQQgghxGfwRY0QQgghhBBCfIYv57Xr12H+8N0fvlvHN4Yv0nFiGxzklreafm5P7JqlY+n0KPU5xYBrpfO8kHp4mY5JZ0iXdFKWCYjtxjOQ+rVXY/o2VTAlgMMjkOBdftpWHV9YCdelH790jeuMvKeDTzZjZ8/UcV28V8eHXdLHTSOYnrZFnVg56bjjkklFRUUKGVBbGHLFWw9Bktj7NGSYSilVswjSx5e07EL5Mrg5NoeGdSzlgEqZzoZd5+H4c3+jpgxHzoMMMSA6hpTHuLlxN/rff3b8Scd3DS/R8f3BcaPM+TE4Su4RrprSQfLR1GwdH8yakjy5XTgA+cEjnejjl9Zs1vGWVKtRfmmFcBWNo605icm5pk1FpEOVlGzk90PytuBjLrGig8/5ztJOjcYxXJKoybhgSVa8faPxef8PER94GdpmuwPpbN+yuCyizv+XdTp+4uvH5qLlR+wZrSX/XnDMe82MeoxToQBkVaN9kMaVl6Mfj/YK52SlVF8K1zHeUKFja6fYyMPBl5wYpBxfOuNuT7UY251ejmebhJB1p4sYF3NF78e4sJBSFsT/5R8ZnafjNVXbjDLv3flaHUe693vu21cETHmwlBsmrlqm4w997uc6/vGR88wiWVzfVArS4UgQ+8rkcK3/uf0Zo/zNtefouOP2yZ448bp3SLmjlO0rpVT2R+g/b2v4q45/+NS5Ov7nCx/R8S3rzPvDFZc8qePuh/HMkXgN2sBE98Ej/466TjfgXJwWjLvtTebz+fgvzb59LHBGjRBCCCGEEEJ8Bl/UCCGEEEIIIcRn+FL6WPtjuLDc9hcsxPqeux7U8e2VZ+j4qcOmZKQo3K8CQqJoLHI9kRmc13fSCMslx7PHhdyoWsj2hMQhKxazfLSr3Sj/9qUP63imkEJ8v2O22MpcgNjPdJ+N6/HvbXCLuz++wNju0lrIPO8dxGKFAwVMRxdd0h8rh/8vBMrh3CSlITVRuIV2uRYn798LN6Hb9qB9Vc2EE9eqhXBAk4tAK2UuhhlpNaV+U4VkB9zBpHRmQaRLx187cKlRZmQjXDEbF47puCKI6f6Lyg4YZe5MzMV2Nuqkz4Hk6p8r4RR5KG+28e/2vkTHp5fDSWusE9LWBcvgUrYtJVzslFKzw5DdZs++WMfB+9erU5XJyBDzhya3/LcVQZ8qZuFidaxSRzcrKsx2crgN43mkH/1r15shlY50m2NuYxhtMFs+Rdw9Ldf/RoWscOQMSLTLw0LyljHl4l0jcD+tL4eE18pi32Eh17LS5jEHRoWEdDokXoZA0su6+MWM5ZUT4b0wr5dj5vg4rvtdI5COS+dhpZTaEsWzTdRC/5tI7iiRMn3JzCjSBKS8XCmlPjjrHh1/W5nOhr5lgvZasQn3gI9//406jgyZdZOpFU6PosslokLaJi77zT2QvymlVLTTvI4veo7TOTawDM+DR84100+s27C/vz2GZ4nTvo7nlyUxPEs82Aapr1JKPXQLnCKvftPfdNzxF5R/pZAdp13y77N+j+fG4DjGVzuE7Q5vMKWO+bNxz6z9sTomOKNGCCGEEEIIIT6DL2qEEEIIIYQQ4jP4okYIIYQQQgghPsOXOWqS/AHYa/96YZOOF66H7vgrs0xf9Cvvf6+O7UFotHNCdyzt35VSSgkJbTHsoaedQGbrRKCRtjzk0glxAjct+oXx3e3DZ+r4/sWmDfVUJFcrci8KsNtuio4a2y2IwAL16UibjgNZoRd35agVg3KdBbGdiKX9sTvnsBgT+Rsin3FsN3TQn1VX6vi1s818pq5slY7PbkWuzdFNzf3D7LY+HQ8XYjpuDyE/cu/6NqOMTK9pEPlmg1m01y/3XWCUObsCuX7LRF3vTqMvb8zgut89stgoXxnEcVpCsCWv3I56G74C+R61QdN2Px5ALl73KuRbtd6vXnyInAErbOY9SUt/adXvZHD9nksujhc/+8TVxufUVWhc0QHsKzWK83r1yx82yjw5hCVAGr6LvGZ10zGdim/oW45rUG8jnyGVMXNfaiqSOh5MYmy1xJiZzeO62Snz/7F5YTGebMR3Zo7aseeUnPLIAdDxWLJggusWWIT87Fef/pSOdyeQAz033meUqQyk1NGQ+WoF1//e5XdpEcvctQ2JdqPMJxthd/5/3vWmox7f7yQW4vpGL8T9rb+zytwwKOpOLtcknj+MZZxcPgWZxlNsGQt5v7BL5zoqNUG+8iTHkO73C6v7OpSZcTfa/vCSnFFGCe+J6/8NNvw/3r1axx/d+Dodz19q5r6PXYhnidv+KJZp6MDzw2cKoi8dxDirlFLRhPApOBM2/NU3iVHUMq/L1V/+i47vVpXqWOCMGiGEEEIIIYT4DL6oEUIIIYQQQojP8L300cvic9sKTCt++bGXGkXqGyGvG+6rUyVxz8pKJY+UzQk5ndzGKrikP+KVtyimTIvSyl3IWV7/u3cbxefc8Fjp85yiVDSNlfx7LJA1PkupxiNds3ScrxTyRPdO3Nf+/1NpwyZeykyljFIppQohUT9RHKcQn9xUfd6BDOCSGiwvcLNqK7W5L6mJQD6VLkJaNc1G/cSOmNdtfCYkcQkho5lVBjnJReVbjTILw9jf4byQdoXQPpqCiK+tWWeUv6kH9vwzgoOqFFvTsOSPBEyJRNTC58zpR5cRndKI8bMoJY1qQlW3Rlr1P2vXYn9WEG1DymKyl0He3b/M/B/hzDvQHvdDua6aatA2rqs220ZfFm1j/wTn7Su8JHNKqew0tNWsGGNCIbPM8DikynUVkOqMC8l+WRj7GouZWnx5e0s1UuI4WayAlL3h/lLMZUts/XfyF2EZkQu+BnluaxhjWSKPfrWgzBTQjzqQXNkK9Sjvm14W/EopFRLyq4K4k44XIRcfy0WNMrvyuB9U7Tu+ZTieVyZp/37wZfjOHkXfsfLmGFSU+wjjWgej6EvhiEuCJ/ivpbfr+FNvvl7HNT8VsuyAlJj7XCop7xeTXI7FroSkb+zihTruXoVrXb1jcofPfWJYx/VZU6pfHsH95ns7IF2Uy2GtOHOXjjvHTZnruzog7328CUtgbejGchiNFVh6qStqShUdIYedXoVlnTIfxngsn/eVUirp4DfYNTXqWOCMGiGEEEIIIYT4DL6oEUIIIYQQQojP8L/0cRLOMYdXjRuf65diennko5iKDBzGtLdjzqSaSDcfqRoR053FgHle0kWymMbxG+I4/q6NkMbNfS5Sx+N0WnshuahNTDtnMM17JGVOQbfVou6yeTRHW6z2Xgi4pB126d9dKJb+v8OzZKryO+H6WIyisuMRyFnccrpUAdKQpiCm5wPReTp20mnlZ5ZVHdaxlM4khVS38oApzcidh7Z8KA+nxoEcXB9/PnCOUeYDjXA62p+v13HGwTV0xDHXpSBDUEqp4Sz6bCxQWnby16EOHb+qwXToTDiQFS1pw282vSFPLbykh4ElcJ3b/q+mlMMeQxtoegz9oOoxuO7mOyfnayqPmblypY6T70ZfKQwbHoPq0h/A0fE7f7lUx11CnrRkiSnRGhJtQ6mpL2s9//SdOt4+OE3HwYApXUym0abz8dIy7pEEJHNF13hZzKKu83P8PU4dD7IfKKWUEs51lriXOlkxrkwgR5uM/OvwR83x70NvgiN1Tw73vq3JFh3Pj3XreKRgOj4HhIW07WUnLXBc98BkEQ86USGDnB6G612yYD4MyX1E+n3cryb5/FO+C/eaYhCx+3biiOYiL6OdlWkU+Hto3Dz+B3ZB7jhzj0e/8rvc0YNg+wwd73hPi/FdaCbupue07cMXKdw7Du9v1vFou3nh33o6XBsf6se9PBxAez24c5pRpl84dy+fC+dtKeMdyWAM7B0073ffScOdemFDj46lZHxeJRxYZ8TRX5RS6vEjcBzOFNBwpNxxyz7zOpWHINfMLzi2NBnOqBFCCCGEEEKIz+CLGiGEEEIIIYT4DP9LH72YwD0n0ANHpUCgUcd5uW5o0SWHk1ZYhvSxtGxuItdHKYOU7l21m70leJ4ORpN0NvIbK8sxBf7gMCRXA2lT2jEiJHA3LLxPx98MrtHx0ECFUaaYxDUtjKH8mINpbzkF7Zb+KCFbLZpryWpqy+BCVxHwlgfZwi+vuBjSR7XuGc8yfqA5BBmalNR0hFA/8f2mpHjxTCxevTmFqfvpEezrsvItRpms0JDUBSCR2Gos0oo2fkaZkE4opR6wIIVIiraSE01i32gtznma6Wb4YqTolB4nnE3bdTz/h4uM77K16Ds9KyD16V0BiYcThvwlMN2URAU3o920/xpSrsaP7dXx+ieFNLjSlJE9PgTH1+h0tDvbRtvsL5iC1agNmcqImpqiJpLeAAAgAElEQVQUVy/VcX8aUptEGnXgOOb/U+W6y4kMtrOCuFbpIdSnlXO524l41kzIfqyVWGy+6PPxy8DjHvksqaL4/JzupOI4/W9fpePL3gnp1kcrv2sU+XEvHOnGc5CsLq3qLHkIt8xeytKlg2NOSLpDlnj+scxnoVBRSJ/FM450oOtKm+kICSGX7F59bAvzvqBM9AwYx3i09JWmE/E/SBfMx18pMZayu8og7ilxEYdc13p/Eg7jB5cj3aPsspKH9z2BpcK18bP4rW3RLmO7nhHcjB8+gNSFqHDIPH0WZPPSkVsppX645Vwdy/E+l0X9XHimWYf7RnGthzKQwEs364Tob2e37zfKbzgCd8eeJM6/LIRz3jYMuWUqZz4oFoSz+2DSXAxbkzXH3bOqcA5/tmeoY4EzaoQQQgghhBDiM/iiRgghhBBCCCE+Y+pKHycihqnIoJCD5MRMdTHoEj/IxaylksLYxmMhbKU8X3nlgteJ6Yhrn72h1w5K/92HWCFMabeHIOMps2eX2lwppVRWXDjpinVd+1M6Lnc5kz00CDnchoOQ4A0XMAWeFNProTGzcgpyAVixsGUohmnvNXVwYKu2MZ3uRsr2xmZBblG+rtTWJ5dgE6byowE4Mg3my0ttrgpxc7p/Yayr5Hb9ovwXjlxhfPe6RribhhQ64LQgxGpbMnBHyhXNIUnKdboLaB+5JZDH9W+CvLlivinJGxMLxraUjep4lzqFcLvBChlQcDqubeer2nVct9mUiEaO4Hq270R7LxyGZGUi17vAstN0vP+6Jh3nHkGbu+Xab+n4k7OxELBSSm26dYmOaytx/NG1qNuRM8yxsDmK+uxxX4MpwoGrMGatCKPtdocgx5FuuEopFYiWXmA5IO9pQdSVk3HdnMrQPvb1QEIUvBp9ZaYPxy9PJnmPtBcK6W0M94fETIxfw3Mgp0stM8eS/1zxJx3X2j/T8UOjkPZ/v/tCo0xtGG25IYw+JqWHuRyOWR/CAu9KKdUWQhpH2kOnLxfCTjrmovRybJbujhlhcei4n2UE4zN8/PxR9HbBdBKQSW/5OZw4473yIdB714GCkNCKsSWYRHmrYO6gZyWuffUeeW6mnH+qcPgyyDfnVIq0h85mY7uoWBA8EMD1SSZxPXZlGnQcDpv3kfIYnu/kWFcMYV8be6YbZdJZ9IXWWqReDIrUmiEhSXTLLWvL0S+rwjj+jDj626ZBHDNXMMfQugq0r27hKNlaj3MZqTBlzBtGIXccmeshl/SAM2qEEEIIIYQQ4jP4okYIIYQQQgghPoMvaoQQQgghhBDiM6ZujtoE+uTx04Qlfx46c8NS3/WK6ohcpYDIaytKfb/Qcrtz3Ky8+C6E78aEfbJaOqpOZayFyEWLW3/TcSIPrXJFyMyNWR7G9f3iULuOG6LQ88vV5pVSam4c+W/LFh3W8aE0Mv9ay6EVPtCE9qCUUva49LdGXBhDd7i/eb6OP9B2wCgv86Yk6Wq0gdJZXyeX3CzkDsUDqIcxC9d3S1b0F9fPlNb5fxpZpuPGMNr192c8ZZTZLJaq6M4jx2xnGjr3f6par+Ooy/L4zwo28nszqMdFLciX27xvro5rXUspDDvIAZLWyi8WiiJXY2QJcpuci8y8y8QYNPOhfchNKNpCp98qbKo3m32y+REcR6QpqPMu3KzjT1/xGlFit1E+HEahqMivSog0obsSC2URlSogTyEQsdVUJL4EORF5sZTFgrpeHW/sMvMzpG31nCaMhWmRU9o1hLyJ4DSzrpuqkQfVP46cjkTr1H0c+AeHbz9dx8ubTQv86tBBHQdFI728CksRGDnHBTOP5A/9GPMcsV1b2ZCOm6PmQhHS8j3qst7XxxT1Pl4w+9Uhkc0eC6D/Smv4/jzyGUfy5jlHhM28HKdl+XzR7DsNNvqyE/N+zvIzwdntOv6/H/yBjp9OI08oapm5UnJphKiFax0W1yom7puO6yHy/hGMT03iWj/062PLR/ILrd9Bv+g+gn5VvNy8j3bUY6ySOWK19WhHZ1QjJ/6xQSzFopRS/UmUCYg6mT8N+dGyHSulVG0Y+55XhiVGZL21h/t17M7dlM8iLUH032QR251XCZ8Cd/kf7MeyG3Om4TgZseRDTaU57p5dheendVfQnp8QQgghhBBCpjR8USOEEEIIIYQQnzF1tQ4TWPL2LcXPygwIq35ZxFRYKRXCFH9Q2IJmLXmJxA6e5Whb2uI2mcaUqVx1fUKM9QF8bI/rIjUdEoyEsBLOCWlHZdi0PA5ZkF1khWVwyMK1itjmtPdADlPlPZlKVYppEUgP3nLOX43v9iXrdfxY50wcP4Nz3nEEtuIXd5iN5daSR1QqU+Nvi/BUM2Q1MQvyBbn8QELY46frTUvbeSHUXU7IZWT85v2mPf+FtZAPZBxc34VlkDX8bgRW7W6r6JYySIl6cqjrM6shY9pSgPQxWfQe0mqDCfEp7rndlCAg5EqO2T7teliuq2pcs9M+DpnwwTeYS2aEzoQcrmwppCD2HZBB1v3HBh1nL19plN/1L6hbuwzt5K+7UDdzd5iyWEliEON0eVRIa0RzOJgRv0splRJ241Z0at7KFjVAwlst+peUpu2INBhl4sKeP2xjuwEhIYpGIOOayH5dWsMrt43/FKHrg7Bf/9iiX+n4/qGFpTZXSpmW9rcPnKljKT+N2+YyCBUhyKpnRiFZlZLGgus5wBbPDOU2yoeFxCsgzmWwYIrmpRRSjrO9WfRrWb91ITnGmRLNkTxk4LJ9hV2yshbRpoJjU7NN5Pfu1/EnPvkWHVfvQEpFMeDdL4o2fncgLa6PjTKBpNk+cg3of+sbMTaVq8cnd9I+I3U+lp3IluN3z3vzM+Z2i7Bc0vhKSAo752Cbp+ra8aFgXvdAHP2nulosq9CHVI3RMVM+GgigX1n7xb0DjwUqKLIgqneYMkRb1J3VPaDjYhJjcDGN+1DxLjN95pElv9Xx+7vOVKWQ0nyllPrqXy/TccdPxHPwtSWLG0zNXkgIIYQQQgghpzB8USOEEEIIIYQQnzE19SJHId0sVo93hBujLWSEjjn9agmnx0IeEgMri3fZopBHKrecRH4Ux8lmcIlnN8Mdxp4/V0kKO4QLmiWdJt0aTf8y3oLfWhD/A7CFfeD06LBRJid+38LKbh0P5yDTcEszJNK9S9KVxhS8Wx4pJUbvXPAwthPSuk0jcFrbk4NcQinTgWi4ALlDttrfMtXRGUd3xpNuSKMzzOFhQEgW+rOQ6DSHIU/81sw/GGU2ZOBatjWNa9pgQ5q6oAryr74C5LNKKfXTnnN1PDuG/mPIjYQhU6JoyjUl0tXLPg1yjcLWnaU29wdeMmhngnEhi9/ZuwbykYEzUSa+3yzS9DNIrMJD6Ltv+smvdfyf51yj46oNpqwjEEWfaqhF3VZescf7PAXBAexvtBrnEh3Db94y0myUidr4nVbcu979zBmV0Oo8MQw3NCnBm1Zujj/SdbY6DEmPI+RJeQflx7KmY1lTHPUzKqT5NTNNedBUYWwRZEx1Nq7VRTXbjO2GC7inJB20lzLRjqQksMI2HWSl9NAYS4S8MWZ536sqA+gjfXnca6RcMuRyva0Rcm0p15xdDrdPWd4pmv97l79ZIu/Po3nTaXJHDlKyGXcLGfKHS+7q5DHBc5Ic3//wxa/oWN6PQq66ku6b0hVZukPKlIy0yy1TOljfPo70ip/cNlOVxHI9Q/oszSVy5zodJz67Wse7v2DK3lsexrVvuBUuv3Xj5rileQ6/s/Hom/wd9zX9x59ts66sGuFsLJyRA1Xol/kxpAOoS0wH2RVvf5eO5TBRsw2/2d5jlgl+COfQs8p8zjkanFEjhBBCCCGEEJ/BFzVCCCGEEEII8RlTS/ro4XRm19Uam8VaMP2YPAyJljFT7VrN1xayxkJOvL9Kh5qYKONau9LKS4kk/l4UEsuAkEj0rDGdvOqF9NESbkQTrOvtO3KVOG8pE9kzAhnAQMZ029tS9aSONw616nh+JRZRzBW9/58gpR5lwqXLCSIOWOZFHMxCDrJDQRqWEC5yr2nCtH/FBO5QFYGU53e+Q/yMnEL91Aq50M29kBqOrjSlP31i8ejGCGQBUnrz9r2vNMqcVglZo5RIysVC/zC6XMfTw3AcVMqUvRY8/q9U0YEyDQFTviUXqmwK4viZZkgPgltL7tYfeMhEgtNbdOwMmXLiwiikbXU/XIsy6VU6jr/5sFEmswa3giMb4Xh68/w2HS9ciP32nos+rZRSdhDjcfnnSzux2pVC7iXOUSmlwsNi7BBjqXTu2j9ojvPXzN6k4w2VC9RURLr3ZR30yfEc2m06b8pMJTExzsnFVquEu27XqFkfe/Kou7FxIXNrHFRTkY5/wT3kho/C4W/WZfuM7d7Y8qiO20P9qhRSKngoZ7qMjjmQCEq5oymDNO81afEwIOP5kSOqFOdFTdfGbeI5Y1cW/fJHnVhwd2XtAR13u2T+0nlOuipnC2hrcpFipZQaFK7KoT4fy2EnkH9LOftr3vo+HZftFW3cLZNzSj9sWU6x5DbFlHl/dNog0BubhWvo6froM6njRLT/J+4j6qzFxnc7r8cYEj5zkY6dORiDyh9Gv8qbxqaqcj+uqXSXlC7aLhWyUi9FPabX474w/2JI7TdthJS8cZ7Z3/t2Ygy0m4R8/BDqzZ6B+sx3mRLiaA/OLVuFeszU4rnixl8+aJS5/n7YYE77CJ4v1dc/oI4GZ9QIIYQQQgghxGfwRY0QQgghhBBCfMbUkj566ADzHW3G58SgWHw1J10fxa7C5rSzXOS6kBWuj3Iz4QxZzJsuMoYJpIgtsYPxHKR1g2eb2sn674l95b2do/yMWM9YPZ6Aq2Vnb7WOW2ebMq2s+F9BQxQSPClXjLikixkh4ch7yCLNRV7NbeQC2vI4fWnMye/LQJqaLYO0RClTYrk/JySs/l7vWgkjTRW3siW3eboHkrqaGtO1aX+23r25Usp0xfrZnN8b392XhFxHOkpKGVBHFG6fOZeTVjKPPiOd7GLClWt4APX2dAbOkkqZcscKG1KM5DQcv7RQz9/0XQonsXi3+ZvD92BhaikPqvr5Y/j7z839BS6E/LTpY6iPhevR1x75lpDMtZvlL58Lh70df3Ppwv9xKplMyb8rpZRQj6m8cMoNpoSDbtZsG1KiVYx4ywP9hFumXxU8pOO9Q5DaDYk2HYyYEq+wWMw6J/qEHH5Cwg03kTBd/UbHxW1fSPML9dhXpAMyncLOyTl3+oHWL0DemPuC+d3PmiD9PfgG/L7AuZBOv3Ueyp8T22WUnxsqLbULCRn5iGOOq33iXiUl/O/Z/jodF34FiVXDXXuN8vnunpLHVArSyfvumq9juRCvUkp9aRC/87QoXOjSwvVyR9p0U92TxD0tMOLh3OcHPFJhlFIqOLtdx7f88EYd9xRQByHXc0WFeFaLCkfJgIeTYNJ1zK8PQI66PIZnhqnq+ujJE+aC1x1PiA/iNxXWLBdfQLvYe4Y5HokuopLTxPO6eGyb9oSZYtIbwjg662e49yTvnaHjeWtxvwvETOliVRKpRnYD2nuhD26qVgh9xG4yfSeL0dIuw4Vd6L9f+NoS47sOtc69+aThjBohhBBCCCGE+Ay+qBFCCCGEEEKIz+CLGiGEEEIIIYT4jCmWo1Zawzuw2NSfKgc5MzJVyRKSYidm6pNDIZRJF4X+VJQPRbBNznHpi6Wlv/hOlhlJwsY0VFY6j+NZSB2zzzXM2Uqc3zlx6PtHFuF3X1yxxSgjLY8HM6jH5ihyi8YLEaNM3jFzVf6BzF2TuO35A6JRhESjmB5D/lxBCKT7C2b+i8yjWjcKC1jl86UUiqUvm0F+I/IJz7zC1KIfziKHpjYIG2mZ+/XhIxcZZSLCXv/sCui3N6ewFENzyMxblEhL6ZTIq8gV8XdL5I6OOmhrSikVc0rnRSUbUL8vWI6aR1+2Imb7LopcruE3rtZxug7lpz0mbLxdQ5Fdizos9A9M6tQCDz2l47KH8PfH/xl5PakZIn9grpm78viNZ+q4Wq1VpShOkKNmy6+MQRthIWc24D2jyJkrxqZGjtrgFR3G52mhzTouiHyzihpYRkdDZs5y2MaY1VqBvjOcQduP2ri/BALmwFSwRdsT961EFv0rFvJ5wq0XE+QtyXyvli+J3K8vIbxD1Yj4LKO87KeBSo9RI+fK/RW5ToUh5MJVKpn3h/i5ZKdXvwX5P1dWv8b8shc25g8U2/F3kQdfGDGXylBF2bd9nKM2gT1/fu9+Hb/2jf+m48g+YdOeN8sXE+hz0ifACouxJSjuOwFznqNYixzsvy3GuFmhHlMvGsR9zX5gQ8lNmh/wLj7Ze3HT3xDLWrTWDj1rW6WUcpLey0zIvDRJUfTl/KHDJbd5oeCMGiGEEEIIIYT4DL6oEUIIIYQQQojPmFrSRw8S071lGtJe3wmJDxMpO7LC8lgUCQTwoeiSPgayQhYUFzb+QsZTENawjTVjRvlgc5OO812wx5bSCVX0nur3A7ka/O79OciStozA/rc1bE5Nt4TwOSAutozdNrqOtM5V2K46BAlefwb21mW2KTPNi/0VxP8qEnlIW2IxaLG2ZGFZr5RSIzlIjN7U8IiO/1KxSPmZfFxYnQsdZLKI352ZDRnN6xtgVa2UUneNwG62OQxpalqsy/Cq2ieNMmNCipgQ0sVkAXGPgmSkNmhKbZZXw7786WHIJWW9X7sYEotdKSwHoJRSdeXYXzQAKUPBdAh+YfCQLk8kCax/uEvHWz8KG+HWWyHdGjnXtH+OPwq5o3Um2mTxyc1qMqSugeRr5JW4fvmdFTrOpUypYfV/l5Y7Gkwg47YzYmwVcjxp0RzaZ1ZaokHYJ9ea8lG/4hrK1O402mtESBzHU6JPuvZREcFf0nnUQyyI9j2Ww7Uq5F3/j5W3LlENg0NY7qAuWnr5Dt8zgRzueJH91EsudTLId8KqX3V6b3fKMdF4IqSpn//xd3XcYnu36+Ygnhly4lkrZNkl/74vj3ulUkq12uiLXx9crOOHbjXl+F7nTIgXnFEjhBBCCCGEEJ/BFzVCCCGEEEII8Rn+lz5OwvUw225OQVtjmIK2CtK2UTqtmRIJqQaxhFTEiUKrEgyKMjlT+ii3U0EcJ5/FtHlEOD2Wh01BS7YD8sCAkD5aASGp9LmroF2D33Q4i5Xjp0Uh86yyE0aZdBF1Na0M23k5OCqllCPkpI6oOSldjAdxLlKqqJRSQaE/SglHx74UpA/9ZZB51brO+UgCUr2D1fidxbC/K6ggnE4d8T+aWhvyNks4wj2dMiV1n2yEvO2+FKSth4Qb5L2jpvyzP4trOpKFHCstrnt9dFz8HfJGpcy6bolBbvnexvt1fPVj79Jxez1czpRS6uLKrToeKOBcUk3+rqt/IN3L4nWQpmXnQCp95GpT2jtrcIWO+5eiTyReByeytnvN8W9oHuojg+pUxS2QEBVmQ1q84B07jPKTupoTSH3qN6GPRYfQX2O96MeObUofBxxcg/aDZr37laLrX6OjefwmKXdMjSEulptyrXQeY2NNBG5mK6sP6Pip0TYdHwjByVAppRzpnimqRMr0nai4hz7rVxDibwqjcLK84V/freP4DkhWi2PmfV06YRaz6HNWSDg9xjEGF8tNt/G8GJ/7liFuVGYKAXbm6lmUQhIPOKNGCCGEEEIIIT6DL2qEEEIIIYQQ4jOmgPRxEq6HI67FTqXTYplwYIwIgU7BnHbOZMWlyEu5JMpYVumFQpVSpvanTCz6F8I5O8IpsuDSwPSeAblPk1hwtljwt9Oj5ILZu3U8Jmz1pOtiPGDKeJqCWLD1zMr9Os4IJ8GkcAt073sylNumzDQmnJ/KbchmF1dgUcNqO1kyVkqpJTWw1jqYhQTQKvN3XTXOgRugdH1ssnB95n4D8o+7nzCXn/xL0yt07DRCTjW4DPHoLFe/ahTXRCo7KoVcb1jIrPJm+apd+Dy0E/X2gX1C9rZ3k453/3y5Ub5lDuSSo2Jx9TWr4YD4Qi1lOXg9Fq/uv0D0g4y5kLNdgWtTSGBcmlkO2c6e18ItsKoav1EppQZOhxw3uRJt1xmCnK5zjTn0F8XYJiWywQbIHTs+j3N2LyAqFwP2dLGcYDHi8TaMfxd+DBLbWx/GNZsxv8sok89gXNhVXqumAtlys33fu2+BjnP7Ic21hcNwrt8c/7pDkFXNORsL+P7+MFxZB0awjdonYqVUWRLnYIkVlpMx1EmyCX3SLE3I1OKaL9+r45Vle3UcsswxqE08J9TbpZ0ad+ewzS7hbK2U+Zzwjc5LdDx2k8eJUepIJgln1AghhBBCCCHEZ/BFjRBCCCGEEEJ8Bl/UCCGEEEIIIcRn+D5HzcuePtgGG2+r1sx7ckT+WEvzkI6zwta4LGRaWtsB7PxASljGN8DmNZND+WA9cjfcNFTDbtwWuR/DKeTISFtlpZTavQTa5yY1NXniduRINF6GzJ+rm5FD9KvelUaZsfcg12b/fyCH5bLZ27BNzsxJmxNDrs5QHha5r6p+UscDDjIr4pbZPoYdlOnNIw/r4eG5On5032wdf+usXxrla4KoO5k/N6PJ3xbhQdHGpZ6+KoC+kGnAtUbW0d/Jd/fgg4irUb2q+vhPc1LkPf7uJMwh7UgeSyk8OLJQxw/s6NDxPLXhhJ6bF433HdJxoAD7dOVKVShEzJy1f5C/G32ltgZjnLXBtF9PC3t9az9yLSJZjKW2uaKJCo/gJOqewVgUeBiVa1jwu6ylPfPSJkn8ME5I5pR+4tLf6TjhmC0yJ/Isb+y69LiO/0Ixbq54oa4U49z/DC5XJQmZDSQcx3i2VoxT4Qj68RtPe0LHPxo53yifj6PtBOQyMyIcWIRrG7+99GkRclKZZI7Xf3/jCh3fOo4ylXvNZ7jgqBgUMyIftwLPC4U4ng0LMfNe03sGngWyVTjOLNWvSkJ7fjJJOKNGCCGEEEIIIT6DL2qEEEIIIYQQ4jN8L30sOqWng4vjWFU+vGuG8V0Eakc1vBNCwqL4teOl1UVKKaWiQlc1dBByG1tKh1yz1vKVd8CCzbIl9ELSmf7J6ab1eVm3xwlNoenwli89quPUJbN0PC7s9EfOG1Am+DzjWvx1m7HNuPHpsJLWubg+m9SKYzjbUkDmOkdt1PFH3vsWY6ubP/hVHb9uPb5rfdWW4zz+80v8clgTf7HyAnzRij4S2brOewdCqmGFw+LPQp5cMARy3kgds1yCI+DuWB7Fs6IziT7S8Q7z/L+mFopPkLa8UHJHSf6QWP7h5uNbFOCkW6Y/l3HJ8V6+wlr7tI4fWiL7d2mbbDcdStT7W4/1xF44Zn10rfH5qUfP0vHMPK5pPibkiXnzWkcGxHcPY5yyGxp0/KvrL8J+N5sy/0AWN7jQOL4LbNmnY2dsbIJfQcjUYeh03GuuPf9xHSfyppS6IXz0Nu8IfXDOMZ/ZejJ4pnviN0vUUZlCz3bk5MIZNUIIIYQQQgjxGXxRI4QQQgghhBCfYRU5/UoIIYQQQgghvoIzaoQQQgghhBDiM/iiRgghhBBCCCE+gy9qhBBCCCGEEOIz+KJGCCGEEEIIIT6DL2qEEEIIIYQQ4jP4okYIIYQQQgghPoMvaoQQQgghhBDiM/iiRgghhBBCCCE+gy9qhBBCCCGEEOIz+KJGCCGEEEIIIT6DL2qEEEIIIYQQ4jP4okYIIYQQQgghPoMvaoQQQgghhBDiM/iiRgghhBBCCCE+gy9qhBBCCCGEEOIz+KJGCCGEEEIIIT6DL2qEEEIIIYQQ4jP4okYIIYQQQgghPoMvaoQQQgghhBDiM/iiRgghhBBCCCE+gy9qhBBCCCGEEOIz+KJGCCGEEEIIIT6DL2qEEEIIIYQQ4jP4okYIIYQQQgghPoMvaoQQQgghhBDiM/iiRgghhBBCCCE+gy9qhBBCCCGEEOIz+KJGCCGEEEIIIT6DL2qEEEIIIYQQ4jOCL+TBLg1cW3whj/csLAtx0ftU9nx5lY5jc0d0nH+yRsdtn33U+zgBG7FTOLZzPAHc69xmHX2riTnpdfUigXU1dTjeujrZ9bTrW2fr+D1r7jW+68pW6fjBI/N0vLppn47nlfXq+PbO5Ub5vrG4jluqR3UcuOQwNppgzD2RnAp9yq7BvWbvBxaYXy4Y1+H0Wtyf0nnczo/srddxWad5m2//3g4dF/oHjvtcj4dToa5eLPiurib5PDdZUq84S8eHrnZ0fNGi7Tre0NOq47JfVBvlK3712PGdwAn8Pb6rK4nlOrVJ/Nad319pfC5vSOg4twn1kKtEvVnT0jqueCRmlG/8tsfzu3x2lzyPz/GTqSvOqBFCCCGEEEKIz3hBZ9ROOpZ4Ly2ab8jdv1+o4/B6bNd8zTYdZ67CW333+84xyjfdOMEMGyGEnGQaZ2P25NGh2eZ3UczShIN5HdeF8J/Li+L4z/Ij8TlG+QMHGnScuRmza+W1KF8YGHwupz31meR/yhsexX+GPzH9Dh232H82tguI/6/mFO5jjth3bHEI27judXvfhvhnA7iPbV7hqFJYQfMxoZjPl9yOkOcF9wzMZPBQNdnTGnX8H2vvMop05vepUowVynT8yvon8cXnzO1yn0U/+f41V+m4sAUz2BOqrV4gxcEJx2t88/qtk/ydRz6Msel7F/3A+K7Rxv2qe3GljmOBjI778vh737IKo/xvtlymY/vBDaXPcyJO8Gzu0eCMGiGEEEIIIYT4DL6oEUIIIYQQQojPOOWlj1YorONiLqvjsdesMrarjXfpOPyZbaoUkT+t03H6V0uM7wJ/QcK3sxkSIeP4+Zw4Mdc78kkwHSGEnNrkLlmh43+b83sd/7F/qbFddTCp41e1btTx/OgRHf/P6DIdZwtm0vVVyzbp+M8JGI3MO9iMjV6k0kcrCBmivAcppdShj0Pec3f7TTp+MAWpTq6YMMrELOxjxAmpUoQsyF0jTGkAACAASURBVBMDypTm5Iq47X+lGbKflXdcp+Pal+3EOVPqSE4mUlo2ScmZFcB2RaHoLb8dz1kJJ2KUWZ+YpeM94zDjmRWHZPxgtk7HzaFho/yqsr06dr4JaZ66SGwkn/MmknROJRmkV/14PNMG21qNz72XtOm4/1w8IzdN79HxBzddZ5S5f+X3dTxL1IOUf48FUQfXbHyLeQ4fGtNx33UwkanegrGx+VcTmC49hzZ5PHBGjRBCCCGEEEJ8Bl/UCCGEEEIIIcRn8EWNEEIIIYQQQnzGqZ+jZuNdtChSxLouNXX3TT9BLkVYHUB5YU0stfrTf2DmBmx/H/TOHcL+2AqJ8jI/ocicNELI80vnS5AjWxFI6ThomVbsXRkseD2cx+KgBQX9/YF0redxDiaxQPMV5z6l4ztDyGub9+7JnvWphTsvTbLkcuQz35PEPaXaRs5gbcC8V9giJ8JRuKkNO6jrFru01b5SStkOyvwhgSUBPrfgdzq+cfqlOs53Ik+R/B3juaAwOetx5zz0hUIM5UP3wPKdSyEcBa/r61qo2Ou6Lazo1vHaxDzju9bwkI5DFajT4RzGw/5suY47otiXUkrdOQbfgvfPuE/H3zjtFToubEXup2VPcM4vsP37cTGJc935ozN1HKtOGd9Fw8j/mibGre4u3FOC/ebz9talyOE9N4rxLCTawd8SqLfRHea9q9iExbDb5yEXbrQNz/H7X4J3gsK2DqN8+8fXip09//lqnFEjhBBCCCGEEJ/BFzVCCCGEEEII8RmnpvRRTH866XTJTaa3mlbRFb+FvEOKRrym0EP3rTf/8JqVJbdzkpCwTKnpbELIlCc7DbKQL++9TMcjqaix3by6Ph1XhSBN2ZdqKLnffNGU7TyzG5bLz1iII/3mdqc0kxjf93zFXBbmj9O/quM+B1KdI3nIftpsU2JVFyjTcaNd+n+thaL3rf2uRKOO00XIJVdE9+v40nu26vjPp0MeSUowwb183y+xDIYjNnvpvM06fnQBlmiY9o1HjfKBCki8nLExRQSiv0k7fqVMS37J+2uxxNKGbIXx3Z7sNB03h2H5nnHQl9qieG5sD2HMVEqpsQLG1Fob1vCHroTVf4uQPhadCZ4Bp9LzoZfc8YeQO9Y2jup4sLvK2C5poXwgCslptCKj42zYlH+/9fdv1/Fj131Fx2lxLv9++7t0HJwtlktQSlWX4x53oAtLLhQz4n5lY18NZ/Qb5Q9/FH229Quiz9KenxBCCCGEEEJeHPBFjRBCCCGEEEJ8xikpfQyE4RDjpDFlmrkK8sTZVTuMMj0eEsnJyhWbhJQy2D5Dx/n9B7GrIM5rIicwQgg5IRQwfo3eARer6LA5lu14Jbarm57Q8QVVkOrsTDfp+MF9pmPanJ9Da5SuwzjXvdrbffCUw+P+MGcdJFE/nfZl47uns5DdVAeSqhQ7cpXG5+YgrumHupfrWDrS/WTG33R88yikV0op1RiEhC7hwOVs2IGkck0M98dnHrvYKH94lSkjejHiJVvb+4tlxud7zvmWjt858zwd7xHbvHrT/Tp+YPO5Rvng/UixsEKQqb6onh8m8Qw2WXfML/ZDshYLmNcwJ+Tc00KQ6i2OHdLxk+OzdPxA8TSj/Ege/eecGGq46XHT5VDjuJy/pXOl+7spgl0DybYdQ50MDUBmGigz68oRckNnHPeOdDLoXSaCdvB/ui/S8a5RSPVzjZD9x4Pmfai3V8gvhfRSBRBbIu7rMuWaM18inHC/oJ53OKNGCCGEEEIIIT6DL2qEEEIIIYQQ4jNOSemjk82V/Hvnhfi59flwyW2ehZhqn0h6kMjgu9BSyFnKpPTRY/FtQo6LScpzM1dA+js2A31hdA62KcQgEShGTLmAHUejvW7hBh3f34XFIHsOmgtLWhm0eUuoOQJZcc7y30UTmCYFRJ8JnQZpyqvmbNTxrdtXGGWcfXEd56twAsExyC1caz+r2b+BNKz45GY1pSniOjetxTWzNu82NutbBXe6bTFIHJeUH9Zx2oEspeIeXFellLIfekLHVe1tOj6yZpp6MWJXQyrz2rqHdHxvst3Yri4IGaFcXLwzBwnR/FivUeZgHg32g/WQOEqx1IMpyCVzLgfIluBIyXMeEwtmb8m26PhjzXcZ273tkvfp+Fnux6cwgSgkrNJNevzas3V8/aIHjTJS7ujFTzat1vGKT+83vhuBKtJ45nhRySC9FhSeJPmLcE/47Tb0ks+d+Ttju0fHIOd+uBeS4g/NRvsfzkHeOJg1x8AyGzeoV/z5vTpubsENrmbWTJzXvgPeJz1FZZCJ83ANw2FIuTNCgl8smHVohZzS3zmizKjreV1sdv8BHDOTFgtji2ePRK9ZVwbC3dGK4FpLB0h5jkopdWQQ42v7uZA7W49sVM8HnFEjhBBCCCGEEJ/BFzVCCCGEEEII8RmnpPTRa6r47PO26fiR7XON7zqUWNDOQ0o2kXRxfART4pE6TJmWyY0C4r3YPYU/lRY4JP7Co+0U1pxhfP7Od27U8X2JhTqOBbCw5OmRTh0PiwV43WwQEq6PzIM0pGXhkLGddNIqTOL/Qk7RexspDbOFRvKZNKR2n1j2J6NM+0osShoS2stHk5BLyMVJlVLqf70BUrOL3/aOo56zn7HHcT2dMOoiUDClHJEefHfGKricDeUhGZEuaWVDLllsDaR+TgwSMSt/7FKlU4Ejbzhdx5XWPTqWC0wrpdRYAXcIKVGcF8Yi132ufhgPoL1WBbA/W9xTNmRQB6+u2GeU35HDcZLC9TEhzk3224RLOnngTaj7ufepUxfXPdrxcIYuXD+g44cH5ri+PaKORsXjwi1w+R7ju+/ddr6OZ1z7jI5PebmjF5a4P4jnvIG3rDY2e+X7oBndl9qOv1cgFeXuwcVGmbE8+kJ9GfrYn4cgC+9MYPH3ZTWQhSulVFMEkuJpqyEz374I8u+XN0Aa9/lfXWeUn/lJc7FzzSRTG/xA91kYN2IhODWmk0KqKxeVVspMdxAyREP66JIeWinsI59DXMyL9hGUC2m7XCNFGenuWBROk9IBMlRmPvBHI/h8eA1k6m2PqOcFzqgRQgghhBBCiM/gixohhBBCCCGE+Ay+qBFCCCGEEEKIzzg1ctQCLs2rR47a9dNgZfz0b08ruY1SSllBWHxKLbiTyZTa/O9lhqDBHToN2lZpVu4kk4qcWKwgmnAxny+90VmmFl098Uzp7TwP4p1PaDc06LgwC7bmx3yME4S8HqH1u4zv3vCZG3R8zrue1HGqgLb7kAOrfXe+2LQIdPflQfSFJ8Zn6zjjmEOKzHWJBFA/AQvX0BEW8jJ2I8uU2zh+X7ZCx/Gg2Uc3KeSvyX0nCshHqA6a/XLRL5E7MONOj7yBKUKhBtc82YK8parWZmO7dAu2S4hcjW3DaNMzypF/mKo120alsC4fnwPrYifsWvvgRULZlT06zor/h8r+oJRSwwXkn0VF4vOoiiov5DIJjSK/Mi32PT04rOMjeTOvRR7zSL5GHQ1p26+UUlcs2KrjXe6NTyUs1/+xi3iuOPiJc3R83XQsv/CLLSuNIrNljppHrlHyXNTheMGs98tmI6/+yVdjGYD4bx4/ysmfQsjr5vFsN3yRmT+YEXmVMvdM9r+OeI9RZv3IDB1nxX2sIYzlWo6EMbZtGMS9RSml0vlZOv7nGbi/doWQv/vAMHLDA6fjfqqUUvZclC/sFnmlsh0W/W3VH1uCe0TewXnX1aKNp7Iho4xhnS+HqkDpfDWlzKWDHEd6R5See3LceXFyf3IZIvGMUdmIc64vTxjFD/Vh3LSXjannG86oEUIIIYQQQojP4IsaIYQQQgghhPiMU0L6aAVc06Ieapv2IOxT67blSm800XE8JJFKKWWncQ5Ny7vVUaE9/wlByh0DixbouH8lpqbTdea1Dq6EbKX5N7t1XOjpVSWZqG4aIW49dBkkeOd/y1iYQe0/K+W9jxOIcT0ipmTJFk02LyQgh4TlcH0UU/yhgCmzkPb6B1P43c3Clng4Z1qJS7mklD7a1rFL4pJColkbxHmOCxljNGD26+lhSDH6c6gfKX2sCpp1M+vXsPT3t9Dk6Fhp1PN4M+Ly6nJju0AadSvrfehmyHsuvAFCt8eqzT5VaER/G52J20qk/8U5rknpU7qI+0bUMtunlGIFFPqEtM2Xkl+llBosoO7qbPSDAQcSoooApGD7c/VG+YGCWff/wBbHl+ecEOeilFJvrH9Yxx9XptRvqhMQEl63HX/XDbhvBJdDWhqxhOT+sDnuG/uO4DrKfZ/RBpv3WZE+o8x93binvfEzf9Tx7b9p9DzOKcckpH/xuFlXQ+I+lM6jLd/0zAU6rr4rbpR5yfvW6viZ4RYdVwlpfF0E8dMHW43y89+zV8c/euNVOu64bkfJc86kTAlgMRopuZ18vvV6tj2ZBCpwX51Vg6Uqdg9i3CmPICVhSb25ZMX9/ZCD2jH0pYKw4JdW+0qZlvqBAC5KQWxmRQolt3dTFDJIK4O29spZT+t47cAso0xuHM8i09s9nhtPIJxRI4QQQgghhBCfwRc1QgghhBBCCPEZ/pQ+ul0cSyHcfzzd/pRShTVniE+QbJT9dbuxnZxRdssaj/Z3pZSyU5g+vWAa5HTrlMdvccvpAnKldDnVLbZzz3ufanJJD1csN3YlnJd2fArunXYG5ct6ELd+a4NRXspO9n0ScpZZ34AsqTA0pCZDcd8hHbd9BhKHJ3tWG9s1VmzG8ceeP5cg2d6tMdMBMTkN1+Sxrpk4t/JxVYoy25RphSz0uVQBso3mMKSPUoaolOnOmBTucY7Cucj9ToSU5EUCpaXL0hHv7+cDKVOvkD4OZyFRuq52j1HmrplrdBzepqY0oUH8Ly7Zgj5ViJvXSVbBk72QO9bfibHsY19A+/6FfbFRPt0EqVFOKOuifd4unqcyh7OQgp4WhbRtXsSUxfflK9XRSBdNCXNAyIbHHLRvua+CjXr3kjoqpVTUEq7G4v+2DTYky2458faM6Rg61bFCYlwS94bCS84wtgsm0H8a/i8enS75Jcb2ex64QB0r6x6H0+6/XXOf8d1AAv1q4zhcCQ9+Auc249OTc6Y1ZJ1ZUaceTop+wUv6Zy0/Xcc1MdOVz3QYRiEpWSuY3Uo90Y97YkNZaSfOgQzqo7baPOb4mvk6Hp2Pazqewz2xVkgn57aYMtd8NaSCcgZloudbP5A6D/Lc/T3oP9Xl+K3ZAp5v3c7MlnAGDoZK/9ZCwnxVkea5oTDKBGvFM8ow6i0Qc+3XcJdGv7bqcW4Lyzp1fOvACrO8kFKm8zg3W4wZ9gPmc+fxwBk1QgghhBBCCPEZfFEjhBBCCCGEEJ/hT+njCZyKL9pC7pXG1PaJlp81bMTU6svetFHH69SKUps/Gynl9KGzzwuCh9wxsHSh8fnQSyErqhDrQo4sgYwnfgi6hs5/dUlYkjhOzXaxcOKc6dhoPVy93OeVu0TUqZRliDgyYpbJrIYsInTPk+r5Iv+xQR1/s+MXxncfP/RyHW/62zwdl6/AdRu3vWUWOQd6A7mYpZQ0plx6EvmdXAxbyh0DQkYgJZFKKVUQi27L8knXIrw4vinpGwlA4tifgQQsL/a7M9tklIkewdgw1btidADXc3wmfo2dcklHi0JC14UFWmv6Si9p7DIiVNF+SF4c4Y5bs+vY3XVPBZbHDuhYuiYuCJtyJ+ngaCx+LdwhR1xt2gkESm4nyYkFf21XKw4Jl0Ipa5ROk3NCOM+BgumONyOEMcY683IdF5/crKYK0qnO61lg8AOmtK3h5ZAy7bpxlY7PiqB+In9eN8FBS/9ffP4P4JR37mvNbZY3QTa7eRCS019e/zUdf+zrl+q4MGouoixxu1hOGdwLj/9/nBiu+6Fec+H2q1qe0fFgGO13zSLIt89YfdAos24Uz4edwgl5JIZ7yJCQPi5rgDROKaVaPrVFx/K+dU8npIGnV3XpeM9InVE+UiXakZo6HLgGcdzGWCPdamdWIJXkj+uWG+Wl43BdE7Y7cgjXx4q63gmc0pL6bHZyrzTyGTsaw/NPqhPj8eNjc7C9635nl2EMHRpDmxh4KWpu1gOTOpVJwRk1QgghhBBCCPEZfFEjhBBCCCGEEJ/BFzVCCCGEEEII8Rm+zFELzoQ9dNeVWP19DBJi9darYGP76sqnjPLfHThfx6P5rToezEN/OuNxU3dfZkOnKvNncsIHVP79QLLWKN8S2lZyO+t+5D2Vh2D9WRcx9e/3bIbV7Jz/FrkkD4rfdqrZ8buwGxp0PHb+bB33LzaXOKjdius78jrY6M78IbTCkTthWbzzR2eaxxH64tROlOm+AG0idA1yEEJjrrypMlWSsHD0z9SZdRU7Av15g3r+OLxJ5Ft1mN8trjyi4+0j+NIRVrVjwkpY2horZVoeBwP4bjCP69aTrjDKyH2ERBxwWfT+A6ntV8rMSxvNI4+qJijPU+S4ucoP5WJiO6mfx3bxgOtcpnpimqDygLCJPtN7eZFCDD+6Ylvp/L8t2ZSOs5Vm+3aC4rqL7hLbiDwQf5tMHz/BdtinNwU36Vja4xeK5lgyKqy/A0q2T8QxV/uU36WLGFfk0hQNQeQqFVz2/HW2sKAvlv5frS0sq3OuJWbqArh39azCkgCNz1/q7QnHCpZ+9Jm3DuPK0LdrS26jlFJ7r/2ujpd85V913Ky8rfKdZLLk3wvbkAf6ki2vML67ad4vdXzlpvfp+I7mpTrueAD7feTb5rIwMmVYrG6iGn+LXK3CwKCSyGvjB2v4Yr50HmYhgnZpB81tam200Rll+H13HFqk466kuTRGUfRNeU+RsbRi70wil1cppQ6OI09uMIX7zvQKLF8j91UZNvv1WDl+z1TKUSvfhXEnVY4GV1+OOviP6Xfq+OX70F+UUiowijFwPC1+eUHUR9C8KTs5kbueFksuZMQyV8L238maY1gxI/LiGuFHkF6PNvGaqx/XcVvU7CM37zlbx1VlGE97R83nnxMFZ9QIIYQQQgghxGfwRY0QQgghhBBCfIYvpY+Fn2DK8ruzvqHjCmFFvCuHVdyfcdlrX1O9Xse1AWEbLTQ5l5VvNcoMC7tvKfuQcVa+17pUEWMOpm+7C5g+/a/Zt4vzh4xgrGhe+o9cfC++uwjfSVnapw+9zCiTuMC0ep6KBJtRd1s/LqRDD+N3h12Owz1n47vqO3GtI3eu1fHdR7BEwqoPQcaolFIBIV+wru/Gfgexr2wE20QXmTLVzE5Y98a6cC7BFNpK1OVqLhR8nrKbE0HNVpyP2+q+NojfIdS5Kpk17b//QbjMtMStCKEvxYV0Udp/S9t+pZQqs9Fns+IiyO0csayB45KGyc95sTyA/G3NYUgXuhTqRinTkl9KL4fz6O9NwWGjTLGs9PWYilSug7334ZfB3tvKuOyOhUykWNr5WN05DtlQrsJbhm0LRU9hYMhzu1ON8cUYyyrEfac7D4mUu082BmENfzAHO+pKy9tKXUp1C6p0ZUl7/rBlytekxDIrZI1NQUi06kW/7XQ1lRUR9J3xVrSDRs8z9h+FIbTL7vedo+Ndj+PHzvvpWqNMw6MYW27owpIvzV+B3HH8OvNek6oT8i2hzhOZFmpYSNQ/MePXRvkRsWTCfZfDkv/zXVgWQbapG/73rUb5UQc6/WVRLBnxqXteiY1c0kdlC5mYD6SPXikf6XqM00EPKb1SSs2PwhL/4TJYrseCphQ8Ke4JcdH+5X0zYqN9SPm/UkqFhbZ0fm2vjkPi71LK7yZf5jHw+pyWL5eW+9rTMCK8+4z36nj+E3uM7fZ+B+lBiaSQPga87zFFKWUMoa7sCK61kxfX070vp7TMVSrBP3n1G3Q8tMx8rmi6A6lOhWGMm63qgHo+4IwaIYQQQgghhPgMvqgRQgghhBBCiM/wpfRxz5OQwD3UiFXdq2y4jkk5x5jLhm92vF/Hb/jcDTouG8BU9be/eqNR5leDcHEp95hGzwnpVcw2p83LhZPWvRfP13HX9zFl+s1FcHDanmkxyleI3ybllosjmLbvSZqOMuXqhZc+WitONz4H0qgHR0jGHCEdHG+DLDQbN6f3x4WTZ8dP4eConnhGhz03n6EkReH4MzIPx2xcOE/H/YVHdPzYl+DQpZRSFzzzTzoe+yPkYHMfg8Zy9+uEG9tGuDkppVS5mEW3CvgQRBWqeI/pQjU2HecZqPN2Ezte6p/wlpqNiH4iz3WoD+0q3AwZYEC5nP2EJm44h31JeWPniOmEFRXf1UXgThZQQl4o1DXSZVUp00G1T8gYF1dA0nd/P8aI1pgpY5QSzYMJXPfucfzmqGXWlXQwnJpiFFAchbTOsiHNs3KmpCk4INpnaZM11ZuFNDiYMq9McAB9N9oHx7Nizttp8lRjdIaQS4t2G5tAqjhcwLWS97SQiJOO6QFXHUA/6s7j/hIVFScdIN1Ip0gpkZTulBUB9CPbNQ70FyAFs2ePq5OOJa7pJJ2R7RqM6ee+foOO/3LPch3PWRc1ypxVgXSJO/qW6PhHB3+v4xHHlEumxXi2KztNx1JmejCHcWnd+Cyj/C2j4rlEuEbXhNEGdo5AYva5riuM8jNrcD94MITnkn1vgLN222cPGWWKGW8ZoZ/IR1HvmZTZ3g9mISNeUobfN5BAf1Om8bfKFVBXY1n0OdmXpOtjyDaPOZ6FdHIshPIFIfNfVof7VkOZ2Xe6q4UcL46TcxJm6sVUodAD+Wfkz4hdSmrVVI37yoE9aMuBCoxnz7oP2+jnFXE8e0vppCPGNss2ZapylJBtIoNHSFX7k+06rtpsHt79G55vOKNGCCGEEEIIIT6DL2qE/L/2zjzIjqu84rdfv/3NrlkkjUYaaTTWYmMJryAbKwZsygQqhACBsFOEwkDKhuSPAJVAQpIqwDEkRQTFUmyVFEVCCIrBK2Ab78ZrLNvaPLK20Wg0m2be/vq9/AN9zm2/lse2pOmRz++v783r292vu++93dPnO58QQgghhBARI5LSx84diK999y4/vn4SEqe+BKQD61Io5GuMMT1UmDd/JV4vTxyDXKsjZkt/1mXh/vecAri/g93t0gF90JoE5Jbffv8b/fjiXhQ+Xe6SZMW1i1/2xyFRyDeaO1AWbrDdLVvMM03382TARcfHrlgRulznTrx2js/ScUtivzNHcay8fruYbg5KAHN4K+RoifNRuHP4fbacZPq9+C7/x5ArPnUN5Cxb7r7aj++55OtW+wMjKDmduASygsKbEcdn8LI9X7ELxjJcEzo5Re6FcVsWUcvSy/suWx54MnHGJvx4R8AN9Sxyv+p/+4gfZ0voF0lytRrMYl3G2LLEzSQ9ZLad++/WZ5b4chFeJkkysaA0rERurO/twnXAzqr1Jfh/E0sljTGmLwEJV70Vy20/ttmPfz6z2WoTP04FgZvu8eLByeLc9vXhWDTSdj9sxMn5ym0u+DxUJOeroMKsjo5QbVnsgtEXR5WU6VWyD2Np7aq4feCeqeK7tEGcJ7ljKjDXjNO1z/2FJVqt8eauk8bY/XC8RoXjaX5zab35gPSyQPLCgSW21HhBmKfcMXcnxn2WEb6uHZLGv3zPbX58U36D1f7RPFIy1reO+fGn9qNIdS7gJDhewtyRpHuOqTLkVuw6F3dsiVYqjjaH53Aed5fxW3pbcI/D6zLGmB0jGH9fsx5WxN/60Nf8+LOXvdVqk7pyn4kqLFmttOIadQMFkXke4Pu5EjkcF5L2HM2jFssV2VWzXCU5XeBY87EfnUYfPWcp5t3BNO4T981BnmmMMR4pbWNtGEwWq/TRxCiNoY7z4aTs8WS2THMRnYRcC0kaZ20Zsqng/HhsU8znhJweneCURN+Vy+SynprfWGL9NqZ+au4Y9EZNCCGEEEIIISKGHtSEEEIIIYQQImJEUvrYc8chP044eMX463FUhvzDPtiwJEl2aIwx95ch+brv1d/046yDV6y3FLutNv0JFH1MkODJDUgRwmBHvCeu2ebHR8kha3cVcoegDOw4FcxmWeVQAtKJnodP3yvwmQshmSgsI4lB0V5ucgP2OzfGznF4hZzbAVlp8kFbKjP9ZrhIzq7B3yuk1vE+ucUwMVKX1B/GglwvsmiwX2949INW+4EbEbc8CVe8f74Vsr2rP3YNtpGwX2cXu3BNxstUEJ0UkrWAuyUZhhrn+Kk7j944nEAfL6y0vrucXMu2dkNSvO3u12LfUuHX+5I09vudnff78eeehfTn0lU7rTbsvrWrhH7JRUS9xvz+X7SZJM5fOoxr4uPLfunH3z661WqTI6fDgTT6+FgR0pKvr/qF1eZPk6+Y1/4sNrhYq5e1pY/1BK7jtv3N5Rv37YUjXaPTXqaRhpwlUZinfOQMg2vZbiZ5zy2kcp/07OPGskKWFrN0a4lru8MdJ40UF8yedSBz5XUFXU1ZTt9BEnzuh2NeeKFj7q0s9Ss+d9HTTnzNoB/nbbNf8yc9D/jxvhLm/0cKsB7+/I43+TFLoowxZnUPxg+Xih2z5I2LHhtjF1XuTeE8rswh1aFCF05H3E6JuHNsrR8f3gO5Y98Q7nl27cZcHcvZ5/ptm+BoyfcoX9j3Zj++sNsu0nvHB5Ba0Bko+r3gLMV5q5GBY3urffWtTmEe7HHRR1oykEH2ZOx5eKYSkNf9jmUkn+/IYjuZeIg9rrGP9ZZOFHjemMK97S31jVabWpbuJYYwb8VGj5gzCce1ZYOFEsZAJ43+U6thuXrB7ou8nEv9r1rAvOaQHDaojra2U8G6G9n53e+fKoljGHqjJoQQQgghhBARQw9qQgghhBBCCBEx9KAmhBBCCCGEEBEjkjlqtX37m/69m3JkViXHmy5jjK21vzEP/TbnfgXzYjgXrUqHJdaoN10myCEPuVL78mifdZC3xdvnHAJjjCk0oNNNNlj/im069zwWuv2TjUu5V5y/4nh27lWxF59n17hN2zSu6PfjWMW2+vcyHdQQqQAAFo5JREFU+H0t+3BOuHqCZ7u5mtQU1t0+ggVjNdIq57Av4++yV3DHv/2rH1/1F8hF+00R+QCHtuIctuy3fzNr4xsxh2ITSmYc+1Y7dDh8wZPIRbm91udSA9fipjT6WLwV12WDtPWbuqCnN8aYFJ2UB4vIV7p+9X/58e6qbTlc8KAZ709NmWbUT3Dg2Jp8bxXWzJ/tR17Zbwo4b5d3PBW6rjGyKU+5+C1PVex8rVgF3y12e/4GifOPzeX8uKc9YM+fxS9texzniX9/ai9yOGLnzhjGa0MfY9vslxO1lua5eVx+4v8qvdZ3nLc5lILlO88PwbmCc844t5nznmfrVIomUArmEPUjntPY3v+AC3vxYI5cmryuY/PM4T6VHLkW+ao/uvY6P/7SkTdYy337mUv8uEr5L7kU8sjaMrAE3zq4x2q/MoVyJTeNn+PHSRpL5qr2XDNeRJ87WqDyM27zPPi0a5dSeNsAcsxuSiKfu1ClcXUQ+Wqbl9hj9m0H1vnxzDQmrgbl/Ozv6bTarP8Qfnf+eyZSlAZwfLhbbO6xf/cs5XFyCYuuDPrCimywtATKjxRr6As8b3I+Ilv4G2PnLXI+cIFuYKoGO92XQe6cMcaM1LGftRzOjz1SLx4cvjeiYcKJB/LN6Ji6SRy3CuWOmcB9Z7oFuYZWmQTaTjyJftkInCuvRp+5fXKe4xn7/c+zPMhLQW/UhBBCCCGEECJi6EFNCCGEEEIIISJGJKWPzKNlvOLc1HbAj1n+UQlIQ7gSvWeay3BigWdUlqfw+sLkjmxxHNwO7xvLHa3lA3IvtlwejENicbAWbgF7Kkn/L6yMBw9DcnHocluaUaf38qkJfh3M1eLDt9Mgq9YaVCLGowrxsZq9glmyvp9eR9aqdEhr7TifuYcgOTHGmPP2fhLbuQzb+fJ22Mzz2/Bin32u3RK2nyQFWGaCJCyTtr11Zg/kKV7i9IgZHsgPWZ9f37rDj+/JD2N/qjhwiTT2+1iZ6g0YYxJkPc3X9TRdBPfODVttWlz0xSzVVShQG5ZPBa3EWWoy7kGONUiWyQWyOJ/xSJdqbOlkmaRdUyVIw7525HVWG1NfeDnXSaMDx6xapXGtHBB1xnANOMWyaUbrPpyLrq22bOjoJpSCcEv0RYzG5tNsaXy6qfbgWpupw8a70sA5CMoQ2fqb56BWkhsG5wruIyyZZ7kj2/4PJuw0Ae5vPI9NU98Jm8+MMWaS5UWxhe8ry7+FlIAbPozSGp9cequ13Ecm3+PHy9ogE2VLfbbTHyvjvBljjEf3DEMtOKadCZzToEx1rgXn4YlppGFwqZP79lDZC88+108eppIm7Wgz2I5SARUP27zxN6+02q/ejt+TWoExd+xSnLdyMWG1eUffb/34u2aViRL5Puwr32J0JQNW+3Qt8/xQ9nC/cDQwv1n9ItZ8TmLJfJCWBMbNsH4xEEd/L3r2cffS6IuHL8N3gzeHbnJRUi/b80s6SSlBJHf0ivR44tr3YCyLdF0c62QrrvcqXdfJLNV0MvY9j1kE1WT0Rk0IIYQQQgghIoYe1IQQQgghhBAiYkRe+njdKJybPrL0dj8+Qg5uQWlGGHV6Lo0Z+9V0mETypRLmaBeURLLspcvFdx8beRsttTAV6hsPQTK3/CH7O+/y8xCnIMGotOJ3V3MkqwooAvjtP6tGUsfxPjo7Zr8qd/M4Pu4sdFbOcZI/kJTLxG05incADlEN7/nlWLFMxvrcqOA1eqMGKYRDksZYS85qU7wQMsREry2pecmEyMtuPrjBWuyqjZAIlRvU9enSTyTQvi8FeZAxtqyH3RyZ9nih6d+NMSbtkPwg5F9EQelQqe42Xa5Qx/6zvDFr7GulJw5nrWcb3X68ug3SobZEyWpzLLtYfbaaQI5UXW3oH16qy16Oh6MQ6WfbfhzbgZwtfTzYDolUcoZky0l08HrpzJY+Zjsgd5ykcYXnneG47aD4tNNcd8Nyx+D8tq/a48dLSUrF7oxjVcyPs649fnXE0Een65CIcV/pcXGtlAJ9cpYkxCwbXCjqeezrT/ZD+vfGjU9Yy+WSGH8u7HrWj8crkMYfLMD5b65mOziubxn145EizgH3HZZBGmPMQBrjzNtXP+jHP50534+ffAjj9KN/vc1q/1QF6/tVAQ6O//KzN/nx6k/f68fr1tpyvqc+DRfenqVIqTAH8Tvb2+19Znlu7Jz1JkoMfGS3H++dxHg+krfdhl/Zg/Obb2A8z8Rxsi7s2Ge1YXfGR6YHmm4/TfdmSTf82uc5pYv6/IEajvtEyb5HOGcL3DZZ8m97Qy5+3M6O0O/q5O7okNzRzdiSU3ZtbEljXpolPWxlDue9HpAUuwnMcex0XTcLL+Vuht6oCSGEEEIIIUTE0IOaEEIIIYQQQkSMyEsfH7gTsoAv/9kNfnyYi3YGHHZK5CiXcMJdesJgGSJLIoNOj0zCNC9gybCEJSi1ZEepdpKy7fvZGj9etkDSxxPh/hoFOVkgk1kCaZXDMsCaLRdodEB24rVSYcg2nMNyp+2OVO8mOVUC657cCCkEn4Jqq33enPoytE/S6/UiSWNJpZects9VqYeKV5O7ZIzapybt/4H0Pkz6mPseNyeVEDe9uaIt3dlIcowDaRTd7u2GxJGLeAZlPLvyKNZbIenhgRZbdsJMVXF+jpQhx2JXrDBnVGNsCRj3qx3k7DpFdqGpwLqqVewnF0Fl9y52AjPGmNgMfvfCC7teIjMQzqxqw2/etbo3sCAVB80EKsz/juQhWJxmXNtFq9SNcxMjR616yZaVnsl05CB9DBPQTAakvHN0TbbF0J6d6qqOfX2yeykX4+V5K0v9Y5aKYhtjTJoGtypJoNn1sd5g10e7FxTqXAA4WlKhqYcgSZxeb1/HpRp+awtZkyZSNIbTHL8kactUqzTm8XLHa5CWHijaxaP5OJ7Xv8+Pf7nt1X7c9617/HhD58es9qu2o881HkEKwtoeyORG/hNOl1/c/BOr/fdGUeT7ySNwkEx24PqYy9vXB187xZW2Y/JCc2kXfveSFCSvB/L2cd+SgUP4rfm1aL9krx+vSh6z2jyUh/smF8Pm47E8g7kyH5D/t8ZxTZXpWklQ/1nqon3Vs8eCeAp9aSAz5ce2gHfx0wgUiM6S6+NUicY6WsxJ2ffxjSKlYZRxHvJT6ItOHMezVrGPdSze/F4+29rc8fg5OOwaeervEvRGTQghhBBCCCEihh7UhBBCCCGEECJiRF76uPaHcE2afRdkBPw6maWOxtjOcSx9PJEMMkzWyH8POkVay4VIQMKkIcHttZLs5bEKXt8uu/4esxjxJnDeDMdBDjX/cyIkPhEn2Utx8RDi+pi8x5at/GoTpC8s5/jM8C/8+KospHL7a7gmjTEmRWZNz9Ygk7qIi5MH3ADrdJ0nnOYOji+Z1onQr6Y8yBgfq8ARrVw/x4+fPL7MauPtGTmJO7ewNGYh33rsMJxH3bZwl1unGjJOTnKx1nBnzMYpOs1Rh13gKiTZ7XHRp6oBF+D5uA2HzS3G2PMIF7lmSWTa2HLgYAFt7BtOXCzEjTK4bvcEyy0E2VEcz1zALXOoHVI3lr2NlCEDXpNBIet2N9zBti8BSSKfn8ma7bp4rIrP7Pg5vZ4klrT8yr+z5/vYMFIf2u5GOsH3BzFm31yApPxHRy+22mfJ5fCiATghFmrov0+M2uNfiaStU8PznX1PIU7z9JOpCuagvoztjfi5w1f58XD2qB+vSuG83zp1jtWGi2ZzP2V5cAelAyxN4RowxphniziT57Yc9ONdJcy7I2VIc1e22vdFx0q4VjZ17/fjJ8wKsyhxmo8zTsaW2npcuZzHE2penQvMNyRd5LGK5Y4WjUCq0SzOqUOOkk7ExrPfozdqQgghhBBCCBEx9KAmhBBCCCGEEBFDD2pCCCGEEEIIETEin6Pm7djpxwnSJ/fHYV+6q9IX2r6V9OMnstcPyz+rkuk8P9XON28gDNb5G2PMcALa6R/PXPC87YXwCbHnX/oVO9/hm19Z03Q55ppvXOTHqS47R61axnCRyiD3oVzCtdyYtLXkbgm9JjlFOabkfJ2aRn+JlwL9ivITnDqWK7dhvRXKt6raKSJW+QWnhuVWb8O44h0bM2cq9QJyKrJpst1O2PmL2WdxDr3DzY9HYw45HIngv/goBeAE1RbOaNqTsOce83AhdlD+cakxvymXS1akA7lWnF/KOW5tMWy/g/KreHlj7JwbhuetVsrnnqlHIE9pnlzywYf8+JDXbn23Io0cyzrlIF2c3Wua8WBxtfWZj8/aFErlPF5c6cexwNzPZXe+MYX8sW1/9B0/3n7ZeX784e47rfZnJ/F7vjp1lh9/4uAfNN3nrqSdV8e5VkUP53FVFvlR7SvtEhrbp7A/bmXhc3bcjfjdPfFb/LhCZVU2tx202nz3F6/147GLkZtX7cL9XLDEyOoU8hPHYrh2uC+y7X573D7WYX4EP915Lrafx/x4zatus5a7vbLOj9enRv3YXfsaPz4T8qfr7Tnrc4VLNtG8Ekvg7w0vMOHEKEeNSv04Lv7eqKFNox7IBU6iTYOs++u0nBPHuW7UXniZr5OJ3qgJIYQQQgghRMTQg5oQQgghhBBCRIzISx+ZK+/+hB/fdMnX/HhHQMrBr6otmQfJOYIWxa7Dtr54Jb4kBrnPbN22FWVYgsKvwJMGr2/TtP18w5aIpanNz6/f6sed5l4sFAv4XodI3oR4sZz10QcWehfmRe75FzkhL8eeU/MwfnhpW9JUy5FkpGpLgn5PvQSJ1NFSt/0dSUyrL/XkLFLiMVxVYTKooOS9y803/W66CuvxdEBLerACm/buBGzJ6y7mtPEapK3ZGGz7jTEmR1LICs2PvM8FsupvD2y/Qv/fna3yum2L9NPF0U9s8eOb+7f58VXDl1jL7fw6pGX/9PrH/Xjdd67248EtB7CuDTeEbvP2Io7Bxzsf9uO2mH2PMFGH7HWc5FtpKi/0leUsUbfvZbbnO/14TRLSvKs7dvhxysF1E5tHuQdjjBmlsiVjgVIb56fw+axV55mFZubszudd5oLsM9bnnzwD6WP5AhzT1SlY9fcl7H4x46HPZV30EZfSYo5RiZfjNftcdyRwrlluTLeWJjOCY7txq12b6HaD65NLZRy7BPb+nWeA9DHI1ATJ8D0crEac4pp9XTtFKpmwHPNSsYzjW5xEmatY1pYuWrLIWvM+E8vievCOH7e3H6N9C8+COmnojZoQQgghhBBCRAw9qAkhhBBCCCFExIim9JElfiTvG/48pBXjN+G1phd43uRX1ezmmCDBkxuoQM6vqsdrbX58f2HIj7fmnvbjirFliLwH7NLF7lITdbxKHYjbr1JnSYLS+f17TVMkdRRCvEgKBUh9UnO23KPS/cLGlrmqLRtyvPlJrs5kkiR9LJFTYo7c5SY825b0rS2QbN1R6vXjVpJRLYnZ7nLnpff7cYLmtw5yP3uQ5rOl8RmrfSvNT7Mkt+S5it0CZwJpAtMkESt5C+8I2fk0fqslY8zb8+jw+yBRvOLSD2C5u5rPt0Nf/aj1OXsQx2H5dZArOin0hT3/+EqrDUuCGym6LymQm3QZfSc5E96PWE3792TUyMrURCFwXzMGyVdqCscpvg/Oro2qLW1t246464mFd32cGsZxf6Swyo+TLn5bf+B+qm0/vovTgWMZ8Ui512rDssjfzsHxcziFY9VPzqHFwLU/WsJ940U59Ovudlgcp+5Dm6E/h3N5kP+egvP3DEwvzfOLQBcBDfuaeo4j4+//XqDHE9du43RiTOXzW5zCc4FDzo71sn2/zu6QrBYuTGBsc1pprA5IH083eqMmhBBCCCGEEBFDD2pCCCGEEEIIETEiKX0Mc1TxdqEwJRe5Tju2SxlLIdn1MemEy3tKIUU9lyXweprljiz/MMYuNspFssOKX3cEHpE/fej19GnOCCHEySSVhuynsMy2qmp7+oVNBdW6LSVJLId7YXpnS3DxlwXdKYzbkyRxPC8FudS7f/Aeq82qz0FC5y6Bm2N95TI/Lq6wbTSnzsK5mluDOc2pUrFWmuoSs/Zkk4Z5oFXQuPeuY368+2+wTXZYNsaYvVXIiwZzKJy82ywMidtQFHrwthMsSMTuevR5l1l77X3zWlejDEnh0F/Nr81Cc6LyvVNkltlu6Pf84JTtzgmpnYOxZVkSfeloGW6BPzu+2WqTeQRFrvtykBEPJXDx3z87ZLXZUV/hx0uTkLodqaL4tUsyu95kuMspS5zP74aT6O77sf1fFs6y2qzI4rcNZ0iausaWPi96nIDUsRESMwFpPcsly1T43LCDY4pWVrHHwEaVPvM2qZC2SdluqBbO6X3HpTdqQgghhBBCCBEx9KAmhBBCCCGEEBFDD2pCCCGEEEIIETEimaPWqIcIVUnbmoshL63DzVuL9dDnaqP5s6hnbM0rV4LvcKEJTpPYf7YOzWqva+uTeX2cI5dzoF8vkRVyKqBxzcXLpims520svFWuEGJxUixg/Fq1cdT6bmJ//wta1/6jXdbndAbjcbnj5WnV35nAvHFhGnkpWYeO+43h+cfeBPK9DMXpR+zllplTA2dw18rn+/FQws45HPeQt33vEdild5tdp2jPxMsZbxQ5kQ8PrvTjoSxyKj/TvdNq84YxtHn2b2F1v+Y7N/vxhS0jVhu27r8yg/7n0j1YqYHsvnrgfuyb05v8+P1tyJE7//tv8eP+WeSknp06aLUfrXT48Q/3X+zHzh7bD2HR0Kg3/bNTs/+eam1+75tNY07JF+1yMDHKJXvvAPIor5u5wo9dF9vJLrF9LIoV3IsX8lj3ZWv3+PHokkE0QLWFBUFv1IQQQgghhBAiYuhBTQghhBBCCCEiRiSlj6HQq2a2P+Vq9cYYM+fhVWaLi9eq7XFIU6r18J9eJkv/qSpeOydOYO8fcxpNl+O/T9O6JtqfstrfuOtsP15jnt86WAghXgjOEYyLmf6q9V2sGlz6xNTH0tbntg2ws87ng0u/PLjrU6/y4x2fh0Bx57FeP15+3+PhK4i54d8RXL7GsokOkRqFphIYYxwX22xUIQ8a+B/8/YsXDFtt/mMvpGRL32LPY0KcbNZ+CtK2Mfr7sbNf4ceXDV9qtcmYB/w4cctv/fidA1v8OE4ySmOMKa7t8ePr2pvfH2aP4H4yPl20vzx81A9/NYXyFv3mHtOML7zjfdbn2MhhP26ZgNauZaF1dy+Shtf8ftnbYctUu38MmWe5jUprtWCcSweGRo+c8/9h5k1+nNuNL+jW38xCVWqMMSZBmUu9oxg3n5ndgG0++IAJo1F7gRPmS0Rv1IQQQgghhBAiYuhBTQghhBBCCCEihtOQk6AQQgghhBBCRAq9URNCCCGEEEKIiKEHNSGEEEIIIYSIGHpQE0IIIYQQQoiIoQc1IYQQQgghhIgYelATQgghhBBCiIihBzUhhBBCCCGEiBh6UBNCCCGEEEKIiKEHNSGEEEIIIYSIGHpQE0IIIYQQQoiIoQc1IYQQQgghhIgYelATQgghhBBCiIihBzUhhBBCCCGEiBh6UBNCCCGEEEKIiKEHNSGEEEIIIYSIGHpQE0IIIYQQQoiIoQc1IYQQQgghhIgYelATQgghhBBCiIihBzUhhBBCCCGEiBh6UBNCCCGEEEKIiKEHNSGEEEIIIYSIGHpQE0IIIYQQQoiIoQc1IYQQQgghhIgYelATQgghhBBCiIjx/xF0tQHMyhbbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e2ee02550>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# show random images from train\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(y_train))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(x_train[random_index, :])\n",
    "#         ax.set_title(cifar10_classes[y_train[random_index, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.233401Z",
     "start_time": "2019-01-13T03:43:35.157643Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8e2b27cef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ebcde4400>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.330229Z",
     "start_time": "2019-01-13T03:43:35.234371Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape([-1, 28, 28, 1])\n",
    "x_test = x_test.reshape([-1, 28, 28, 1])\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "IMG_SHAPE = x_train.shape[1:]\n",
    "\n",
    "# center images\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.343899Z",
     "start_time": "2019-01-13T03:43:35.331488Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_encoder(img_shape, code_size):\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # encoder\n",
    "    encoder = Sequential()\n",
    "    encoder.add(InputLayer(img_shape))\n",
    "    \n",
    "    encoder.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    encoder.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    encoder.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "\n",
    "    encoder.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    encoder.add(Flatten())                  #flatten image to vector\n",
    "    \n",
    "    encoder.add(Dense(1024))\n",
    "    encoder.add((BatchNormalization()))\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(Dense(1024))\n",
    "    encoder.add((BatchNormalization()))\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(Dense(code_size))\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "\n",
    "def build_decoder(img_shape, code_size):\n",
    "    decoder = Sequential()\n",
    "    decoder.add(InputLayer((code_size,)))\n",
    "    \n",
    "#     decoder.add(Dense(1024))\n",
    "#     decoder.add((BatchNormalization()))\n",
    "#     decoder.add(Activation('relu'))\n",
    "\n",
    "    decoder.add(Dense(1024))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    decoder.add(Dense(1024))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    decoder.add(Dense(1024))\n",
    "    decoder.add(BatchNormalization())\n",
    "    \n",
    "    decoder.add(Reshape((2,2,256)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=32, kernel_size=(3, 3)))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=1, kernel_size=(3, 3), activation='sigmoid', padding='same', name='autoencoder'))\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "def build_classifier(code_size):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(InputLayer((code_size,)))\n",
    "    \n",
    "    classifier.add(Dense(4096))\n",
    "    classifier.add((BatchNormalization()))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    \n",
    "    classifier.add(Dense(4096))\n",
    "    classifier.add((BatchNormalization()))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    \n",
    "    classifier.add(Dense(10, activation='softmax', name='classifier'))\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.348731Z",
     "start_time": "2019-01-13T03:43:35.345290Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build_encoder(img_shape, code_size):\n",
    "#     weight_decay = 1e-4\n",
    "    \n",
    "#     # encoder\n",
    "#     encoder = Sequential()\n",
    "#     encoder.add(InputLayer(img_shape))\n",
    "    \n",
    "#     encoder.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "# #     encoder.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "    \n",
    "#     encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "# #     encoder.add(Conv2D(filters=64,kernel_size=(3, 3),activation='relu',padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(Conv2D(filters=64,kernel_size=(3, 3),activation='relu',padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "    \n",
    "#     encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "# #     encoder.add(Conv2D(filters=128,kernel_size=(3, 3),activation='relu',padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(Conv2D(filters=128,kernel_size=(3, 3),activation='relu',padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "\n",
    "    \n",
    "# #     encoder.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "#     encoder.add(Flatten())                  #flatten image to vector\n",
    "    \n",
    "# #     encoder.add(Dense(1024, activation='relu'))\n",
    "# #     encoder.add(Dropout(0.1))\n",
    "# #     encoder.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "#     encoder.add(Dense(code_size))\n",
    "    \n",
    "#     return encoder\n",
    "\n",
    "\n",
    "# def build_decoder(img_shape, code_size):\n",
    "#     decoder = Sequential()\n",
    "#     decoder.add(InputLayer((code_size,)))\n",
    "    \n",
    "# #     decoder.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "#     decoder.add(Dense(1024))\n",
    "    \n",
    "#     decoder.add(Reshape((2,2,256)))\n",
    "    \n",
    "# #     decoder.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "# #     decoder.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "# #     decoder.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "# #     decoder.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=1, kernel_size=(3, 3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "#     return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.352670Z",
     "start_time": "2019-01-13T03:43:35.349852Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build_deep_autoencoder(img_shape, code_size):\n",
    "#     H,W,C = img_shape\n",
    "    \n",
    "#     # encoder\n",
    "#     encoder = keras.models.Sequential()\n",
    "#     encoder.add(L.InputLayer(img_shape))\n",
    "    \n",
    "#     encoder.add(L.Conv2D(filters=32, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "#     encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     encoder.add(L.Conv2D(filters=64, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "#     encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     encoder.add(L.Conv2D(filters=128, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "#     encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "# #     encoder.add(L.Conv2D(filters=256, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "# #     encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     encoder.add(L.Flatten())                  #flatten image to vector\n",
    "    \n",
    "#     encoder.add(L.Dense(2048, activation='elu'))\n",
    "#     encoder.add(L.Dense(2048, activation='elu'))\n",
    "# #     encoder.add(L.Dense(1024, activation='elu'))\n",
    "\n",
    "# #     encoder.add(L.Dropout(rate=0.2))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "# #     encoder.add(L.Dense(512, activation='elu'))\n",
    "# #     encoder.add(L.Dropout(rate=0.2))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "\n",
    "# #     encoder.add(L.Dense(256, activation='elu'))\n",
    "    \n",
    "# #     encoder.add(L.Dense(128, activation='elu'))\n",
    "    \n",
    "#     encoder.add(L.Dense(code_size)) \n",
    "\n",
    "#     # decoder\n",
    "#     decoder = keras.models.Sequential()\n",
    "#     decoder.add(L.InputLayer((code_size,)))\n",
    "    \n",
    "#     decoder.add(L.Dense(4*4*128))  #actual decoder, height*width*3 units\n",
    "    \n",
    "# #     decoder.add(L.Dense(128, activation='elu'))\n",
    "# #     decoder.add(L.Dropout(rate=0.2))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "    \n",
    "# #     decoder.add(L.Dense(256, activation='elu'))\n",
    "# #     decoder.add(L.Dropout(rate=0.2))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "# #     decoder.add(L.Dense(512, activation='elu'))\n",
    "    \n",
    "#     decoder.add(L.Dense(2048, activation='elu'))\n",
    "#     decoder.add(L.Dense(2048, activation='elu'))\n",
    "# #     decoder.add(L.Dense(1024, activation='elu'))\n",
    "    \n",
    "#     decoder.add(L.Reshape((4,4,128)))\n",
    "    \n",
    "#     decoder.add(L.Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=2, activation='elu', padding='same'))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     decoder.add(L.Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=2, activation='elu', padding='same'))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     decoder.add(L.Conv2DTranspose(filters=3, kernel_size=(3, 3), strides=2, activation=None, padding='same'))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "    \n",
    "# #     decoder.add(L.Conv2DTranspose(filters=3, kernel_size=(3, 3), strides=2, activation=None, padding='same'))\n",
    "    \n",
    "#     return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.441852Z",
     "start_time": "2019-01-13T03:43:35.353562Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.889936Z",
     "start_time": "2019-01-13T03:43:35.442840Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                32800     \n",
      "=================================================================\n",
      "Total params: 2,529,952\n",
      "Trainable params: 2,524,896\n",
      "Non-trainable params: 5,056\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "autoencoder (Conv2D)         (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 3,124,865\n",
      "Trainable params: 3,117,761\n",
      "Non-trainable params: 7,104\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              135168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 16,990,218\n",
      "Trainable params: 16,973,834\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = build_encoder(IMG_SHAPE, code_size=bit_size)\n",
    "decoder = build_decoder(IMG_SHAPE, code_size=bit_size)\n",
    "classifier = build_classifier(code_size=bit_size)\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.898831Z",
     "start_time": "2019-01-13T03:43:35.890966Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.adam(decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:35.902372Z",
     "start_time": "2019-01-13T03:43:35.899996Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    lrate = 0.001\n",
    "    \n",
    "    if epoch > 300:\n",
    "        lrate= 0.0005\n",
    "    if epoch > 400:\n",
    "        lrate = 0.0001\n",
    "        \n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:36.223419Z",
     "start_time": "2019-01-13T03:43:35.903443Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = L.Input(IMG_SHAPE)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "classification = classifier(code)\n",
    "\n",
    "outputs = [reconstruction, classification]\n",
    "\n",
    "autoencoder = keras.models.Model(inputs=inp, outputs=outputs)\n",
    "autoencoder.compile(optimizer=optimizer, loss={'sequential_3': 'categorical_crossentropy', 'sequential_2': 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:36.228431Z",
     "start_time": "2019-01-13T03:43:36.224364Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 32)           2529952     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 28, 28, 1)    3124865     sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 10)           16990218    sequential_1[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 22,645,035\n",
      "Trainable params: 22,616,491\n",
      "Non-trainable params: 28,544\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:36.232822Z",
     "start_time": "2019-01-13T03:43:36.229602Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:05.580444Z",
     "start_time": "2019-01-13T03:43:36.234281Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.6206 - sequential_2_loss: 0.0326 - sequential_3_loss: 0.5881 - val_loss: 2.9519 - val_sequential_2_loss: 0.0952 - val_sequential_3_loss: 2.8567\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.2948 - sequential_2_loss: 0.0232 - sequential_3_loss: 0.2716 - val_loss: 2.7111 - val_sequential_2_loss: 0.0927 - val_sequential_3_loss: 2.6184\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.2421 - sequential_2_loss: 0.0211 - sequential_3_loss: 0.2210 - val_loss: 1.5504 - val_sequential_2_loss: 0.0594 - val_sequential_3_loss: 1.4909\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.2100 - sequential_2_loss: 0.0199 - sequential_3_loss: 0.1901 - val_loss: 0.6505 - val_sequential_2_loss: 0.0352 - val_sequential_3_loss: 0.6153\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1834 - sequential_2_loss: 0.0190 - sequential_3_loss: 0.1644 - val_loss: 0.7338 - val_sequential_2_loss: 0.0329 - val_sequential_3_loss: 0.7010\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1613 - sequential_2_loss: 0.0185 - sequential_3_loss: 0.1428 - val_loss: 0.6888 - val_sequential_2_loss: 0.0241 - val_sequential_3_loss: 0.6646\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1471 - sequential_2_loss: 0.0181 - sequential_3_loss: 0.1290 - val_loss: 0.4450 - val_sequential_2_loss: 0.0244 - val_sequential_3_loss: 0.4207\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1330 - sequential_2_loss: 0.0176 - sequential_3_loss: 0.1154 - val_loss: 0.3936 - val_sequential_2_loss: 0.0253 - val_sequential_3_loss: 0.3683\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1146 - sequential_2_loss: 0.0171 - sequential_3_loss: 0.0974 - val_loss: 0.6179 - val_sequential_2_loss: 0.0220 - val_sequential_3_loss: 0.5959\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.1034 - sequential_2_loss: 0.0168 - sequential_3_loss: 0.0866 - val_loss: 0.5642 - val_sequential_2_loss: 0.0227 - val_sequential_3_loss: 0.5415\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0992 - sequential_2_loss: 0.0166 - sequential_3_loss: 0.0826 - val_loss: 0.5578 - val_sequential_2_loss: 0.0217 - val_sequential_3_loss: 0.5360\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0937 - sequential_2_loss: 0.0164 - sequential_3_loss: 0.0773 - val_loss: 0.4805 - val_sequential_2_loss: 0.0274 - val_sequential_3_loss: 0.4532\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0824 - sequential_2_loss: 0.0160 - sequential_3_loss: 0.0664 - val_loss: 0.4943 - val_sequential_2_loss: 0.0203 - val_sequential_3_loss: 0.4740\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0759 - sequential_2_loss: 0.0158 - sequential_3_loss: 0.0602 - val_loss: 0.5346 - val_sequential_2_loss: 0.0213 - val_sequential_3_loss: 0.5133\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0675 - sequential_2_loss: 0.0156 - sequential_3_loss: 0.0519 - val_loss: 0.5261 - val_sequential_2_loss: 0.0188 - val_sequential_3_loss: 0.5073\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0625 - sequential_2_loss: 0.0155 - sequential_3_loss: 0.0470 - val_loss: 0.5230 - val_sequential_2_loss: 0.0215 - val_sequential_3_loss: 0.5015\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0669 - sequential_2_loss: 0.0154 - sequential_3_loss: 0.0514 - val_loss: 0.4395 - val_sequential_2_loss: 0.0205 - val_sequential_3_loss: 0.4190\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0590 - sequential_2_loss: 0.0151 - sequential_3_loss: 0.0438 - val_loss: 0.5028 - val_sequential_2_loss: 0.0211 - val_sequential_3_loss: 0.4818\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0556 - sequential_2_loss: 0.0151 - sequential_3_loss: 0.0405 - val_loss: 0.5627 - val_sequential_2_loss: 0.0205 - val_sequential_3_loss: 0.5422\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0557 - sequential_2_loss: 0.0151 - sequential_3_loss: 0.0406 - val_loss: 0.6051 - val_sequential_2_loss: 0.0215 - val_sequential_3_loss: 0.5836\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0510 - sequential_2_loss: 0.0148 - sequential_3_loss: 0.0361 - val_loss: 0.6710 - val_sequential_2_loss: 0.0235 - val_sequential_3_loss: 0.6475\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0459 - sequential_2_loss: 0.0148 - sequential_3_loss: 0.0311 - val_loss: 0.6029 - val_sequential_2_loss: 0.0175 - val_sequential_3_loss: 0.5855\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0445 - sequential_2_loss: 0.0145 - sequential_3_loss: 0.0299 - val_loss: 0.6907 - val_sequential_2_loss: 0.0204 - val_sequential_3_loss: 0.6703\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0424 - sequential_2_loss: 0.0145 - sequential_3_loss: 0.0279 - val_loss: 0.6725 - val_sequential_2_loss: 0.0205 - val_sequential_3_loss: 0.6520\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0435 - sequential_2_loss: 0.0145 - sequential_3_loss: 0.0290 - val_loss: 0.5766 - val_sequential_2_loss: 0.0210 - val_sequential_3_loss: 0.5556\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0432 - sequential_2_loss: 0.0144 - sequential_3_loss: 0.0287 - val_loss: 0.6018 - val_sequential_2_loss: 0.0198 - val_sequential_3_loss: 0.5820\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0454 - sequential_2_loss: 0.0144 - sequential_3_loss: 0.0310 - val_loss: 0.5658 - val_sequential_2_loss: 0.0189 - val_sequential_3_loss: 0.5469\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0400 - sequential_2_loss: 0.0141 - sequential_3_loss: 0.0259 - val_loss: 0.6437 - val_sequential_2_loss: 0.0217 - val_sequential_3_loss: 0.6220\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0440 - sequential_2_loss: 0.0142 - sequential_3_loss: 0.0297 - val_loss: 0.5526 - val_sequential_2_loss: 0.0179 - val_sequential_3_loss: 0.5347\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0379 - sequential_2_loss: 0.0141 - sequential_3_loss: 0.0238 - val_loss: 0.6926 - val_sequential_2_loss: 0.0188 - val_sequential_3_loss: 0.6738\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0335 - sequential_2_loss: 0.0138 - sequential_3_loss: 0.0197 - val_loss: 0.4661 - val_sequential_2_loss: 0.0182 - val_sequential_3_loss: 0.4479\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0338 - sequential_2_loss: 0.0138 - sequential_3_loss: 0.0200 - val_loss: 0.5449 - val_sequential_2_loss: 0.0192 - val_sequential_3_loss: 0.5257\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0361 - sequential_2_loss: 0.0138 - sequential_3_loss: 0.0223 - val_loss: 0.6150 - val_sequential_2_loss: 0.0178 - val_sequential_3_loss: 0.5972\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0384 - sequential_2_loss: 0.0138 - sequential_3_loss: 0.0247 - val_loss: 0.9049 - val_sequential_2_loss: 0.0170 - val_sequential_3_loss: 0.8878\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0330 - sequential_2_loss: 0.0136 - sequential_3_loss: 0.0194 - val_loss: 0.4829 - val_sequential_2_loss: 0.0168 - val_sequential_3_loss: 0.4661\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0352 - sequential_2_loss: 0.0136 - sequential_3_loss: 0.0217 - val_loss: 0.7110 - val_sequential_2_loss: 0.0178 - val_sequential_3_loss: 0.6932\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0348 - sequential_2_loss: 0.0136 - sequential_3_loss: 0.0213 - val_loss: 0.7022 - val_sequential_2_loss: 0.0175 - val_sequential_3_loss: 0.6847\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0310 - sequential_2_loss: 0.0135 - sequential_3_loss: 0.0175 - val_loss: 0.5292 - val_sequential_2_loss: 0.0159 - val_sequential_3_loss: 0.5132\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0297 - sequential_2_loss: 0.0132 - sequential_3_loss: 0.0165 - val_loss: 0.6501 - val_sequential_2_loss: 0.0163 - val_sequential_3_loss: 0.6338\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0353 - sequential_2_loss: 0.0135 - sequential_3_loss: 0.0218 - val_loss: 0.7967 - val_sequential_2_loss: 0.0187 - val_sequential_3_loss: 0.7780\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0342 - sequential_2_loss: 0.0134 - sequential_3_loss: 0.0208 - val_loss: 0.6184 - val_sequential_2_loss: 0.0187 - val_sequential_3_loss: 0.5997\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0284 - sequential_2_loss: 0.0131 - sequential_3_loss: 0.0154 - val_loss: 0.5376 - val_sequential_2_loss: 0.0155 - val_sequential_3_loss: 0.5221\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0302 - sequential_2_loss: 0.0131 - sequential_3_loss: 0.0170 - val_loss: 0.5579 - val_sequential_2_loss: 0.0191 - val_sequential_3_loss: 0.5388\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0277 - sequential_2_loss: 0.0131 - sequential_3_loss: 0.0146 - val_loss: 0.6243 - val_sequential_2_loss: 0.0182 - val_sequential_3_loss: 0.6061\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0320 - sequential_2_loss: 0.0131 - sequential_3_loss: 0.0189 - val_loss: 0.6638 - val_sequential_2_loss: 0.0170 - val_sequential_3_loss: 0.6468\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0251 - sequential_2_loss: 0.0127 - sequential_3_loss: 0.0123 - val_loss: 0.5305 - val_sequential_2_loss: 0.0151 - val_sequential_3_loss: 0.5154\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0253 - sequential_2_loss: 0.0127 - sequential_3_loss: 0.0126 - val_loss: 0.4782 - val_sequential_2_loss: 0.0173 - val_sequential_3_loss: 0.4609\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0278 - sequential_2_loss: 0.0127 - sequential_3_loss: 0.0151 - val_loss: 0.4965 - val_sequential_2_loss: 0.0172 - val_sequential_3_loss: 0.4793\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0267 - sequential_2_loss: 0.0127 - sequential_3_loss: 0.0140 - val_loss: 0.8003 - val_sequential_2_loss: 0.0175 - val_sequential_3_loss: 0.7828\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0320 - sequential_2_loss: 0.0128 - sequential_3_loss: 0.0193 - val_loss: 0.4702 - val_sequential_2_loss: 0.0184 - val_sequential_3_loss: 0.4518\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0226 - sequential_2_loss: 0.0125 - sequential_3_loss: 0.0101 - val_loss: 0.6106 - val_sequential_2_loss: 0.0165 - val_sequential_3_loss: 0.5941\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0388 - sequential_2_loss: 0.0134 - sequential_3_loss: 0.0255 - val_loss: 0.6680 - val_sequential_2_loss: 0.0211 - val_sequential_3_loss: 0.6469\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0305 - sequential_2_loss: 0.0130 - sequential_3_loss: 0.0174 - val_loss: 0.6025 - val_sequential_2_loss: 0.0174 - val_sequential_3_loss: 0.5851\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0274 - sequential_2_loss: 0.0128 - sequential_3_loss: 0.0147 - val_loss: 0.6292 - val_sequential_2_loss: 0.0163 - val_sequential_3_loss: 0.6129\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0244 - sequential_2_loss: 0.0126 - sequential_3_loss: 0.0118 - val_loss: 0.5724 - val_sequential_2_loss: 0.0157 - val_sequential_3_loss: 0.5567\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0209 - sequential_2_loss: 0.0123 - sequential_3_loss: 0.0086 - val_loss: 0.5939 - val_sequential_2_loss: 0.0159 - val_sequential_3_loss: 0.5779\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0215 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0093 - val_loss: 0.5721 - val_sequential_2_loss: 0.0149 - val_sequential_3_loss: 0.5572\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0251 - sequential_2_loss: 0.0123 - sequential_3_loss: 0.0128 - val_loss: 0.5226 - val_sequential_2_loss: 0.0148 - val_sequential_3_loss: 0.5078\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0224 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0102 - val_loss: 0.5117 - val_sequential_2_loss: 0.0172 - val_sequential_3_loss: 0.4945\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0243 - sequential_2_loss: 0.0123 - sequential_3_loss: 0.0121 - val_loss: 0.5614 - val_sequential_2_loss: 0.0152 - val_sequential_3_loss: 0.5462\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0245 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0124 - val_loss: 0.5116 - val_sequential_2_loss: 0.0155 - val_sequential_3_loss: 0.4961\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0237 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0115 - val_loss: 0.5631 - val_sequential_2_loss: 0.0174 - val_sequential_3_loss: 0.5457\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0269 - sequential_2_loss: 0.0123 - sequential_3_loss: 0.0146 - val_loss: 0.6170 - val_sequential_2_loss: 0.0159 - val_sequential_3_loss: 0.6010\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0263 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0141 - val_loss: 0.6245 - val_sequential_2_loss: 0.0194 - val_sequential_3_loss: 0.6051\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0212 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0092 - val_loss: 0.5753 - val_sequential_2_loss: 0.0144 - val_sequential_3_loss: 0.5609\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0253 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0133 - val_loss: 0.5695 - val_sequential_2_loss: 0.0167 - val_sequential_3_loss: 0.5528\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0235 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0115 - val_loss: 0.5306 - val_sequential_2_loss: 0.0156 - val_sequential_3_loss: 0.5150\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0218 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0098 - val_loss: 0.5445 - val_sequential_2_loss: 0.0153 - val_sequential_3_loss: 0.5291\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0232 - sequential_2_loss: 0.0119 - sequential_3_loss: 0.0113 - val_loss: 0.5648 - val_sequential_2_loss: 0.0142 - val_sequential_3_loss: 0.5505\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0257 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0137 - val_loss: 0.4802 - val_sequential_2_loss: 0.0153 - val_sequential_3_loss: 0.4649\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0219 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0101 - val_loss: 0.5510 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.5367\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0206 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0088 - val_loss: 0.5893 - val_sequential_2_loss: 0.0163 - val_sequential_3_loss: 0.5730\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0224 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0105 - val_loss: 0.5346 - val_sequential_2_loss: 0.0167 - val_sequential_3_loss: 0.5179\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0196 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0078 - val_loss: 0.5455 - val_sequential_2_loss: 0.0136 - val_sequential_3_loss: 0.5320\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0197 - sequential_2_loss: 0.0115 - sequential_3_loss: 0.0082 - val_loss: 0.5811 - val_sequential_2_loss: 0.0147 - val_sequential_3_loss: 0.5664\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0220 - sequential_2_loss: 0.0116 - sequential_3_loss: 0.0104 - val_loss: 0.5644 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.5502\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0190 - sequential_2_loss: 0.0114 - sequential_3_loss: 0.0076 - val_loss: 0.5066 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.4925\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0231 - sequential_2_loss: 0.0116 - sequential_3_loss: 0.0115 - val_loss: 0.6664 - val_sequential_2_loss: 0.0176 - val_sequential_3_loss: 0.6487\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0225 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0107 - val_loss: 0.5231 - val_sequential_2_loss: 0.0156 - val_sequential_3_loss: 0.5075\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0220 - sequential_2_loss: 0.0116 - sequential_3_loss: 0.0103 - val_loss: 0.6395 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.6252\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0076 - val_loss: 0.5329 - val_sequential_2_loss: 0.0157 - val_sequential_3_loss: 0.5171\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0200 - sequential_2_loss: 0.0114 - sequential_3_loss: 0.0087 - val_loss: 0.7007 - val_sequential_2_loss: 0.0179 - val_sequential_3_loss: 0.6828\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0217 - sequential_2_loss: 0.0116 - sequential_3_loss: 0.0101 - val_loss: 0.6516 - val_sequential_2_loss: 0.0140 - val_sequential_3_loss: 0.6376\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0183 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0070 - val_loss: 0.5996 - val_sequential_2_loss: 0.0149 - val_sequential_3_loss: 0.5847\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0183 - sequential_2_loss: 0.0112 - sequential_3_loss: 0.0071 - val_loss: 0.5658 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.5517\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0209 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0096 - val_loss: 0.5459 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.5316\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0192 - sequential_2_loss: 0.0112 - sequential_3_loss: 0.0080 - val_loss: 0.5349 - val_sequential_2_loss: 0.0137 - val_sequential_3_loss: 0.5211\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0178 - sequential_2_loss: 0.0112 - sequential_3_loss: 0.0067 - val_loss: 0.5524 - val_sequential_2_loss: 0.0140 - val_sequential_3_loss: 0.5384\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0079 - val_loss: 0.5486 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.5353\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0195 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0085 - val_loss: 0.5954 - val_sequential_2_loss: 0.0135 - val_sequential_3_loss: 0.5819\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0175 - sequential_2_loss: 0.0110 - sequential_3_loss: 0.0065 - val_loss: 0.6241 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.6095\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0201 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0090 - val_loss: 0.5728 - val_sequential_2_loss: 0.0136 - val_sequential_3_loss: 0.5592\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0195 - sequential_2_loss: 0.0112 - sequential_3_loss: 0.0083 - val_loss: 0.6290 - val_sequential_2_loss: 0.0163 - val_sequential_3_loss: 0.6127\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0221 - sequential_2_loss: 0.0114 - sequential_3_loss: 0.0107 - val_loss: 0.5243 - val_sequential_2_loss: 0.0161 - val_sequential_3_loss: 0.5082\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0167 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0057 - val_loss: 0.6341 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.6200\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0215 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0102 - val_loss: 0.5842 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.5696\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0184 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0073 - val_loss: 0.5852 - val_sequential_2_loss: 0.0142 - val_sequential_3_loss: 0.5710\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0224 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0112 - val_loss: 0.5730 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.5587\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0168 - sequential_2_loss: 0.0110 - sequential_3_loss: 0.0057 - val_loss: 0.5603 - val_sequential_2_loss: 0.0132 - val_sequential_3_loss: 0.5471\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0159 - sequential_2_loss: 0.0108 - sequential_3_loss: 0.0051 - val_loss: 0.6821 - val_sequential_2_loss: 0.0142 - val_sequential_3_loss: 0.6678\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0109 - sequential_3_loss: 0.0080 - val_loss: 0.5498 - val_sequential_2_loss: 0.0154 - val_sequential_3_loss: 0.5344\n",
      "Epoch 102/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0186 - sequential_2_loss: 0.0109 - sequential_3_loss: 0.0077 - val_loss: 0.5624 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.5486\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0175 - sequential_2_loss: 0.0108 - sequential_3_loss: 0.0067 - val_loss: 0.5938 - val_sequential_2_loss: 0.0136 - val_sequential_3_loss: 0.5802\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0172 - sequential_2_loss: 0.0107 - sequential_3_loss: 0.0065 - val_loss: 0.5308 - val_sequential_2_loss: 0.0134 - val_sequential_3_loss: 0.5174\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0161 - sequential_2_loss: 0.0106 - sequential_3_loss: 0.0055 - val_loss: 0.5073 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.4940\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0168 - sequential_2_loss: 0.0106 - sequential_3_loss: 0.0062 - val_loss: 0.5546 - val_sequential_2_loss: 0.0137 - val_sequential_3_loss: 0.5409\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0155 - sequential_2_loss: 0.0106 - sequential_3_loss: 0.0049 - val_loss: 0.5937 - val_sequential_2_loss: 0.0139 - val_sequential_3_loss: 0.5797\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0170 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0066 - val_loss: 0.5627 - val_sequential_2_loss: 0.0136 - val_sequential_3_loss: 0.5491\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0148 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0044 - val_loss: 0.5058 - val_sequential_2_loss: 0.0132 - val_sequential_3_loss: 0.4926\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0132 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0030 - val_loss: 0.5522 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5393\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0172 - sequential_2_loss: 0.0104 - sequential_3_loss: 0.0068 - val_loss: 0.7603 - val_sequential_2_loss: 0.0162 - val_sequential_3_loss: 0.7441\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0208 - sequential_2_loss: 0.0106 - sequential_3_loss: 0.0101 - val_loss: 0.5618 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.5485\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0158 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0054 - val_loss: 0.5166 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5040\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0164 - sequential_2_loss: 0.0104 - sequential_3_loss: 0.0060 - val_loss: 0.6233 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.6086\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0161 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0058 - val_loss: 0.6723 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.6595\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0144 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0042 - val_loss: 0.5214 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.5075\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0196 - sequential_2_loss: 0.0104 - sequential_3_loss: 0.0091 - val_loss: 0.6548 - val_sequential_2_loss: 0.0147 - val_sequential_3_loss: 0.6402\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0174 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0069 - val_loss: 0.5329 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.5196\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0170 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0066 - val_loss: 0.5979 - val_sequential_2_loss: 0.0142 - val_sequential_3_loss: 0.5838\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0166 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0060 - val_loss: 0.5993 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.5860\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0187 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0082 - val_loss: 0.6702 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.6560\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0147 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0044 - val_loss: 0.6276 - val_sequential_2_loss: 0.0131 - val_sequential_3_loss: 0.6145\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0147 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0044 - val_loss: 0.5482 - val_sequential_2_loss: 0.0130 - val_sequential_3_loss: 0.5353\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0152 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0049 - val_loss: 0.5241 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.5099\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0156 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0054 - val_loss: 0.5491 - val_sequential_2_loss: 0.0135 - val_sequential_3_loss: 0.5356\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0144 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0042 - val_loss: 0.5319 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5191\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0131 - sequential_2_loss: 0.0101 - sequential_3_loss: 0.0031 - val_loss: 0.6121 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5993\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0195 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0092 - val_loss: 0.6790 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.6651\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0154 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0050 - val_loss: 0.5453 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5324\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0139 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0039 - val_loss: 0.5930 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5804\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0141 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0042 - val_loss: 0.5653 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5526\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0158 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0058 - val_loss: 0.5486 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5361\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0152 - sequential_2_loss: 0.0099 - sequential_3_loss: 0.0052 - val_loss: 0.5600 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5474\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0169 - sequential_2_loss: 0.0101 - sequential_3_loss: 0.0068 - val_loss: 0.5043 - val_sequential_2_loss: 0.0130 - val_sequential_3_loss: 0.4912\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0148 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0048 - val_loss: 0.5748 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5619\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0155 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0055 - val_loss: 0.5413 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5287\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0152 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0052 - val_loss: 0.5278 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5151\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0127 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0029 - val_loss: 0.5610 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5485\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0141 - sequential_2_loss: 0.0097 - sequential_3_loss: 0.0043 - val_loss: 0.6536 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.6403\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0137 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0039 - val_loss: 0.5524 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5397\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0141 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0042 - val_loss: 0.5636 - val_sequential_2_loss: 0.0130 - val_sequential_3_loss: 0.5505\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0162 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0064 - val_loss: 0.5834 - val_sequential_2_loss: 0.0135 - val_sequential_3_loss: 0.5699\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0088 - val_loss: 0.5303 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.5165\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0150 - sequential_2_loss: 0.0099 - sequential_3_loss: 0.0051 - val_loss: 0.5592 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5463\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0152 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0054 - val_loss: 0.5706 - val_sequential_2_loss: 0.0134 - val_sequential_3_loss: 0.5571\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0138 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0040 - val_loss: 0.5361 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5236\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0139 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0042 - val_loss: 0.5891 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5766\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0157 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0060 - val_loss: 0.6308 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.6185\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0121 - sequential_2_loss: 0.0096 - sequential_3_loss: 0.0025 - val_loss: 0.5842 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5713\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0109 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0014 - val_loss: 0.5635 - val_sequential_2_loss: 0.0131 - val_sequential_3_loss: 0.5504\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0123 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0029 - val_loss: 0.5734 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5608\n",
      "Epoch 152/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0136 - sequential_2_loss: 0.0096 - sequential_3_loss: 0.0041 - val_loss: 0.5947 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5823\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0119 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0026 - val_loss: 0.5586 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5464\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0139 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0045 - val_loss: 0.6201 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.6074\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0130 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0036 - val_loss: 0.6005 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5883\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0118 - sequential_2_loss: 0.0093 - sequential_3_loss: 0.0025 - val_loss: 0.6215 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.6088\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0168 - sequential_2_loss: 0.0096 - sequential_3_loss: 0.0072 - val_loss: 0.7644 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.7498\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0092 - val_loss: 0.6063 - val_sequential_2_loss: 0.0132 - val_sequential_3_loss: 0.5931\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0118 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0023 - val_loss: 0.6516 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.6394\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0141 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0046 - val_loss: 0.6406 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.6277\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0129 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0035 - val_loss: 0.5623 - val_sequential_2_loss: 0.0123 - val_sequential_3_loss: 0.5500\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0129 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0035 - val_loss: 0.5747 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5624\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0116 - sequential_2_loss: 0.0093 - sequential_3_loss: 0.0023 - val_loss: 0.5646 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5521\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0137 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0044 - val_loss: 0.7728 - val_sequential_2_loss: 0.0144 - val_sequential_3_loss: 0.7583\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0140 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0045 - val_loss: 0.5525 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5398\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0134 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0039 - val_loss: 0.6232 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.6106\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0126 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0033 - val_loss: 0.6296 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.6169\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0142 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0048 - val_loss: 0.7057 - val_sequential_2_loss: 0.0139 - val_sequential_3_loss: 0.6918\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0142 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0046 - val_loss: 0.5904 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5776\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0116 - sequential_2_loss: 0.0093 - sequential_3_loss: 0.0023 - val_loss: 0.5375 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5257\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0091 - sequential_2_loss: 0.0089 - sequential_3_loss: 1.8041e-04 - val_loss: 0.5593 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5478\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0095 - sequential_2_loss: 0.0088 - sequential_3_loss: 7.3301e-04 - val_loss: 0.5700 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5585\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0092 - sequential_2_loss: 0.0087 - sequential_3_loss: 5.0524e-04 - val_loss: 0.5747 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5632\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0092 - sequential_2_loss: 0.0086 - sequential_3_loss: 5.7774e-04 - val_loss: 0.5605 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5491\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0094 - sequential_2_loss: 0.0086 - sequential_3_loss: 7.8082e-04 - val_loss: 0.6188 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.6069\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0139 - sequential_2_loss: 0.0089 - sequential_3_loss: 0.0050 - val_loss: 0.8410 - val_sequential_2_loss: 0.0153 - val_sequential_3_loss: 0.8257\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0236 - sequential_2_loss: 0.0099 - sequential_3_loss: 0.0137 - val_loss: 0.5795 - val_sequential_2_loss: 0.0135 - val_sequential_3_loss: 0.5660\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0161 - sequential_2_loss: 0.0097 - sequential_3_loss: 0.0064 - val_loss: 0.5360 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.5214\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0130 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0036 - val_loss: 0.5902 - val_sequential_2_loss: 0.0123 - val_sequential_3_loss: 0.5778\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0166 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0071 - val_loss: 0.5645 - val_sequential_2_loss: 0.0137 - val_sequential_3_loss: 0.5508\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0134 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0040 - val_loss: 0.6121 - val_sequential_2_loss: 0.0130 - val_sequential_3_loss: 0.5992\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0141 - sequential_2_loss: 0.0093 - sequential_3_loss: 0.0048 - val_loss: 0.5434 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5307\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0123 - sequential_2_loss: 0.0092 - sequential_3_loss: 0.0032 - val_loss: 0.5731 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5606\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0129 - sequential_2_loss: 0.0091 - sequential_3_loss: 0.0038 - val_loss: 0.5405 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5283\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0113 - sequential_2_loss: 0.0090 - sequential_3_loss: 0.0023 - val_loss: 0.5107 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.4988\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0106 - sequential_2_loss: 0.0088 - sequential_3_loss: 0.0018 - val_loss: 0.5409 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5294\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0101 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0014 - val_loss: 0.5250 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5134\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0097 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0011 - val_loss: 0.5607 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5492\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0107 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0021 - val_loss: 0.5374 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5256\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0102 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0016 - val_loss: 0.5633 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5515\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0102 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0017 - val_loss: 0.5406 - val_sequential_2_loss: 0.0120 - val_sequential_3_loss: 0.5286\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0087 - sequential_2_loss: 0.0084 - sequential_3_loss: 3.3298e-04 - val_loss: 0.5639 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5525\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0083 - sequential_2_loss: 0.0082 - sequential_3_loss: 5.6212e-05 - val_loss: 0.5544 - val_sequential_2_loss: 0.0111 - val_sequential_3_loss: 0.5434\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0081 - sequential_2_loss: 0.0081 - sequential_3_loss: 2.0117e-05 - val_loss: 0.5651 - val_sequential_2_loss: 0.0110 - val_sequential_3_loss: 0.5541\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0080 - sequential_2_loss: 0.0080 - sequential_3_loss: 1.1776e-05 - val_loss: 0.5704 - val_sequential_2_loss: 0.0110 - val_sequential_3_loss: 0.5594\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0079 - sequential_2_loss: 0.0079 - sequential_3_loss: 9.4638e-06 - val_loss: 0.5766 - val_sequential_2_loss: 0.0110 - val_sequential_3_loss: 0.5656\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0078 - sequential_2_loss: 0.0078 - sequential_3_loss: 6.7179e-06 - val_loss: 0.5810 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5701\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0078 - sequential_2_loss: 0.0078 - sequential_3_loss: 9.3755e-06 - val_loss: 0.5870 - val_sequential_2_loss: 0.0112 - val_sequential_3_loss: 0.5758\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0077 - sequential_2_loss: 0.0077 - sequential_3_loss: 4.7702e-06 - val_loss: 0.5898 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5789\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0077 - sequential_2_loss: 0.0077 - sequential_3_loss: 4.9219e-06 - val_loss: 0.5937 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5829\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0076 - sequential_2_loss: 0.0076 - sequential_3_loss: 5.5916e-06 - val_loss: 0.5983 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5874\n",
      "Epoch 202/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0076 - sequential_2_loss: 0.0076 - sequential_3_loss: 4.2773e-06 - val_loss: 0.6017 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5909\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0075 - sequential_2_loss: 0.0075 - sequential_3_loss: 3.7602e-06 - val_loss: 0.6041 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5934\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0075 - sequential_2_loss: 0.0074 - sequential_3_loss: 4.6105e-06 - val_loss: 0.6077 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5969\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0074 - sequential_2_loss: 0.0074 - sequential_3_loss: 2.8543e-06 - val_loss: 0.6106 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5999\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0074 - sequential_2_loss: 0.0074 - sequential_3_loss: 2.7488e-06 - val_loss: 0.6139 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6032\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0073 - sequential_2_loss: 0.0073 - sequential_3_loss: 2.7892e-06 - val_loss: 0.6173 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6066\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0073 - sequential_2_loss: 0.0073 - sequential_3_loss: 2.7966e-06 - val_loss: 0.6203 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6095\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0073 - sequential_2_loss: 0.0073 - sequential_3_loss: 2.4474e-06 - val_loss: 0.6238 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6132\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0072 - sequential_2_loss: 0.0072 - sequential_3_loss: 2.0360e-06 - val_loss: 0.6268 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6162\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0072 - sequential_2_loss: 0.0072 - sequential_3_loss: 2.0146e-06 - val_loss: 0.6290 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6185\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0423 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0329 - val_loss: 0.6434 - val_sequential_2_loss: 0.0193 - val_sequential_3_loss: 0.6241\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0279 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0177 - val_loss: 0.4481 - val_sequential_2_loss: 0.0139 - val_sequential_3_loss: 0.4341\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0174 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0079 - val_loss: 0.5023 - val_sequential_2_loss: 0.0131 - val_sequential_3_loss: 0.4892\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0132 - sequential_2_loss: 0.0091 - sequential_3_loss: 0.0041 - val_loss: 0.5516 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5390\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0134 - sequential_2_loss: 0.0090 - sequential_3_loss: 0.0045 - val_loss: 0.5762 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5637\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0136 - sequential_2_loss: 0.0090 - sequential_3_loss: 0.0046 - val_loss: 0.5661 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5539\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0025 - val_loss: 0.5427 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5305\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0108 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0021 - val_loss: 0.5696 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5571\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0128 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0041 - val_loss: 0.6002 - val_sequential_2_loss: 0.0152 - val_sequential_3_loss: 0.5850\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0166 - sequential_2_loss: 0.0090 - sequential_3_loss: 0.0076 - val_loss: 0.5694 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5567\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0106 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0021 - val_loss: 0.5042 - val_sequential_2_loss: 0.0116 - val_sequential_3_loss: 0.4926\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0092 - sequential_2_loss: 0.0083 - sequential_3_loss: 9.0965e-04 - val_loss: 0.5533 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5419\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0090 - sequential_2_loss: 0.0081 - sequential_3_loss: 8.8049e-04 - val_loss: 0.5653 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5539\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0089 - sequential_2_loss: 0.0081 - sequential_3_loss: 8.0146e-04 - val_loss: 0.5407 - val_sequential_2_loss: 0.0111 - val_sequential_3_loss: 0.5296\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0081 - sequential_2_loss: 0.0079 - sequential_3_loss: 1.3958e-04 - val_loss: 0.5651 - val_sequential_2_loss: 0.0111 - val_sequential_3_loss: 0.5540\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0080 - sequential_2_loss: 0.0078 - sequential_3_loss: 1.9056e-04 - val_loss: 0.6175 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.6061\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0126 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0045 - val_loss: 0.5404 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5275\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0144 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0058 - val_loss: 0.5866 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5738\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0105 - sequential_2_loss: 0.0083 - sequential_3_loss: 0.0023 - val_loss: 0.5476 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5355\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0124 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0039 - val_loss: 0.6859 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.6727\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0134 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0046 - val_loss: 0.6451 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.6326\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0124 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0037 - val_loss: 0.5414 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5290\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0027 - val_loss: 0.5660 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5537\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0113 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0028 - val_loss: 0.5723 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5601\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0116 - sequential_2_loss: 0.0084 - sequential_3_loss: 0.0032 - val_loss: 0.5515 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.5396\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0092 - sequential_2_loss: 0.0082 - sequential_3_loss: 9.8135e-04 - val_loss: 0.5443 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5330\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0099 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0019 - val_loss: 0.5563 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5448\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0086 - sequential_2_loss: 0.0079 - sequential_3_loss: 7.0877e-04 - val_loss: 0.5566 - val_sequential_2_loss: 0.0113 - val_sequential_3_loss: 0.5453\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0101 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0021 - val_loss: 0.6235 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.6109\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0137 - sequential_2_loss: 0.0084 - sequential_3_loss: 0.0054 - val_loss: 0.6530 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.6392\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0133 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0048 - val_loss: 0.5318 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5197\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0087 - sequential_2_loss: 0.0081 - sequential_3_loss: 6.6954e-04 - val_loss: 0.5341 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5226\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0093 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0013 - val_loss: 0.5492 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5375\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0088 - sequential_2_loss: 0.0079 - sequential_3_loss: 8.5125e-04 - val_loss: 0.5832 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5716\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0083 - sequential_2_loss: 0.0078 - sequential_3_loss: 5.5182e-04 - val_loss: 0.5668 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5553\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0112 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0032 - val_loss: 0.6340 - val_sequential_2_loss: 0.0148 - val_sequential_3_loss: 0.6191\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0139 - sequential_2_loss: 0.0083 - sequential_3_loss: 0.0056 - val_loss: 0.6743 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.6619\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0118 - sequential_2_loss: 0.0084 - sequential_3_loss: 0.0034 - val_loss: 0.5688 - val_sequential_2_loss: 0.0120 - val_sequential_3_loss: 0.5568\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0092 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0011 - val_loss: 0.6091 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5974\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0095 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0014 - val_loss: 0.5832 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5715\n",
      "Epoch 252/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0109 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0029 - val_loss: 0.6644 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.6517\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0031 - val_loss: 0.5416 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5299\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0111 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0030 - val_loss: 0.6482 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.6353\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0118 - sequential_2_loss: 0.0083 - sequential_3_loss: 0.0035 - val_loss: 0.7581 - val_sequential_2_loss: 0.0120 - val_sequential_3_loss: 0.7460\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0110 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0028 - val_loss: 0.5813 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5692\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0115 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0033 - val_loss: 0.6260 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.6141\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0104 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0023 - val_loss: 0.6480 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.6356\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0030 - val_loss: 0.5717 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5596\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0119 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0036 - val_loss: 0.5712 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.5592\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0090 - sequential_2_loss: 0.0080 - sequential_3_loss: 9.8798e-04 - val_loss: 0.5565 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5447\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0094 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0014 - val_loss: 0.5956 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5839\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0021 - val_loss: 0.6081 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5956\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0116 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0035 - val_loss: 0.6076 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5955\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0107 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0026 - val_loss: 0.5685 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5567\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0098 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0018 - val_loss: 0.5681 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5564\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0021 - val_loss: 0.5524 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5409\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0092 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0013 - val_loss: 0.6633 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.6516\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0093 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0015 - val_loss: 0.5812 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5691\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0095 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0017 - val_loss: 0.6354 - val_sequential_2_loss: 0.0116 - val_sequential_3_loss: 0.6238\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0022 - val_loss: 0.6000 - val_sequential_2_loss: 0.0120 - val_sequential_3_loss: 0.5880\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0020 - val_loss: 0.6035 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5911\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0104 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0025 - val_loss: 0.6190 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.6071\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0137 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0055 - val_loss: 0.5966 - val_sequential_2_loss: 0.0132 - val_sequential_3_loss: 0.5834\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0123 - sequential_2_loss: 0.0083 - sequential_3_loss: 0.0040 - val_loss: 0.5490 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5369\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0098 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0018 - val_loss: 0.5621 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5504\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0084 - sequential_2_loss: 0.0078 - sequential_3_loss: 5.7593e-04 - val_loss: 0.5623 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5510\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0079 - sequential_2_loss: 0.0076 - sequential_3_loss: 2.8560e-04 - val_loss: 0.5479 - val_sequential_2_loss: 0.0111 - val_sequential_3_loss: 0.5368\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0079 - sequential_2_loss: 0.0075 - sequential_3_loss: 4.2247e-04 - val_loss: 0.5849 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5725\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0079 - sequential_2_loss: 0.0076 - sequential_3_loss: 3.4379e-04 - val_loss: 0.5623 - val_sequential_2_loss: 0.0112 - val_sequential_3_loss: 0.5510\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0075 - sequential_2_loss: 0.0074 - sequential_3_loss: 7.7024e-05 - val_loss: 0.5742 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5633\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0072 - sequential_2_loss: 0.0072 - sequential_3_loss: 2.2197e-05 - val_loss: 0.5844 - val_sequential_2_loss: 0.0110 - val_sequential_3_loss: 0.5734\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0094 - sequential_2_loss: 0.0075 - sequential_3_loss: 0.0018 - val_loss: 0.6038 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5912\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0138 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0057 - val_loss: 0.6053 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5925\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0146 - sequential_2_loss: 0.0084 - sequential_3_loss: 0.0062 - val_loss: 0.5731 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5607\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0102 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0022 - val_loss: 0.5620 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5495\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0090 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0012 - val_loss: 0.6250 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.6135\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0090 - sequential_2_loss: 0.0077 - sequential_3_loss: 0.0013 - val_loss: 0.5873 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5748\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0093 - sequential_2_loss: 0.0077 - sequential_3_loss: 0.0016 - val_loss: 0.5831 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5715\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0097 - sequential_2_loss: 0.0077 - sequential_3_loss: 0.0020 - val_loss: 0.6246 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.6129\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0104 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0026 - val_loss: 0.5863 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5746\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0091 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0014 - val_loss: 0.5689 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5570\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0077 - sequential_3_loss: 0.0024 - val_loss: 0.5949 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5832\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0119 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0041 - val_loss: 0.5886 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.5767\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0103 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0024 - val_loss: 0.5748 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5623\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0082 - sequential_2_loss: 0.0077 - sequential_3_loss: 5.6915e-04 - val_loss: 0.5727 - val_sequential_2_loss: 0.0112 - val_sequential_3_loss: 0.5614\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0081 - sequential_2_loss: 0.0075 - sequential_3_loss: 5.6881e-04 - val_loss: 0.6130 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.6016\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0145 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0063 - val_loss: 0.5931 - val_sequential_2_loss: 0.0140 - val_sequential_3_loss: 0.5792\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0111 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0031 - val_loss: 0.5565 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5447\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0087 - sequential_2_loss: 0.0077 - sequential_3_loss: 9.5909e-04 - val_loss: 0.5444 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5328\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0078 - sequential_2_loss: 0.0075 - sequential_3_loss: 2.3723e-04 - val_loss: 0.5600 - val_sequential_2_loss: 0.0112 - val_sequential_3_loss: 0.5488\n",
      "Epoch 302/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0072 - sequential_2_loss: 0.0071 - sequential_3_loss: 4.6609e-05 - val_loss: 0.5630 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5521\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0070 - sequential_2_loss: 0.0070 - sequential_3_loss: 2.3492e-05 - val_loss: 0.5688 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5579\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0070 - sequential_2_loss: 0.0069 - sequential_3_loss: 2.5248e-05 - val_loss: 0.5724 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5615\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0069 - sequential_2_loss: 0.0069 - sequential_3_loss: 1.2046e-05 - val_loss: 0.5755 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5646\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0068 - sequential_2_loss: 0.0068 - sequential_3_loss: 1.1231e-05 - val_loss: 0.5789 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5680\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0068 - sequential_2_loss: 0.0068 - sequential_3_loss: 1.2285e-05 - val_loss: 0.5822 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5714\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0068 - sequential_2_loss: 0.0067 - sequential_3_loss: 9.2982e-06 - val_loss: 0.5857 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5749\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0067 - sequential_2_loss: 0.0067 - sequential_3_loss: 7.0059e-06 - val_loss: 0.5886 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5777\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0067 - sequential_2_loss: 0.0067 - sequential_3_loss: 8.0532e-06 - val_loss: 0.5915 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5806\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0067 - sequential_2_loss: 0.0066 - sequential_3_loss: 7.6544e-06 - val_loss: 0.5939 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5831\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0066 - sequential_2_loss: 0.0066 - sequential_3_loss: 8.7364e-06 - val_loss: 0.5957 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5849\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0066 - sequential_2_loss: 0.0066 - sequential_3_loss: 5.3690e-06 - val_loss: 0.5998 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5889\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0066 - sequential_2_loss: 0.0066 - sequential_3_loss: 5.6121e-06 - val_loss: 0.6023 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5914\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 4.7386e-06 - val_loss: 0.6059 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5951\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 3.9149e-06 - val_loss: 0.6068 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5960\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 3.9243e-06 - val_loss: 0.6093 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5985\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 4.5792e-06 - val_loss: 0.6115 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6007\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 3.6335e-06 - val_loss: 0.6129 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6021\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0064 - sequential_3_loss: 4.5179e-06 - val_loss: 0.6146 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6039\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0064 - sequential_3_loss: 2.8128e-06 - val_loss: 0.6168 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6060\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0064 - sequential_3_loss: 3.1204e-06 - val_loss: 0.6184 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6077\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0064 - sequential_3_loss: 2.0821e-06 - val_loss: 0.6203 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6095\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0063 - sequential_3_loss: 3.2380e-06 - val_loss: 0.6222 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6114\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0063 - sequential_2_loss: 0.0063 - sequential_3_loss: 2.2412e-06 - val_loss: 0.6236 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6129\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0063 - sequential_2_loss: 0.0063 - sequential_3_loss: 2.6344e-06 - val_loss: 0.6243 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6136\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0063 - sequential_2_loss: 0.0063 - sequential_3_loss: 1.7711e-06 - val_loss: 0.6259 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6152\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0063 - sequential_2_loss: 0.0063 - sequential_3_loss: 1.6678e-06 - val_loss: 0.6285 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6178\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.3982e-06 - val_loss: 0.6293 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6186\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.3660e-06 - val_loss: 0.6307 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6200\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.6672e-06 - val_loss: 0.6327 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6220\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.3871e-06 - val_loss: 0.6355 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6248\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.5003e-06 - val_loss: 0.6357 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6250\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.5035e-06 - val_loss: 0.6370 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6263\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.7968e-06 - val_loss: 0.6402 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6295\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 1.1754e-06 - val_loss: 0.6421 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6314\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 1.1374e-06 - val_loss: 0.6423 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6316\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 1.1464e-06 - val_loss: 0.6456 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6348\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 9.5596e-07 - val_loss: 0.6470 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6364\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 1.2579e-06 - val_loss: 0.6483 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6376\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 1.1431e-06 - val_loss: 0.6508 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6402\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 1.2381e-06 - val_loss: 0.6517 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6409\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 6.9518e-07 - val_loss: 0.6535 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6428\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 7.0691e-07 - val_loss: 0.6553 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6446\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 9.3701e-07 - val_loss: 0.6581 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6475\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 6.6599e-07 - val_loss: 0.6591 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6485\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 9.0085e-07 - val_loss: 0.6607 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6500\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 6.8133e-07 - val_loss: 0.6606 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6500\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.1628e-07 - val_loss: 0.6626 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6520\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.9769e-07 - val_loss: 0.6638 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6532\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.9159e-07 - val_loss: 0.6665 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6559\n",
      "Epoch 352/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.7032e-07 - val_loss: 0.6671 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6565\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.5474e-07 - val_loss: 0.6679 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6574\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 9.5099e-07 - val_loss: 0.6718 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6612\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 2.4214e-06 - val_loss: 0.6767 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6661\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.8651e-07 - val_loss: 0.6759 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6652\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 4.3072e-07 - val_loss: 0.6794 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6689\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.8095e-07 - val_loss: 0.6808 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6702\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 4.6020e-07 - val_loss: 0.6816 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6711\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.9991e-07 - val_loss: 0.6828 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6722\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.2179e-07 - val_loss: 0.6836 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6731\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 1.2255e-06 - val_loss: 0.6892 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6787\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 3.5782e-07 - val_loss: 0.6887 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6782\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 4.2305e-07 - val_loss: 0.6914 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6809\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 1.1617e-06 - val_loss: 0.6870 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6765\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 4.7789e-07 - val_loss: 0.6913 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6808\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 3.3312e-07 - val_loss: 0.6916 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6811\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 4.8851e-07 - val_loss: 0.6953 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6847\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 3.1631e-07 - val_loss: 0.6952 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6847\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 3.4897e-07 - val_loss: 0.6944 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6838\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0060 - sequential_3_loss: 0.0052 - val_loss: 1.3274 - val_sequential_2_loss: 0.0167 - val_sequential_3_loss: 1.3106\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0197 - sequential_2_loss: 0.0076 - sequential_3_loss: 0.0121 - val_loss: 0.6067 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5945\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0075 - sequential_2_loss: 0.0068 - sequential_3_loss: 7.1446e-04 - val_loss: 0.5776 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5669\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0066 - sequential_2_loss: 0.0065 - sequential_3_loss: 1.3905e-04 - val_loss: 0.5738 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5631\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0063 - sequential_3_loss: 4.9595e-05 - val_loss: 0.5760 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.5653\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 3.9308e-05 - val_loss: 0.5844 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.5737\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 2.0888e-05 - val_loss: 0.5914 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.5807\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 1.8940e-05 - val_loss: 0.5972 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.5866\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0060 - sequential_3_loss: 3.0565e-05 - val_loss: 0.6074 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.5968\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0067 - sequential_2_loss: 0.0061 - sequential_3_loss: 6.1058e-04 - val_loss: 0.6071 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5963\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0061 - sequential_3_loss: 9.0929e-05 - val_loss: 0.6096 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.5990\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 1.6141e-05 - val_loss: 0.6130 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6023\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 1.0256e-05 - val_loss: 0.6154 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6048\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 8.4288e-06 - val_loss: 0.6169 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6062\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 6.5033e-06 - val_loss: 0.6198 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6093\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 6.2999e-06 - val_loss: 0.6245 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6140\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 4.4156e-06 - val_loss: 0.6260 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6154\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.8283e-06 - val_loss: 0.6279 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6173\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 5.9792e-06 - val_loss: 0.6304 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6198\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 4.2464e-06 - val_loss: 0.6306 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6200\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 2.9966e-06 - val_loss: 0.6331 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6226\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 2.7570e-06 - val_loss: 0.6355 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6250\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 2.5839e-06 - val_loss: 0.6362 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6257\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 3.2101e-06 - val_loss: 0.6387 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6281\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 2.3526e-06 - val_loss: 0.6415 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6310\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 2.4819e-06 - val_loss: 0.6435 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6330\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 2.3960e-06 - val_loss: 0.6453 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6347\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 1.9576e-06 - val_loss: 0.6472 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6366\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 1.6395e-06 - val_loss: 0.6483 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6377\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 2.1768e-06 - val_loss: 0.6511 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6406\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 1.0379e-06 - val_loss: 0.6524 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6418\n",
      "Epoch 402/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 1.3387e-06 - val_loss: 0.6529 - val_sequential_2_loss: 0.0104 - val_sequential_3_loss: 0.6425\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 9.8789e-07 - val_loss: 0.6535 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6430\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 9.7763e-07 - val_loss: 0.6537 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6432\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 1.3820e-06 - val_loss: 0.6539 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6435\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 1.3159e-06 - val_loss: 0.6547 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6442\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.0681e-06 - val_loss: 0.6552 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6447\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1855e-06 - val_loss: 0.6551 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6446\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1801e-06 - val_loss: 0.6550 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6446\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.0478e-06 - val_loss: 0.6556 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6451\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1652e-06 - val_loss: 0.6557 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6452\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1272e-06 - val_loss: 0.6568 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6463\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.1459e-07 - val_loss: 0.6570 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6465\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 9.5783e-07 - val_loss: 0.6574 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6469\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.0860e-06 - val_loss: 0.6574 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6469\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1319e-06 - val_loss: 0.6580 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6475\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1358e-06 - val_loss: 0.6588 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6483\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1419e-06 - val_loss: 0.6593 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6488\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.7675e-07 - val_loss: 0.6596 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6490\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1008e-06 - val_loss: 0.6589 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6484\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 7.8700e-07 - val_loss: 0.6591 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6486\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.3965e-06 - val_loss: 0.6612 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6507\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.6805e-06 - val_loss: 0.6610 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6505\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1032e-06 - val_loss: 0.6612 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6507\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.6531e-07 - val_loss: 0.6616 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6511\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.7733e-06 - val_loss: 0.6650 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6545\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 6.6353e-07 - val_loss: 0.6673 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6568\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.8759e-06 - val_loss: 0.6664 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6559\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.5732e-07 - val_loss: 0.6663 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6558\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.2956e-06 - val_loss: 0.6660 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6555\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.4684e-07 - val_loss: 0.6671 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6566\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.4270e-07 - val_loss: 0.6683 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6577\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 6.5600e-07 - val_loss: 0.6688 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6583\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 1.1289e-06 - val_loss: 0.6689 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6584\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0052 - sequential_3_loss: 6.8188e-07 - val_loss: 0.6694 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6589\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.6840e-07 - val_loss: 0.6690 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6585\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.3096e-07 - val_loss: 0.6699 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6594\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 6.3449e-07 - val_loss: 0.6707 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6602\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 6.5494e-07 - val_loss: 0.6698 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6593\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 8.0674e-07 - val_loss: 0.6723 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6618\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.2705e-07 - val_loss: 0.6722 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6617\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.0396e-07 - val_loss: 0.6730 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6625\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 7.2826e-07 - val_loss: 0.6741 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6636\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.8858e-07 - val_loss: 0.6757 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6652\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.7505e-07 - val_loss: 0.6751 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6646\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.2183e-07 - val_loss: 0.6772 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6667\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.7246e-07 - val_loss: 0.6770 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6664\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.4022e-07 - val_loss: 0.6781 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6675\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.8754e-07 - val_loss: 0.6786 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6681\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.5045e-07 - val_loss: 0.6777 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6672\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.4478e-07 - val_loss: 0.6794 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6689\n",
      "Epoch 452/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 7.0418e-07 - val_loss: 0.6786 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6681\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.9408e-07 - val_loss: 0.6797 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6692\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.3983e-07 - val_loss: 0.6823 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6718\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.1970e-07 - val_loss: 0.6823 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6718\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.3040e-07 - val_loss: 0.6819 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6714\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.9412e-07 - val_loss: 0.6846 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6741\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.6278e-07 - val_loss: 0.6854 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6748\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.8500e-07 - val_loss: 0.6852 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6747\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.7521e-07 - val_loss: 0.6860 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6755\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.4177e-07 - val_loss: 0.6880 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6775\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.9118e-07 - val_loss: 0.6890 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6785\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.3315e-07 - val_loss: 0.6889 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6783\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.7901e-07 - val_loss: 0.6914 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6809\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.8199e-07 - val_loss: 0.6902 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6797\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.0565e-07 - val_loss: 0.6910 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6805\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.4714e-07 - val_loss: 0.6937 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6832\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.7777e-07 - val_loss: 0.6946 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6841\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.2309e-07 - val_loss: 0.6977 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6872\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.9684e-07 - val_loss: 0.6957 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6852\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.4985e-07 - val_loss: 0.6980 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6875\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5996e-07 - val_loss: 0.6988 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6883\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.9574e-07 - val_loss: 0.6987 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6881\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.5883e-07 - val_loss: 0.7003 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6898\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.0040e-07 - val_loss: 0.7016 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6911\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 4.3966e-07 - val_loss: 0.7029 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6924\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.4118e-07 - val_loss: 0.7051 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6946\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.6710e-07 - val_loss: 0.7056 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6951\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5133e-07 - val_loss: 0.7040 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6935\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5640e-07 - val_loss: 0.7066 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6961\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5972e-07 - val_loss: 0.7088 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6983\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.2190e-07 - val_loss: 0.7101 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6996\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.4388e-07 - val_loss: 0.7114 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7008\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.2671e-07 - val_loss: 0.7097 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6992\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5828e-07 - val_loss: 0.7149 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7043\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.2736e-07 - val_loss: 0.7142 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7036\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.5816e-07 - val_loss: 0.7157 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7052\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.0553e-07 - val_loss: 0.7178 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7072\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.0634e-07 - val_loss: 0.7178 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7073\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.3393e-07 - val_loss: 0.7185 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7079\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.1669e-07 - val_loss: 0.7199 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7093\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.9895e-07 - val_loss: 0.7200 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7094\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.1824e-07 - val_loss: 0.7218 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7113\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.2741e-07 - val_loss: 0.7235 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7130\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.9093e-07 - val_loss: 0.7244 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7138\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.7709e-07 - val_loss: 0.7254 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7148\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.9887e-07 - val_loss: 0.7269 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7164\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.9520e-07 - val_loss: 0.7281 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7176\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0050 - sequential_2_loss: 0.0050 - sequential_3_loss: 1.9565e-07 - val_loss: 0.7290 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7185\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0050 - sequential_2_loss: 0.0050 - sequential_3_loss: 1.9544e-07 - val_loss: 0.7308 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e2b239e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x=x_train,\n",
    "                y=[x_train,y_train],\n",
    "                epochs=num_epochs, batch_size=batch_size, shuffle=True,\n",
    "                validation_data=[x_test, [x_test, y_test]],\n",
    "                callbacks=[tensorboard, LearningRateScheduler(lr_scheduler)],\n",
    "                verbose=1,\n",
    "                initial_epoch= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:09.293337Z",
     "start_time": "2019-01-13T05:03:05.582422Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yag.send('mizutaninikkou@gmail.com', subject = \"Training Done\", contents='Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:11.575505Z",
     "start_time": "2019-01-13T05:03:09.294487Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autoencoder.save(F'{model_path}autoencoder{autoencoder_id}.h5')\n",
    "encoder.save(F'{model_path}encoder{autoencoder_id}.h5')\n",
    "decoder.save(F'{model_path}decoder{autoencoder_id}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:11.579272Z",
     "start_time": "2019-01-13T05:03:11.576490Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:16.912647Z",
     "start_time": "2019-01-13T05:03:11.580321Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicoroble/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/keras/models.py:252: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "autoencoder = load_model(F'{model_path}autoencoder{autoencoder_id}.h5')\n",
    "encoder = load_model(F'{model_path}encoder{autoencoder_id}.h5')\n",
    "decoder = load_model(F'{model_path}decoder{autoencoder_id}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:17.049711Z",
     "start_time": "2019-01-13T05:03:16.913638Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code = encoder.predict(x_test[0][None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:17.186307Z",
     "start_time": "2019-01-13T05:03:17.050747Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reco = decoder.predict(code)[0].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:17.264132Z",
     "start_time": "2019-01-13T05:03:17.187457Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8d84c51668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e1030dc18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:17.342833Z",
     "start_time": "2019-01-13T05:03:17.265340Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8d838b9b70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8d837db710>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
