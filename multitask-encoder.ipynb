{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:58.012495Z",
     "start_time": "2019-01-14T12:12:56.993013Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicoroble/anaconda3/envs/tensorflow-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras, keras.layers as L, keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from keras.models import save_model\n",
    "import keras\n",
    "from keras.datasets import cifar10, fashion_mnist\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import UpSampling2D, Reshape, Dense,Activation, Flatten, Dropout, BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, InputLayer\n",
    "from keras.layers import Merge\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:58.250413Z",
     "start_time": "2019-01-14T12:12:58.013970Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bit_size = 32\n",
    "num_epochs = 500\n",
    "batch_size = 500\n",
    "autoencoder_id='multitaskfinal'\n",
    "model_path = 'fashion-models/'\n",
    "log_path = F'fashion-logs/autoencoder-{autoencoder_id}'\n",
    "tensorboard = TensorBoard(log_dir=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:58.317330Z",
     "start_time": "2019-01-14T12:12:58.252148Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yagmail\n",
    "email = os.environ.get('GMAIL')\n",
    "pswd = os.environ.get('GMAILPASS')\n",
    "yag = yagmail.SMTP(email, pswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:58.323057Z",
     "start_time": "2019-01-14T12:12:58.318711Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSaveCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, file_name):\n",
    "        super(ModelSaveCallback, self).__init__()\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model_filename = self.file_name.format(epoch)\n",
    "        save_model(self.model, model_filename)\n",
    "        print(\"Model saved in {}\".format(model_filename))\n",
    "\n",
    "\n",
    "# !!! remember to clear session/graph if you rebuild your graph to avoid out-of-memory errors !!!\n",
    "def reset_tf_session():\n",
    "    curr_session = tf.get_default_session()\n",
    "    # close current session\n",
    "    if curr_session is not None:\n",
    "        curr_session.close()\n",
    "    # reset graph\n",
    "    K.clear_session()\n",
    "    # create new session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    s = tf.InteractiveSession(config=config)\n",
    "    K.set_session(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:58.628474Z",
     "start_time": "2019-01-14T12:12:58.324353Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:58.632440Z",
     "start_time": "2019-01-14T12:12:58.629789Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: (60000, 28, 28) (60000,)\n",
      "Test samples: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train samples:\", x_train.shape, y_train.shape)\n",
    "print(\"Test samples:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:58.785776Z",
     "start_time": "2019-01-14T12:12:58.780925Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "cifar10_classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "                   \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:59.699178Z",
     "start_time": "2019-01-14T12:12:59.168273Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05340b67b8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# show random images from train\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(y_train))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(x_train[random_index, :])\n",
    "#         ax.set_title(cifar10_classes[y_train[random_index, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:59.779930Z",
     "start_time": "2019-01-14T12:12:59.700888Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0530538668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05305ec860>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:12:59.982641Z",
     "start_time": "2019-01-14T12:12:59.869010Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape([-1, 28, 28, 1])\n",
    "x_test = x_test.reshape([-1, 28, 28, 1])\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "IMG_SHAPE = x_train.shape[1:]\n",
    "\n",
    "# center images\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:01.074784Z",
     "start_time": "2019-01-14T12:13:01.057348Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_encoder(img_shape, code_size):\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # encoder\n",
    "    encoder = Sequential()\n",
    "    encoder.add(InputLayer(img_shape))\n",
    "    \n",
    "    encoder.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    encoder.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    encoder.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "\n",
    "    encoder.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "    encoder.add(BatchNormalization())\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    encoder.add(Flatten())                  #flatten image to vector\n",
    "    \n",
    "    encoder.add(Dense(1024))\n",
    "    encoder.add((BatchNormalization()))\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(Dense(1024))\n",
    "    encoder.add((BatchNormalization()))\n",
    "    encoder.add(Activation('relu'))\n",
    "    \n",
    "    encoder.add(Dense(code_size))\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "\n",
    "def build_decoder(img_shape, code_size):\n",
    "    decoder = Sequential()\n",
    "    decoder.add(InputLayer((code_size,)))\n",
    "    \n",
    "#     decoder.add(Dense(1024))\n",
    "#     decoder.add((BatchNormalization()))\n",
    "#     decoder.add(Activation('relu'))\n",
    "\n",
    "    decoder.add(Dense(1024))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    decoder.add(Dense(1024))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    decoder.add(Dense(1024))\n",
    "    decoder.add(BatchNormalization())\n",
    "    \n",
    "    decoder.add(Reshape((2,2,256)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=32, kernel_size=(3, 3)))\n",
    "    decoder.add(BatchNormalization())\n",
    "    decoder.add(Activation('relu'))\n",
    "    \n",
    "    decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "    decoder.add(Conv2D(filters=1, kernel_size=(3, 3), activation='sigmoid', padding='same', name='autoencoder'))\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "def build_classifier(code_size):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(InputLayer((code_size,)))\n",
    "    \n",
    "    classifier.add(Dense(4096))\n",
    "    classifier.add((BatchNormalization()))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    \n",
    "    classifier.add(Dense(4096))\n",
    "    classifier.add((BatchNormalization()))\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    \n",
    "    classifier.add(Dense(10, activation='softmax', name='classifier'))\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:01.601822Z",
     "start_time": "2019-01-14T12:13:01.591714Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build_encoder(img_shape, code_size):\n",
    "#     weight_decay = 1e-4\n",
    "    \n",
    "#     # encoder\n",
    "#     encoder = Sequential()\n",
    "#     encoder.add(InputLayer(img_shape))\n",
    "    \n",
    "#     encoder.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "# #     encoder.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "    \n",
    "#     encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "# #     encoder.add(Conv2D(filters=64,kernel_size=(3, 3),activation='relu',padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(Conv2D(filters=64,kernel_size=(3, 3),activation='relu',padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "    \n",
    "#     encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "# #     encoder.add(Conv2D(filters=128,kernel_size=(3, 3),activation='relu',padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(Conv2D(filters=128,kernel_size=(3, 3),activation='relu',padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "\n",
    "    \n",
    "# #     encoder.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     encoder.add(BatchNormalization())\n",
    "    \n",
    "#     encoder.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "#     encoder.add(Flatten())                  #flatten image to vector\n",
    "    \n",
    "# #     encoder.add(Dense(1024, activation='relu'))\n",
    "# #     encoder.add(Dropout(0.1))\n",
    "# #     encoder.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "#     encoder.add(Dense(code_size))\n",
    "    \n",
    "#     return encoder\n",
    "\n",
    "\n",
    "# def build_decoder(img_shape, code_size):\n",
    "#     decoder = Sequential()\n",
    "#     decoder.add(InputLayer((code_size,)))\n",
    "    \n",
    "# #     decoder.add(Dense(1024, activation='relu'))\n",
    "    \n",
    "#     decoder.add(Dense(1024))\n",
    "    \n",
    "#     decoder.add(Reshape((2,2,256)))\n",
    "    \n",
    "# #     decoder.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "# #     decoder.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "# #     decoder.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "# #     decoder.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "# #     decoder.add(BatchNormalization())\n",
    "    \n",
    "#     decoder.add(UpSampling2D((2,2)))\n",
    "    \n",
    "#     decoder.add(Conv2D(filters=1, kernel_size=(3, 3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "#     return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:02.092467Z",
     "start_time": "2019-01-14T12:13:02.082372Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build_deep_autoencoder(img_shape, code_size):\n",
    "#     H,W,C = img_shape\n",
    "    \n",
    "#     # encoder\n",
    "#     encoder = keras.models.Sequential()\n",
    "#     encoder.add(L.InputLayer(img_shape))\n",
    "    \n",
    "#     encoder.add(L.Conv2D(filters=32, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "#     encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     encoder.add(L.Conv2D(filters=64, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "#     encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     encoder.add(L.Conv2D(filters=128, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "#     encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "# #     encoder.add(L.Conv2D(filters=256, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "# #     encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     encoder.add(L.Flatten())                  #flatten image to vector\n",
    "    \n",
    "#     encoder.add(L.Dense(2048, activation='elu'))\n",
    "#     encoder.add(L.Dense(2048, activation='elu'))\n",
    "# #     encoder.add(L.Dense(1024, activation='elu'))\n",
    "\n",
    "# #     encoder.add(L.Dropout(rate=0.2))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "    \n",
    "# #     encoder.add(L.Dense(512, activation='elu'))\n",
    "# #     encoder.add(L.Dropout(rate=0.2))\n",
    "# #     encoder.add(L.BatchNormalization())\n",
    "\n",
    "# #     encoder.add(L.Dense(256, activation='elu'))\n",
    "    \n",
    "# #     encoder.add(L.Dense(128, activation='elu'))\n",
    "    \n",
    "#     encoder.add(L.Dense(code_size)) \n",
    "\n",
    "#     # decoder\n",
    "#     decoder = keras.models.Sequential()\n",
    "#     decoder.add(L.InputLayer((code_size,)))\n",
    "    \n",
    "#     decoder.add(L.Dense(4*4*128))  #actual decoder, height*width*3 units\n",
    "    \n",
    "# #     decoder.add(L.Dense(128, activation='elu'))\n",
    "# #     decoder.add(L.Dropout(rate=0.2))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "    \n",
    "# #     decoder.add(L.Dense(256, activation='elu'))\n",
    "# #     decoder.add(L.Dropout(rate=0.2))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "# #     decoder.add(L.Dense(512, activation='elu'))\n",
    "    \n",
    "#     decoder.add(L.Dense(2048, activation='elu'))\n",
    "#     decoder.add(L.Dense(2048, activation='elu'))\n",
    "# #     decoder.add(L.Dense(1024, activation='elu'))\n",
    "    \n",
    "#     decoder.add(L.Reshape((4,4,128)))\n",
    "    \n",
    "#     decoder.add(L.Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=2, activation='elu', padding='same'))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     decoder.add(L.Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=2, activation='elu', padding='same'))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "    \n",
    "#     decoder.add(L.Conv2DTranspose(filters=3, kernel_size=(3, 3), strides=2, activation=None, padding='same'))\n",
    "# #     decoder.add(L.BatchNormalization())\n",
    "    \n",
    "# #     decoder.add(L.Conv2DTranspose(filters=3, kernel_size=(3, 3), strides=2, activation=None, padding='same'))\n",
    "    \n",
    "#     return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:02.632821Z",
     "start_time": "2019-01-14T12:13:02.516007Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:04.019342Z",
     "start_time": "2019-01-14T12:13:03.077105Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                32800     \n",
      "=================================================================\n",
      "Total params: 2,529,952\n",
      "Trainable params: 2,524,896\n",
      "Non-trainable params: 5,056\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "autoencoder (Conv2D)         (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 3,124,865\n",
      "Trainable params: 3,117,761\n",
      "Non-trainable params: 7,104\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              135168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 16,990,218\n",
      "Trainable params: 16,973,834\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = build_encoder(IMG_SHAPE, code_size=bit_size)\n",
    "decoder = build_decoder(IMG_SHAPE, code_size=bit_size)\n",
    "classifier = build_classifier(code_size=bit_size)\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:08.839795Z",
     "start_time": "2019-01-14T12:13:08.831930Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.adam(decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:09.512527Z",
     "start_time": "2019-01-14T12:13:09.500457Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    lrate = 0.001\n",
    "    \n",
    "    if epoch > 300:\n",
    "        lrate= 0.0005\n",
    "    if epoch > 400:\n",
    "        lrate = 0.0001\n",
    "        \n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:10.857908Z",
     "start_time": "2019-01-14T12:13:10.234387Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = L.Input(IMG_SHAPE)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "classification = classifier(code)\n",
    "\n",
    "outputs = [reconstruction, classification]\n",
    "\n",
    "autoencoder = keras.models.Model(inputs=inp, outputs=outputs)\n",
    "autoencoder.compile(optimizer=optimizer, loss={'sequential_3': 'categorical_crossentropy', 'sequential_2': 'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T12:13:20.138433Z",
     "start_time": "2019-01-14T12:13:20.135467Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:36.228431Z",
     "start_time": "2019-01-13T03:43:36.224364Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 32)           2529952     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 28, 28, 1)    3124865     sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 10)           16990218    sequential_1[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 22,645,035\n",
      "Trainable params: 22,616,491\n",
      "Non-trainable params: 28,544\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T03:43:36.232822Z",
     "start_time": "2019-01-13T03:43:36.229602Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:05.580444Z",
     "start_time": "2019-01-13T03:43:36.234281Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.6206 - sequential_2_loss: 0.0326 - sequential_3_loss: 0.5881 - val_loss: 2.9519 - val_sequential_2_loss: 0.0952 - val_sequential_3_loss: 2.8567\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.2948 - sequential_2_loss: 0.0232 - sequential_3_loss: 0.2716 - val_loss: 2.7111 - val_sequential_2_loss: 0.0927 - val_sequential_3_loss: 2.6184\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.2421 - sequential_2_loss: 0.0211 - sequential_3_loss: 0.2210 - val_loss: 1.5504 - val_sequential_2_loss: 0.0594 - val_sequential_3_loss: 1.4909\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.2100 - sequential_2_loss: 0.0199 - sequential_3_loss: 0.1901 - val_loss: 0.6505 - val_sequential_2_loss: 0.0352 - val_sequential_3_loss: 0.6153\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1834 - sequential_2_loss: 0.0190 - sequential_3_loss: 0.1644 - val_loss: 0.7338 - val_sequential_2_loss: 0.0329 - val_sequential_3_loss: 0.7010\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1613 - sequential_2_loss: 0.0185 - sequential_3_loss: 0.1428 - val_loss: 0.6888 - val_sequential_2_loss: 0.0241 - val_sequential_3_loss: 0.6646\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1471 - sequential_2_loss: 0.0181 - sequential_3_loss: 0.1290 - val_loss: 0.4450 - val_sequential_2_loss: 0.0244 - val_sequential_3_loss: 0.4207\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1330 - sequential_2_loss: 0.0176 - sequential_3_loss: 0.1154 - val_loss: 0.3936 - val_sequential_2_loss: 0.0253 - val_sequential_3_loss: 0.3683\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.1146 - sequential_2_loss: 0.0171 - sequential_3_loss: 0.0974 - val_loss: 0.6179 - val_sequential_2_loss: 0.0220 - val_sequential_3_loss: 0.5959\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.1034 - sequential_2_loss: 0.0168 - sequential_3_loss: 0.0866 - val_loss: 0.5642 - val_sequential_2_loss: 0.0227 - val_sequential_3_loss: 0.5415\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0992 - sequential_2_loss: 0.0166 - sequential_3_loss: 0.0826 - val_loss: 0.5578 - val_sequential_2_loss: 0.0217 - val_sequential_3_loss: 0.5360\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0937 - sequential_2_loss: 0.0164 - sequential_3_loss: 0.0773 - val_loss: 0.4805 - val_sequential_2_loss: 0.0274 - val_sequential_3_loss: 0.4532\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0824 - sequential_2_loss: 0.0160 - sequential_3_loss: 0.0664 - val_loss: 0.4943 - val_sequential_2_loss: 0.0203 - val_sequential_3_loss: 0.4740\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0759 - sequential_2_loss: 0.0158 - sequential_3_loss: 0.0602 - val_loss: 0.5346 - val_sequential_2_loss: 0.0213 - val_sequential_3_loss: 0.5133\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0675 - sequential_2_loss: 0.0156 - sequential_3_loss: 0.0519 - val_loss: 0.5261 - val_sequential_2_loss: 0.0188 - val_sequential_3_loss: 0.5073\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0625 - sequential_2_loss: 0.0155 - sequential_3_loss: 0.0470 - val_loss: 0.5230 - val_sequential_2_loss: 0.0215 - val_sequential_3_loss: 0.5015\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0669 - sequential_2_loss: 0.0154 - sequential_3_loss: 0.0514 - val_loss: 0.4395 - val_sequential_2_loss: 0.0205 - val_sequential_3_loss: 0.4190\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0590 - sequential_2_loss: 0.0151 - sequential_3_loss: 0.0438 - val_loss: 0.5028 - val_sequential_2_loss: 0.0211 - val_sequential_3_loss: 0.4818\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0556 - sequential_2_loss: 0.0151 - sequential_3_loss: 0.0405 - val_loss: 0.5627 - val_sequential_2_loss: 0.0205 - val_sequential_3_loss: 0.5422\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0557 - sequential_2_loss: 0.0151 - sequential_3_loss: 0.0406 - val_loss: 0.6051 - val_sequential_2_loss: 0.0215 - val_sequential_3_loss: 0.5836\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0510 - sequential_2_loss: 0.0148 - sequential_3_loss: 0.0361 - val_loss: 0.6710 - val_sequential_2_loss: 0.0235 - val_sequential_3_loss: 0.6475\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0459 - sequential_2_loss: 0.0148 - sequential_3_loss: 0.0311 - val_loss: 0.6029 - val_sequential_2_loss: 0.0175 - val_sequential_3_loss: 0.5855\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0445 - sequential_2_loss: 0.0145 - sequential_3_loss: 0.0299 - val_loss: 0.6907 - val_sequential_2_loss: 0.0204 - val_sequential_3_loss: 0.6703\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0424 - sequential_2_loss: 0.0145 - sequential_3_loss: 0.0279 - val_loss: 0.6725 - val_sequential_2_loss: 0.0205 - val_sequential_3_loss: 0.6520\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0435 - sequential_2_loss: 0.0145 - sequential_3_loss: 0.0290 - val_loss: 0.5766 - val_sequential_2_loss: 0.0210 - val_sequential_3_loss: 0.5556\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0432 - sequential_2_loss: 0.0144 - sequential_3_loss: 0.0287 - val_loss: 0.6018 - val_sequential_2_loss: 0.0198 - val_sequential_3_loss: 0.5820\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0454 - sequential_2_loss: 0.0144 - sequential_3_loss: 0.0310 - val_loss: 0.5658 - val_sequential_2_loss: 0.0189 - val_sequential_3_loss: 0.5469\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0400 - sequential_2_loss: 0.0141 - sequential_3_loss: 0.0259 - val_loss: 0.6437 - val_sequential_2_loss: 0.0217 - val_sequential_3_loss: 0.6220\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0440 - sequential_2_loss: 0.0142 - sequential_3_loss: 0.0297 - val_loss: 0.5526 - val_sequential_2_loss: 0.0179 - val_sequential_3_loss: 0.5347\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0379 - sequential_2_loss: 0.0141 - sequential_3_loss: 0.0238 - val_loss: 0.6926 - val_sequential_2_loss: 0.0188 - val_sequential_3_loss: 0.6738\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0335 - sequential_2_loss: 0.0138 - sequential_3_loss: 0.0197 - val_loss: 0.4661 - val_sequential_2_loss: 0.0182 - val_sequential_3_loss: 0.4479\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0338 - sequential_2_loss: 0.0138 - sequential_3_loss: 0.0200 - val_loss: 0.5449 - val_sequential_2_loss: 0.0192 - val_sequential_3_loss: 0.5257\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0361 - sequential_2_loss: 0.0138 - sequential_3_loss: 0.0223 - val_loss: 0.6150 - val_sequential_2_loss: 0.0178 - val_sequential_3_loss: 0.5972\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0384 - sequential_2_loss: 0.0138 - sequential_3_loss: 0.0247 - val_loss: 0.9049 - val_sequential_2_loss: 0.0170 - val_sequential_3_loss: 0.8878\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0330 - sequential_2_loss: 0.0136 - sequential_3_loss: 0.0194 - val_loss: 0.4829 - val_sequential_2_loss: 0.0168 - val_sequential_3_loss: 0.4661\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0352 - sequential_2_loss: 0.0136 - sequential_3_loss: 0.0217 - val_loss: 0.7110 - val_sequential_2_loss: 0.0178 - val_sequential_3_loss: 0.6932\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0348 - sequential_2_loss: 0.0136 - sequential_3_loss: 0.0213 - val_loss: 0.7022 - val_sequential_2_loss: 0.0175 - val_sequential_3_loss: 0.6847\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0310 - sequential_2_loss: 0.0135 - sequential_3_loss: 0.0175 - val_loss: 0.5292 - val_sequential_2_loss: 0.0159 - val_sequential_3_loss: 0.5132\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0297 - sequential_2_loss: 0.0132 - sequential_3_loss: 0.0165 - val_loss: 0.6501 - val_sequential_2_loss: 0.0163 - val_sequential_3_loss: 0.6338\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0353 - sequential_2_loss: 0.0135 - sequential_3_loss: 0.0218 - val_loss: 0.7967 - val_sequential_2_loss: 0.0187 - val_sequential_3_loss: 0.7780\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0342 - sequential_2_loss: 0.0134 - sequential_3_loss: 0.0208 - val_loss: 0.6184 - val_sequential_2_loss: 0.0187 - val_sequential_3_loss: 0.5997\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0284 - sequential_2_loss: 0.0131 - sequential_3_loss: 0.0154 - val_loss: 0.5376 - val_sequential_2_loss: 0.0155 - val_sequential_3_loss: 0.5221\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0302 - sequential_2_loss: 0.0131 - sequential_3_loss: 0.0170 - val_loss: 0.5579 - val_sequential_2_loss: 0.0191 - val_sequential_3_loss: 0.5388\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0277 - sequential_2_loss: 0.0131 - sequential_3_loss: 0.0146 - val_loss: 0.6243 - val_sequential_2_loss: 0.0182 - val_sequential_3_loss: 0.6061\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0320 - sequential_2_loss: 0.0131 - sequential_3_loss: 0.0189 - val_loss: 0.6638 - val_sequential_2_loss: 0.0170 - val_sequential_3_loss: 0.6468\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0251 - sequential_2_loss: 0.0127 - sequential_3_loss: 0.0123 - val_loss: 0.5305 - val_sequential_2_loss: 0.0151 - val_sequential_3_loss: 0.5154\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0253 - sequential_2_loss: 0.0127 - sequential_3_loss: 0.0126 - val_loss: 0.4782 - val_sequential_2_loss: 0.0173 - val_sequential_3_loss: 0.4609\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0278 - sequential_2_loss: 0.0127 - sequential_3_loss: 0.0151 - val_loss: 0.4965 - val_sequential_2_loss: 0.0172 - val_sequential_3_loss: 0.4793\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0267 - sequential_2_loss: 0.0127 - sequential_3_loss: 0.0140 - val_loss: 0.8003 - val_sequential_2_loss: 0.0175 - val_sequential_3_loss: 0.7828\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0320 - sequential_2_loss: 0.0128 - sequential_3_loss: 0.0193 - val_loss: 0.4702 - val_sequential_2_loss: 0.0184 - val_sequential_3_loss: 0.4518\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0226 - sequential_2_loss: 0.0125 - sequential_3_loss: 0.0101 - val_loss: 0.6106 - val_sequential_2_loss: 0.0165 - val_sequential_3_loss: 0.5941\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0388 - sequential_2_loss: 0.0134 - sequential_3_loss: 0.0255 - val_loss: 0.6680 - val_sequential_2_loss: 0.0211 - val_sequential_3_loss: 0.6469\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0305 - sequential_2_loss: 0.0130 - sequential_3_loss: 0.0174 - val_loss: 0.6025 - val_sequential_2_loss: 0.0174 - val_sequential_3_loss: 0.5851\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0274 - sequential_2_loss: 0.0128 - sequential_3_loss: 0.0147 - val_loss: 0.6292 - val_sequential_2_loss: 0.0163 - val_sequential_3_loss: 0.6129\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0244 - sequential_2_loss: 0.0126 - sequential_3_loss: 0.0118 - val_loss: 0.5724 - val_sequential_2_loss: 0.0157 - val_sequential_3_loss: 0.5567\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0209 - sequential_2_loss: 0.0123 - sequential_3_loss: 0.0086 - val_loss: 0.5939 - val_sequential_2_loss: 0.0159 - val_sequential_3_loss: 0.5779\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0215 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0093 - val_loss: 0.5721 - val_sequential_2_loss: 0.0149 - val_sequential_3_loss: 0.5572\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0251 - sequential_2_loss: 0.0123 - sequential_3_loss: 0.0128 - val_loss: 0.5226 - val_sequential_2_loss: 0.0148 - val_sequential_3_loss: 0.5078\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0224 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0102 - val_loss: 0.5117 - val_sequential_2_loss: 0.0172 - val_sequential_3_loss: 0.4945\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0243 - sequential_2_loss: 0.0123 - sequential_3_loss: 0.0121 - val_loss: 0.5614 - val_sequential_2_loss: 0.0152 - val_sequential_3_loss: 0.5462\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0245 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0124 - val_loss: 0.5116 - val_sequential_2_loss: 0.0155 - val_sequential_3_loss: 0.4961\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0237 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0115 - val_loss: 0.5631 - val_sequential_2_loss: 0.0174 - val_sequential_3_loss: 0.5457\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0269 - sequential_2_loss: 0.0123 - sequential_3_loss: 0.0146 - val_loss: 0.6170 - val_sequential_2_loss: 0.0159 - val_sequential_3_loss: 0.6010\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0263 - sequential_2_loss: 0.0122 - sequential_3_loss: 0.0141 - val_loss: 0.6245 - val_sequential_2_loss: 0.0194 - val_sequential_3_loss: 0.6051\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0212 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0092 - val_loss: 0.5753 - val_sequential_2_loss: 0.0144 - val_sequential_3_loss: 0.5609\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0253 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0133 - val_loss: 0.5695 - val_sequential_2_loss: 0.0167 - val_sequential_3_loss: 0.5528\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0235 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0115 - val_loss: 0.5306 - val_sequential_2_loss: 0.0156 - val_sequential_3_loss: 0.5150\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0218 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0098 - val_loss: 0.5445 - val_sequential_2_loss: 0.0153 - val_sequential_3_loss: 0.5291\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0232 - sequential_2_loss: 0.0119 - sequential_3_loss: 0.0113 - val_loss: 0.5648 - val_sequential_2_loss: 0.0142 - val_sequential_3_loss: 0.5505\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0257 - sequential_2_loss: 0.0120 - sequential_3_loss: 0.0137 - val_loss: 0.4802 - val_sequential_2_loss: 0.0153 - val_sequential_3_loss: 0.4649\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0219 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0101 - val_loss: 0.5510 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.5367\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0206 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0088 - val_loss: 0.5893 - val_sequential_2_loss: 0.0163 - val_sequential_3_loss: 0.5730\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0224 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0105 - val_loss: 0.5346 - val_sequential_2_loss: 0.0167 - val_sequential_3_loss: 0.5179\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0196 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0078 - val_loss: 0.5455 - val_sequential_2_loss: 0.0136 - val_sequential_3_loss: 0.5320\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0197 - sequential_2_loss: 0.0115 - sequential_3_loss: 0.0082 - val_loss: 0.5811 - val_sequential_2_loss: 0.0147 - val_sequential_3_loss: 0.5664\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0220 - sequential_2_loss: 0.0116 - sequential_3_loss: 0.0104 - val_loss: 0.5644 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.5502\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0190 - sequential_2_loss: 0.0114 - sequential_3_loss: 0.0076 - val_loss: 0.5066 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.4925\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0231 - sequential_2_loss: 0.0116 - sequential_3_loss: 0.0115 - val_loss: 0.6664 - val_sequential_2_loss: 0.0176 - val_sequential_3_loss: 0.6487\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0225 - sequential_2_loss: 0.0118 - sequential_3_loss: 0.0107 - val_loss: 0.5231 - val_sequential_2_loss: 0.0156 - val_sequential_3_loss: 0.5075\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0220 - sequential_2_loss: 0.0116 - sequential_3_loss: 0.0103 - val_loss: 0.6395 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.6252\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0076 - val_loss: 0.5329 - val_sequential_2_loss: 0.0157 - val_sequential_3_loss: 0.5171\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0200 - sequential_2_loss: 0.0114 - sequential_3_loss: 0.0087 - val_loss: 0.7007 - val_sequential_2_loss: 0.0179 - val_sequential_3_loss: 0.6828\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0217 - sequential_2_loss: 0.0116 - sequential_3_loss: 0.0101 - val_loss: 0.6516 - val_sequential_2_loss: 0.0140 - val_sequential_3_loss: 0.6376\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0183 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0070 - val_loss: 0.5996 - val_sequential_2_loss: 0.0149 - val_sequential_3_loss: 0.5847\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0183 - sequential_2_loss: 0.0112 - sequential_3_loss: 0.0071 - val_loss: 0.5658 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.5517\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0209 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0096 - val_loss: 0.5459 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.5316\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0192 - sequential_2_loss: 0.0112 - sequential_3_loss: 0.0080 - val_loss: 0.5349 - val_sequential_2_loss: 0.0137 - val_sequential_3_loss: 0.5211\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0178 - sequential_2_loss: 0.0112 - sequential_3_loss: 0.0067 - val_loss: 0.5524 - val_sequential_2_loss: 0.0140 - val_sequential_3_loss: 0.5384\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0079 - val_loss: 0.5486 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.5353\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0195 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0085 - val_loss: 0.5954 - val_sequential_2_loss: 0.0135 - val_sequential_3_loss: 0.5819\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0175 - sequential_2_loss: 0.0110 - sequential_3_loss: 0.0065 - val_loss: 0.6241 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.6095\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0201 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0090 - val_loss: 0.5728 - val_sequential_2_loss: 0.0136 - val_sequential_3_loss: 0.5592\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0195 - sequential_2_loss: 0.0112 - sequential_3_loss: 0.0083 - val_loss: 0.6290 - val_sequential_2_loss: 0.0163 - val_sequential_3_loss: 0.6127\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0221 - sequential_2_loss: 0.0114 - sequential_3_loss: 0.0107 - val_loss: 0.5243 - val_sequential_2_loss: 0.0161 - val_sequential_3_loss: 0.5082\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0167 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0057 - val_loss: 0.6341 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.6200\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0215 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0102 - val_loss: 0.5842 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.5696\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0184 - sequential_2_loss: 0.0111 - sequential_3_loss: 0.0073 - val_loss: 0.5852 - val_sequential_2_loss: 0.0142 - val_sequential_3_loss: 0.5710\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0224 - sequential_2_loss: 0.0113 - sequential_3_loss: 0.0112 - val_loss: 0.5730 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.5587\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0168 - sequential_2_loss: 0.0110 - sequential_3_loss: 0.0057 - val_loss: 0.5603 - val_sequential_2_loss: 0.0132 - val_sequential_3_loss: 0.5471\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0159 - sequential_2_loss: 0.0108 - sequential_3_loss: 0.0051 - val_loss: 0.6821 - val_sequential_2_loss: 0.0142 - val_sequential_3_loss: 0.6678\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0109 - sequential_3_loss: 0.0080 - val_loss: 0.5498 - val_sequential_2_loss: 0.0154 - val_sequential_3_loss: 0.5344\n",
      "Epoch 102/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0186 - sequential_2_loss: 0.0109 - sequential_3_loss: 0.0077 - val_loss: 0.5624 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.5486\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0175 - sequential_2_loss: 0.0108 - sequential_3_loss: 0.0067 - val_loss: 0.5938 - val_sequential_2_loss: 0.0136 - val_sequential_3_loss: 0.5802\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0172 - sequential_2_loss: 0.0107 - sequential_3_loss: 0.0065 - val_loss: 0.5308 - val_sequential_2_loss: 0.0134 - val_sequential_3_loss: 0.5174\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0161 - sequential_2_loss: 0.0106 - sequential_3_loss: 0.0055 - val_loss: 0.5073 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.4940\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0168 - sequential_2_loss: 0.0106 - sequential_3_loss: 0.0062 - val_loss: 0.5546 - val_sequential_2_loss: 0.0137 - val_sequential_3_loss: 0.5409\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0155 - sequential_2_loss: 0.0106 - sequential_3_loss: 0.0049 - val_loss: 0.5937 - val_sequential_2_loss: 0.0139 - val_sequential_3_loss: 0.5797\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0170 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0066 - val_loss: 0.5627 - val_sequential_2_loss: 0.0136 - val_sequential_3_loss: 0.5491\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0148 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0044 - val_loss: 0.5058 - val_sequential_2_loss: 0.0132 - val_sequential_3_loss: 0.4926\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0132 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0030 - val_loss: 0.5522 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5393\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0172 - sequential_2_loss: 0.0104 - sequential_3_loss: 0.0068 - val_loss: 0.7603 - val_sequential_2_loss: 0.0162 - val_sequential_3_loss: 0.7441\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0208 - sequential_2_loss: 0.0106 - sequential_3_loss: 0.0101 - val_loss: 0.5618 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.5485\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0158 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0054 - val_loss: 0.5166 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5040\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0164 - sequential_2_loss: 0.0104 - sequential_3_loss: 0.0060 - val_loss: 0.6233 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.6086\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0161 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0058 - val_loss: 0.6723 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.6595\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0144 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0042 - val_loss: 0.5214 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.5075\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0196 - sequential_2_loss: 0.0104 - sequential_3_loss: 0.0091 - val_loss: 0.6548 - val_sequential_2_loss: 0.0147 - val_sequential_3_loss: 0.6402\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0174 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0069 - val_loss: 0.5329 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.5196\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0170 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0066 - val_loss: 0.5979 - val_sequential_2_loss: 0.0142 - val_sequential_3_loss: 0.5838\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0166 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0060 - val_loss: 0.5993 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.5860\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0187 - sequential_2_loss: 0.0105 - sequential_3_loss: 0.0082 - val_loss: 0.6702 - val_sequential_2_loss: 0.0143 - val_sequential_3_loss: 0.6560\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0147 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0044 - val_loss: 0.6276 - val_sequential_2_loss: 0.0131 - val_sequential_3_loss: 0.6145\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0147 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0044 - val_loss: 0.5482 - val_sequential_2_loss: 0.0130 - val_sequential_3_loss: 0.5353\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0152 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0049 - val_loss: 0.5241 - val_sequential_2_loss: 0.0141 - val_sequential_3_loss: 0.5099\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0156 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0054 - val_loss: 0.5491 - val_sequential_2_loss: 0.0135 - val_sequential_3_loss: 0.5356\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0144 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0042 - val_loss: 0.5319 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5191\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0131 - sequential_2_loss: 0.0101 - sequential_3_loss: 0.0031 - val_loss: 0.6121 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5993\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0195 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0092 - val_loss: 0.6790 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.6651\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0154 - sequential_2_loss: 0.0103 - sequential_3_loss: 0.0050 - val_loss: 0.5453 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5324\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0139 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0039 - val_loss: 0.5930 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5804\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0141 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0042 - val_loss: 0.5653 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5526\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0158 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0058 - val_loss: 0.5486 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5361\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0152 - sequential_2_loss: 0.0099 - sequential_3_loss: 0.0052 - val_loss: 0.5600 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5474\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0169 - sequential_2_loss: 0.0101 - sequential_3_loss: 0.0068 - val_loss: 0.5043 - val_sequential_2_loss: 0.0130 - val_sequential_3_loss: 0.4912\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0148 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0048 - val_loss: 0.5748 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5619\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0155 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0055 - val_loss: 0.5413 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5287\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0152 - sequential_2_loss: 0.0100 - sequential_3_loss: 0.0052 - val_loss: 0.5278 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5151\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0127 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0029 - val_loss: 0.5610 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5485\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0141 - sequential_2_loss: 0.0097 - sequential_3_loss: 0.0043 - val_loss: 0.6536 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.6403\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0137 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0039 - val_loss: 0.5524 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5397\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0141 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0042 - val_loss: 0.5636 - val_sequential_2_loss: 0.0130 - val_sequential_3_loss: 0.5505\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0162 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0064 - val_loss: 0.5834 - val_sequential_2_loss: 0.0135 - val_sequential_3_loss: 0.5699\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0088 - val_loss: 0.5303 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.5165\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0150 - sequential_2_loss: 0.0099 - sequential_3_loss: 0.0051 - val_loss: 0.5592 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5463\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0152 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0054 - val_loss: 0.5706 - val_sequential_2_loss: 0.0134 - val_sequential_3_loss: 0.5571\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0138 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0040 - val_loss: 0.5361 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5236\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0139 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0042 - val_loss: 0.5891 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5766\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0157 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0060 - val_loss: 0.6308 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.6185\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0121 - sequential_2_loss: 0.0096 - sequential_3_loss: 0.0025 - val_loss: 0.5842 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5713\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0109 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0014 - val_loss: 0.5635 - val_sequential_2_loss: 0.0131 - val_sequential_3_loss: 0.5504\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0123 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0029 - val_loss: 0.5734 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5608\n",
      "Epoch 152/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0136 - sequential_2_loss: 0.0096 - sequential_3_loss: 0.0041 - val_loss: 0.5947 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5823\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0119 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0026 - val_loss: 0.5586 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5464\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0139 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0045 - val_loss: 0.6201 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.6074\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0130 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0036 - val_loss: 0.6005 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5883\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0118 - sequential_2_loss: 0.0093 - sequential_3_loss: 0.0025 - val_loss: 0.6215 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.6088\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0168 - sequential_2_loss: 0.0096 - sequential_3_loss: 0.0072 - val_loss: 0.7644 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.7498\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0190 - sequential_2_loss: 0.0098 - sequential_3_loss: 0.0092 - val_loss: 0.6063 - val_sequential_2_loss: 0.0132 - val_sequential_3_loss: 0.5931\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0118 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0023 - val_loss: 0.6516 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.6394\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0141 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0046 - val_loss: 0.6406 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.6277\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0129 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0035 - val_loss: 0.5623 - val_sequential_2_loss: 0.0123 - val_sequential_3_loss: 0.5500\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0129 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0035 - val_loss: 0.5747 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5624\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0116 - sequential_2_loss: 0.0093 - sequential_3_loss: 0.0023 - val_loss: 0.5646 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5521\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0137 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0044 - val_loss: 0.7728 - val_sequential_2_loss: 0.0144 - val_sequential_3_loss: 0.7583\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0140 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0045 - val_loss: 0.5525 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5398\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0134 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0039 - val_loss: 0.6232 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.6106\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0126 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0033 - val_loss: 0.6296 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.6169\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0142 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0048 - val_loss: 0.7057 - val_sequential_2_loss: 0.0139 - val_sequential_3_loss: 0.6918\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0142 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0046 - val_loss: 0.5904 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5776\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0116 - sequential_2_loss: 0.0093 - sequential_3_loss: 0.0023 - val_loss: 0.5375 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5257\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0091 - sequential_2_loss: 0.0089 - sequential_3_loss: 1.8041e-04 - val_loss: 0.5593 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5478\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0095 - sequential_2_loss: 0.0088 - sequential_3_loss: 7.3301e-04 - val_loss: 0.5700 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5585\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0092 - sequential_2_loss: 0.0087 - sequential_3_loss: 5.0524e-04 - val_loss: 0.5747 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5632\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0092 - sequential_2_loss: 0.0086 - sequential_3_loss: 5.7774e-04 - val_loss: 0.5605 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5491\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0094 - sequential_2_loss: 0.0086 - sequential_3_loss: 7.8082e-04 - val_loss: 0.6188 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.6069\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0139 - sequential_2_loss: 0.0089 - sequential_3_loss: 0.0050 - val_loss: 0.8410 - val_sequential_2_loss: 0.0153 - val_sequential_3_loss: 0.8257\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0236 - sequential_2_loss: 0.0099 - sequential_3_loss: 0.0137 - val_loss: 0.5795 - val_sequential_2_loss: 0.0135 - val_sequential_3_loss: 0.5660\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0161 - sequential_2_loss: 0.0097 - sequential_3_loss: 0.0064 - val_loss: 0.5360 - val_sequential_2_loss: 0.0146 - val_sequential_3_loss: 0.5214\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0130 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0036 - val_loss: 0.5902 - val_sequential_2_loss: 0.0123 - val_sequential_3_loss: 0.5778\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0166 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0071 - val_loss: 0.5645 - val_sequential_2_loss: 0.0137 - val_sequential_3_loss: 0.5508\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0134 - sequential_2_loss: 0.0094 - sequential_3_loss: 0.0040 - val_loss: 0.6121 - val_sequential_2_loss: 0.0130 - val_sequential_3_loss: 0.5992\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0141 - sequential_2_loss: 0.0093 - sequential_3_loss: 0.0048 - val_loss: 0.5434 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5307\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0123 - sequential_2_loss: 0.0092 - sequential_3_loss: 0.0032 - val_loss: 0.5731 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5606\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0129 - sequential_2_loss: 0.0091 - sequential_3_loss: 0.0038 - val_loss: 0.5405 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5283\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0113 - sequential_2_loss: 0.0090 - sequential_3_loss: 0.0023 - val_loss: 0.5107 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.4988\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0106 - sequential_2_loss: 0.0088 - sequential_3_loss: 0.0018 - val_loss: 0.5409 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5294\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0101 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0014 - val_loss: 0.5250 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5134\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0097 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0011 - val_loss: 0.5607 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5492\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0107 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0021 - val_loss: 0.5374 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5256\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0102 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0016 - val_loss: 0.5633 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5515\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0102 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0017 - val_loss: 0.5406 - val_sequential_2_loss: 0.0120 - val_sequential_3_loss: 0.5286\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0087 - sequential_2_loss: 0.0084 - sequential_3_loss: 3.3298e-04 - val_loss: 0.5639 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5525\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0083 - sequential_2_loss: 0.0082 - sequential_3_loss: 5.6212e-05 - val_loss: 0.5544 - val_sequential_2_loss: 0.0111 - val_sequential_3_loss: 0.5434\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0081 - sequential_2_loss: 0.0081 - sequential_3_loss: 2.0117e-05 - val_loss: 0.5651 - val_sequential_2_loss: 0.0110 - val_sequential_3_loss: 0.5541\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0080 - sequential_2_loss: 0.0080 - sequential_3_loss: 1.1776e-05 - val_loss: 0.5704 - val_sequential_2_loss: 0.0110 - val_sequential_3_loss: 0.5594\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0079 - sequential_2_loss: 0.0079 - sequential_3_loss: 9.4638e-06 - val_loss: 0.5766 - val_sequential_2_loss: 0.0110 - val_sequential_3_loss: 0.5656\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0078 - sequential_2_loss: 0.0078 - sequential_3_loss: 6.7179e-06 - val_loss: 0.5810 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5701\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0078 - sequential_2_loss: 0.0078 - sequential_3_loss: 9.3755e-06 - val_loss: 0.5870 - val_sequential_2_loss: 0.0112 - val_sequential_3_loss: 0.5758\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0077 - sequential_2_loss: 0.0077 - sequential_3_loss: 4.7702e-06 - val_loss: 0.5898 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5789\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0077 - sequential_2_loss: 0.0077 - sequential_3_loss: 4.9219e-06 - val_loss: 0.5937 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5829\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0076 - sequential_2_loss: 0.0076 - sequential_3_loss: 5.5916e-06 - val_loss: 0.5983 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5874\n",
      "Epoch 202/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0076 - sequential_2_loss: 0.0076 - sequential_3_loss: 4.2773e-06 - val_loss: 0.6017 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5909\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0075 - sequential_2_loss: 0.0075 - sequential_3_loss: 3.7602e-06 - val_loss: 0.6041 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5934\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0075 - sequential_2_loss: 0.0074 - sequential_3_loss: 4.6105e-06 - val_loss: 0.6077 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5969\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0074 - sequential_2_loss: 0.0074 - sequential_3_loss: 2.8543e-06 - val_loss: 0.6106 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5999\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0074 - sequential_2_loss: 0.0074 - sequential_3_loss: 2.7488e-06 - val_loss: 0.6139 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6032\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0073 - sequential_2_loss: 0.0073 - sequential_3_loss: 2.7892e-06 - val_loss: 0.6173 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6066\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0073 - sequential_2_loss: 0.0073 - sequential_3_loss: 2.7966e-06 - val_loss: 0.6203 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6095\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0073 - sequential_2_loss: 0.0073 - sequential_3_loss: 2.4474e-06 - val_loss: 0.6238 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6132\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0072 - sequential_2_loss: 0.0072 - sequential_3_loss: 2.0360e-06 - val_loss: 0.6268 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6162\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0072 - sequential_2_loss: 0.0072 - sequential_3_loss: 2.0146e-06 - val_loss: 0.6290 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6185\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0423 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0329 - val_loss: 0.6434 - val_sequential_2_loss: 0.0193 - val_sequential_3_loss: 0.6241\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0279 - sequential_2_loss: 0.0102 - sequential_3_loss: 0.0177 - val_loss: 0.4481 - val_sequential_2_loss: 0.0139 - val_sequential_3_loss: 0.4341\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0174 - sequential_2_loss: 0.0095 - sequential_3_loss: 0.0079 - val_loss: 0.5023 - val_sequential_2_loss: 0.0131 - val_sequential_3_loss: 0.4892\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0132 - sequential_2_loss: 0.0091 - sequential_3_loss: 0.0041 - val_loss: 0.5516 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5390\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0134 - sequential_2_loss: 0.0090 - sequential_3_loss: 0.0045 - val_loss: 0.5762 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.5637\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0136 - sequential_2_loss: 0.0090 - sequential_3_loss: 0.0046 - val_loss: 0.5661 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5539\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0025 - val_loss: 0.5427 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5305\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0108 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0021 - val_loss: 0.5696 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5571\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0128 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0041 - val_loss: 0.6002 - val_sequential_2_loss: 0.0152 - val_sequential_3_loss: 0.5850\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0166 - sequential_2_loss: 0.0090 - sequential_3_loss: 0.0076 - val_loss: 0.5694 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5567\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0106 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0021 - val_loss: 0.5042 - val_sequential_2_loss: 0.0116 - val_sequential_3_loss: 0.4926\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0092 - sequential_2_loss: 0.0083 - sequential_3_loss: 9.0965e-04 - val_loss: 0.5533 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5419\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0090 - sequential_2_loss: 0.0081 - sequential_3_loss: 8.8049e-04 - val_loss: 0.5653 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5539\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0089 - sequential_2_loss: 0.0081 - sequential_3_loss: 8.0146e-04 - val_loss: 0.5407 - val_sequential_2_loss: 0.0111 - val_sequential_3_loss: 0.5296\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0081 - sequential_2_loss: 0.0079 - sequential_3_loss: 1.3958e-04 - val_loss: 0.5651 - val_sequential_2_loss: 0.0111 - val_sequential_3_loss: 0.5540\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0080 - sequential_2_loss: 0.0078 - sequential_3_loss: 1.9056e-04 - val_loss: 0.6175 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.6061\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0126 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0045 - val_loss: 0.5404 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5275\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0144 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0058 - val_loss: 0.5866 - val_sequential_2_loss: 0.0128 - val_sequential_3_loss: 0.5738\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0105 - sequential_2_loss: 0.0083 - sequential_3_loss: 0.0023 - val_loss: 0.5476 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5355\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0124 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0039 - val_loss: 0.6859 - val_sequential_2_loss: 0.0133 - val_sequential_3_loss: 0.6727\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0134 - sequential_2_loss: 0.0087 - sequential_3_loss: 0.0046 - val_loss: 0.6451 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.6326\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0124 - sequential_2_loss: 0.0086 - sequential_3_loss: 0.0037 - val_loss: 0.5414 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5290\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0027 - val_loss: 0.5660 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5537\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0113 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0028 - val_loss: 0.5723 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5601\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0116 - sequential_2_loss: 0.0084 - sequential_3_loss: 0.0032 - val_loss: 0.5515 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.5396\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0092 - sequential_2_loss: 0.0082 - sequential_3_loss: 9.8135e-04 - val_loss: 0.5443 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5330\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0099 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0019 - val_loss: 0.5563 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5448\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0086 - sequential_2_loss: 0.0079 - sequential_3_loss: 7.0877e-04 - val_loss: 0.5566 - val_sequential_2_loss: 0.0113 - val_sequential_3_loss: 0.5453\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0101 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0021 - val_loss: 0.6235 - val_sequential_2_loss: 0.0126 - val_sequential_3_loss: 0.6109\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0137 - sequential_2_loss: 0.0084 - sequential_3_loss: 0.0054 - val_loss: 0.6530 - val_sequential_2_loss: 0.0138 - val_sequential_3_loss: 0.6392\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0133 - sequential_2_loss: 0.0085 - sequential_3_loss: 0.0048 - val_loss: 0.5318 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5197\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0087 - sequential_2_loss: 0.0081 - sequential_3_loss: 6.6954e-04 - val_loss: 0.5341 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5226\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0093 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0013 - val_loss: 0.5492 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5375\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0088 - sequential_2_loss: 0.0079 - sequential_3_loss: 8.5125e-04 - val_loss: 0.5832 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5716\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0083 - sequential_2_loss: 0.0078 - sequential_3_loss: 5.5182e-04 - val_loss: 0.5668 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5553\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0112 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0032 - val_loss: 0.6340 - val_sequential_2_loss: 0.0148 - val_sequential_3_loss: 0.6191\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0139 - sequential_2_loss: 0.0083 - sequential_3_loss: 0.0056 - val_loss: 0.6743 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.6619\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0118 - sequential_2_loss: 0.0084 - sequential_3_loss: 0.0034 - val_loss: 0.5688 - val_sequential_2_loss: 0.0120 - val_sequential_3_loss: 0.5568\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0092 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0011 - val_loss: 0.6091 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5974\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0095 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0014 - val_loss: 0.5832 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5715\n",
      "Epoch 252/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0109 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0029 - val_loss: 0.6644 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.6517\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0031 - val_loss: 0.5416 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5299\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0111 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0030 - val_loss: 0.6482 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.6353\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0118 - sequential_2_loss: 0.0083 - sequential_3_loss: 0.0035 - val_loss: 0.7581 - val_sequential_2_loss: 0.0120 - val_sequential_3_loss: 0.7460\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0110 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0028 - val_loss: 0.5813 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5692\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0115 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0033 - val_loss: 0.6260 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.6141\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0104 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0023 - val_loss: 0.6480 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.6356\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0030 - val_loss: 0.5717 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5596\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0119 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0036 - val_loss: 0.5712 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.5592\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0090 - sequential_2_loss: 0.0080 - sequential_3_loss: 9.8798e-04 - val_loss: 0.5565 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5447\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0094 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0014 - val_loss: 0.5956 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5839\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0021 - val_loss: 0.6081 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5956\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0116 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0035 - val_loss: 0.6076 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5955\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0107 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0026 - val_loss: 0.5685 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5567\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0098 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0018 - val_loss: 0.5681 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5564\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0021 - val_loss: 0.5524 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5409\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0092 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0013 - val_loss: 0.6633 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.6516\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0093 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0015 - val_loss: 0.5812 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5691\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0095 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0017 - val_loss: 0.6354 - val_sequential_2_loss: 0.0116 - val_sequential_3_loss: 0.6238\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0022 - val_loss: 0.6000 - val_sequential_2_loss: 0.0120 - val_sequential_3_loss: 0.5880\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0020 - val_loss: 0.6035 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5911\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0104 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0025 - val_loss: 0.6190 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.6071\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0137 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0055 - val_loss: 0.5966 - val_sequential_2_loss: 0.0132 - val_sequential_3_loss: 0.5834\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0123 - sequential_2_loss: 0.0083 - sequential_3_loss: 0.0040 - val_loss: 0.5490 - val_sequential_2_loss: 0.0121 - val_sequential_3_loss: 0.5369\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0098 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0018 - val_loss: 0.5621 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5504\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0084 - sequential_2_loss: 0.0078 - sequential_3_loss: 5.7593e-04 - val_loss: 0.5623 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.5510\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0079 - sequential_2_loss: 0.0076 - sequential_3_loss: 2.8560e-04 - val_loss: 0.5479 - val_sequential_2_loss: 0.0111 - val_sequential_3_loss: 0.5368\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0079 - sequential_2_loss: 0.0075 - sequential_3_loss: 4.2247e-04 - val_loss: 0.5849 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5725\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0079 - sequential_2_loss: 0.0076 - sequential_3_loss: 3.4379e-04 - val_loss: 0.5623 - val_sequential_2_loss: 0.0112 - val_sequential_3_loss: 0.5510\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0075 - sequential_2_loss: 0.0074 - sequential_3_loss: 7.7024e-05 - val_loss: 0.5742 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5633\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0072 - sequential_2_loss: 0.0072 - sequential_3_loss: 2.2197e-05 - val_loss: 0.5844 - val_sequential_2_loss: 0.0110 - val_sequential_3_loss: 0.5734\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0094 - sequential_2_loss: 0.0075 - sequential_3_loss: 0.0018 - val_loss: 0.6038 - val_sequential_2_loss: 0.0127 - val_sequential_3_loss: 0.5912\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0138 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0057 - val_loss: 0.6053 - val_sequential_2_loss: 0.0129 - val_sequential_3_loss: 0.5925\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0146 - sequential_2_loss: 0.0084 - sequential_3_loss: 0.0062 - val_loss: 0.5731 - val_sequential_2_loss: 0.0124 - val_sequential_3_loss: 0.5607\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0102 - sequential_2_loss: 0.0081 - sequential_3_loss: 0.0022 - val_loss: 0.5620 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5495\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0090 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0012 - val_loss: 0.6250 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.6135\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0090 - sequential_2_loss: 0.0077 - sequential_3_loss: 0.0013 - val_loss: 0.5873 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5748\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0093 - sequential_2_loss: 0.0077 - sequential_3_loss: 0.0016 - val_loss: 0.5831 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5715\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0097 - sequential_2_loss: 0.0077 - sequential_3_loss: 0.0020 - val_loss: 0.6246 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.6129\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0104 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0026 - val_loss: 0.5863 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5746\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0091 - sequential_2_loss: 0.0078 - sequential_3_loss: 0.0014 - val_loss: 0.5689 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5570\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0100 - sequential_2_loss: 0.0077 - sequential_3_loss: 0.0024 - val_loss: 0.5949 - val_sequential_2_loss: 0.0117 - val_sequential_3_loss: 0.5832\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0119 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0041 - val_loss: 0.5886 - val_sequential_2_loss: 0.0119 - val_sequential_3_loss: 0.5767\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0103 - sequential_2_loss: 0.0079 - sequential_3_loss: 0.0024 - val_loss: 0.5748 - val_sequential_2_loss: 0.0125 - val_sequential_3_loss: 0.5623\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0082 - sequential_2_loss: 0.0077 - sequential_3_loss: 5.6915e-04 - val_loss: 0.5727 - val_sequential_2_loss: 0.0112 - val_sequential_3_loss: 0.5614\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0081 - sequential_2_loss: 0.0075 - sequential_3_loss: 5.6881e-04 - val_loss: 0.6130 - val_sequential_2_loss: 0.0114 - val_sequential_3_loss: 0.6016\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0145 - sequential_2_loss: 0.0082 - sequential_3_loss: 0.0063 - val_loss: 0.5931 - val_sequential_2_loss: 0.0140 - val_sequential_3_loss: 0.5792\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0111 - sequential_2_loss: 0.0080 - sequential_3_loss: 0.0031 - val_loss: 0.5565 - val_sequential_2_loss: 0.0118 - val_sequential_3_loss: 0.5447\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0087 - sequential_2_loss: 0.0077 - sequential_3_loss: 9.5909e-04 - val_loss: 0.5444 - val_sequential_2_loss: 0.0115 - val_sequential_3_loss: 0.5328\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0078 - sequential_2_loss: 0.0075 - sequential_3_loss: 2.3723e-04 - val_loss: 0.5600 - val_sequential_2_loss: 0.0112 - val_sequential_3_loss: 0.5488\n",
      "Epoch 302/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0072 - sequential_2_loss: 0.0071 - sequential_3_loss: 4.6609e-05 - val_loss: 0.5630 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5521\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0070 - sequential_2_loss: 0.0070 - sequential_3_loss: 2.3492e-05 - val_loss: 0.5688 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5579\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0070 - sequential_2_loss: 0.0069 - sequential_3_loss: 2.5248e-05 - val_loss: 0.5724 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5615\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0069 - sequential_2_loss: 0.0069 - sequential_3_loss: 1.2046e-05 - val_loss: 0.5755 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5646\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0068 - sequential_2_loss: 0.0068 - sequential_3_loss: 1.1231e-05 - val_loss: 0.5789 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5680\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0068 - sequential_2_loss: 0.0068 - sequential_3_loss: 1.2285e-05 - val_loss: 0.5822 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5714\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0068 - sequential_2_loss: 0.0067 - sequential_3_loss: 9.2982e-06 - val_loss: 0.5857 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5749\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0067 - sequential_2_loss: 0.0067 - sequential_3_loss: 7.0059e-06 - val_loss: 0.5886 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5777\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0067 - sequential_2_loss: 0.0067 - sequential_3_loss: 8.0532e-06 - val_loss: 0.5915 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5806\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0067 - sequential_2_loss: 0.0066 - sequential_3_loss: 7.6544e-06 - val_loss: 0.5939 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5831\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0066 - sequential_2_loss: 0.0066 - sequential_3_loss: 8.7364e-06 - val_loss: 0.5957 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5849\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0066 - sequential_2_loss: 0.0066 - sequential_3_loss: 5.3690e-06 - val_loss: 0.5998 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5889\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0066 - sequential_2_loss: 0.0066 - sequential_3_loss: 5.6121e-06 - val_loss: 0.6023 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5914\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 4.7386e-06 - val_loss: 0.6059 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5951\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 3.9149e-06 - val_loss: 0.6068 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5960\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 3.9243e-06 - val_loss: 0.6093 - val_sequential_2_loss: 0.0109 - val_sequential_3_loss: 0.5985\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 4.5792e-06 - val_loss: 0.6115 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6007\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0065 - sequential_2_loss: 0.0065 - sequential_3_loss: 3.6335e-06 - val_loss: 0.6129 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6021\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0064 - sequential_3_loss: 4.5179e-06 - val_loss: 0.6146 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6039\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0064 - sequential_3_loss: 2.8128e-06 - val_loss: 0.6168 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6060\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0064 - sequential_3_loss: 3.1204e-06 - val_loss: 0.6184 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6077\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0064 - sequential_3_loss: 2.0821e-06 - val_loss: 0.6203 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6095\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0063 - sequential_3_loss: 3.2380e-06 - val_loss: 0.6222 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6114\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0063 - sequential_2_loss: 0.0063 - sequential_3_loss: 2.2412e-06 - val_loss: 0.6236 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.6129\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0063 - sequential_2_loss: 0.0063 - sequential_3_loss: 2.6344e-06 - val_loss: 0.6243 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6136\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0063 - sequential_2_loss: 0.0063 - sequential_3_loss: 1.7711e-06 - val_loss: 0.6259 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6152\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0063 - sequential_2_loss: 0.0063 - sequential_3_loss: 1.6678e-06 - val_loss: 0.6285 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6178\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.3982e-06 - val_loss: 0.6293 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6186\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.3660e-06 - val_loss: 0.6307 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6200\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.6672e-06 - val_loss: 0.6327 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6220\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.3871e-06 - val_loss: 0.6355 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6248\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.5003e-06 - val_loss: 0.6357 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6250\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.5035e-06 - val_loss: 0.6370 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6263\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 1.7968e-06 - val_loss: 0.6402 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6295\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 1.1754e-06 - val_loss: 0.6421 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6314\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 1.1374e-06 - val_loss: 0.6423 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6316\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 1.1464e-06 - val_loss: 0.6456 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6348\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 9.5596e-07 - val_loss: 0.6470 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6364\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 1.2579e-06 - val_loss: 0.6483 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6376\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 1.1431e-06 - val_loss: 0.6508 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6402\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 1.2381e-06 - val_loss: 0.6517 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6409\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 6.9518e-07 - val_loss: 0.6535 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6428\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 7.0691e-07 - val_loss: 0.6553 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6446\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 9.3701e-07 - val_loss: 0.6581 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6475\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 6.6599e-07 - val_loss: 0.6591 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6485\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 9.0085e-07 - val_loss: 0.6607 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6500\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 6.8133e-07 - val_loss: 0.6606 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6500\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.1628e-07 - val_loss: 0.6626 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6520\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.9769e-07 - val_loss: 0.6638 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6532\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.9159e-07 - val_loss: 0.6665 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6559\n",
      "Epoch 352/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.7032e-07 - val_loss: 0.6671 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6565\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 5.5474e-07 - val_loss: 0.6679 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6574\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 9.5099e-07 - val_loss: 0.6718 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6612\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 2.4214e-06 - val_loss: 0.6767 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6661\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.8651e-07 - val_loss: 0.6759 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6652\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 4.3072e-07 - val_loss: 0.6794 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6689\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.8095e-07 - val_loss: 0.6808 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6702\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 4.6020e-07 - val_loss: 0.6816 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6711\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.9991e-07 - val_loss: 0.6828 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6722\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.2179e-07 - val_loss: 0.6836 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6731\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 1.2255e-06 - val_loss: 0.6892 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6787\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 3.5782e-07 - val_loss: 0.6887 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6782\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 4.2305e-07 - val_loss: 0.6914 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6809\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 1.1617e-06 - val_loss: 0.6870 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6765\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 4.7789e-07 - val_loss: 0.6913 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6808\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 3.3312e-07 - val_loss: 0.6916 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6811\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 4.8851e-07 - val_loss: 0.6953 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6847\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 3.1631e-07 - val_loss: 0.6952 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6847\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 3.4897e-07 - val_loss: 0.6944 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6838\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0112 - sequential_2_loss: 0.0060 - sequential_3_loss: 0.0052 - val_loss: 1.3274 - val_sequential_2_loss: 0.0167 - val_sequential_3_loss: 1.3106\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0197 - sequential_2_loss: 0.0076 - sequential_3_loss: 0.0121 - val_loss: 0.6067 - val_sequential_2_loss: 0.0122 - val_sequential_3_loss: 0.5945\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0075 - sequential_2_loss: 0.0068 - sequential_3_loss: 7.1446e-04 - val_loss: 0.5776 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5669\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0066 - sequential_2_loss: 0.0065 - sequential_3_loss: 1.3905e-04 - val_loss: 0.5738 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5631\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0064 - sequential_2_loss: 0.0063 - sequential_3_loss: 4.9595e-05 - val_loss: 0.5760 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.5653\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0062 - sequential_3_loss: 3.9308e-05 - val_loss: 0.5844 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.5737\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 2.0888e-05 - val_loss: 0.5914 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.5807\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0061 - sequential_3_loss: 1.8940e-05 - val_loss: 0.5972 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.5866\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0061 - sequential_2_loss: 0.0060 - sequential_3_loss: 3.0565e-05 - val_loss: 0.6074 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.5968\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0067 - sequential_2_loss: 0.0061 - sequential_3_loss: 6.1058e-04 - val_loss: 0.6071 - val_sequential_2_loss: 0.0108 - val_sequential_3_loss: 0.5963\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0062 - sequential_2_loss: 0.0061 - sequential_3_loss: 9.0929e-05 - val_loss: 0.6096 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.5990\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0060 - sequential_2_loss: 0.0060 - sequential_3_loss: 1.6141e-05 - val_loss: 0.6130 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6023\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 1.0256e-05 - val_loss: 0.6154 - val_sequential_2_loss: 0.0107 - val_sequential_3_loss: 0.6048\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0059 - sequential_2_loss: 0.0059 - sequential_3_loss: 8.4288e-06 - val_loss: 0.6169 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6062\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 6.5033e-06 - val_loss: 0.6198 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6093\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 6.2999e-06 - val_loss: 0.6245 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6140\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 4.4156e-06 - val_loss: 0.6260 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6154\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 3.8283e-06 - val_loss: 0.6279 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6173\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0058 - sequential_2_loss: 0.0058 - sequential_3_loss: 5.9792e-06 - val_loss: 0.6304 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6198\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 4.2464e-06 - val_loss: 0.6306 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6200\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 2.9966e-06 - val_loss: 0.6331 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6226\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 2.7570e-06 - val_loss: 0.6355 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6250\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 2.5839e-06 - val_loss: 0.6362 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6257\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 3.2101e-06 - val_loss: 0.6387 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6281\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0057 - sequential_2_loss: 0.0057 - sequential_3_loss: 2.3526e-06 - val_loss: 0.6415 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6310\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 2.4819e-06 - val_loss: 0.6435 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6330\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 2.3960e-06 - val_loss: 0.6453 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6347\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 1.9576e-06 - val_loss: 0.6472 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6366\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 1.6395e-06 - val_loss: 0.6483 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6377\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 2.1768e-06 - val_loss: 0.6511 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6406\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0056 - sequential_2_loss: 0.0056 - sequential_3_loss: 1.0379e-06 - val_loss: 0.6524 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6418\n",
      "Epoch 402/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 1.3387e-06 - val_loss: 0.6529 - val_sequential_2_loss: 0.0104 - val_sequential_3_loss: 0.6425\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 9.8789e-07 - val_loss: 0.6535 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6430\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 9.7763e-07 - val_loss: 0.6537 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6432\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 1.3820e-06 - val_loss: 0.6539 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6435\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0054 - sequential_2_loss: 0.0054 - sequential_3_loss: 1.3159e-06 - val_loss: 0.6547 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6442\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.0681e-06 - val_loss: 0.6552 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6447\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1855e-06 - val_loss: 0.6551 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6446\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1801e-06 - val_loss: 0.6550 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6446\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.0478e-06 - val_loss: 0.6556 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6451\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1652e-06 - val_loss: 0.6557 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6452\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1272e-06 - val_loss: 0.6568 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6463\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.1459e-07 - val_loss: 0.6570 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6465\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 9.5783e-07 - val_loss: 0.6574 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6469\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.0860e-06 - val_loss: 0.6574 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6469\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1319e-06 - val_loss: 0.6580 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6475\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1358e-06 - val_loss: 0.6588 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6483\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1419e-06 - val_loss: 0.6593 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6488\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.7675e-07 - val_loss: 0.6596 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6490\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1008e-06 - val_loss: 0.6589 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6484\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 7.8700e-07 - val_loss: 0.6591 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6486\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.3965e-06 - val_loss: 0.6612 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6507\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.6805e-06 - val_loss: 0.6610 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6505\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.1032e-06 - val_loss: 0.6612 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6507\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.6531e-07 - val_loss: 0.6616 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6511\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.7733e-06 - val_loss: 0.6650 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6545\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 6.6353e-07 - val_loss: 0.6673 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6568\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.8759e-06 - val_loss: 0.6664 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6559\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.5732e-07 - val_loss: 0.6663 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6558\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 1.2956e-06 - val_loss: 0.6660 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6555\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.4684e-07 - val_loss: 0.6671 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6566\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 8.4270e-07 - val_loss: 0.6683 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6577\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0053 - sequential_3_loss: 6.5600e-07 - val_loss: 0.6688 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6583\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 1.1289e-06 - val_loss: 0.6689 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6584\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0053 - sequential_2_loss: 0.0052 - sequential_3_loss: 6.8188e-07 - val_loss: 0.6694 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6589\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.6840e-07 - val_loss: 0.6690 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6585\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.3096e-07 - val_loss: 0.6699 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6594\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 6.3449e-07 - val_loss: 0.6707 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6602\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 6.5494e-07 - val_loss: 0.6698 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6593\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 8.0674e-07 - val_loss: 0.6723 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6618\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.2705e-07 - val_loss: 0.6722 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6617\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.0396e-07 - val_loss: 0.6730 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6625\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 7.2826e-07 - val_loss: 0.6741 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6636\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.8858e-07 - val_loss: 0.6757 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6652\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.7505e-07 - val_loss: 0.6751 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6646\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.2183e-07 - val_loss: 0.6772 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6667\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.7246e-07 - val_loss: 0.6770 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6664\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.4022e-07 - val_loss: 0.6781 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6675\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.8754e-07 - val_loss: 0.6786 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6681\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.5045e-07 - val_loss: 0.6777 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6672\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.4478e-07 - val_loss: 0.6794 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6689\n",
      "Epoch 452/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 7.0418e-07 - val_loss: 0.6786 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6681\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.9408e-07 - val_loss: 0.6797 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6692\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.3983e-07 - val_loss: 0.6823 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6718\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.1970e-07 - val_loss: 0.6823 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6718\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 5.3040e-07 - val_loss: 0.6819 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6714\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.9412e-07 - val_loss: 0.6846 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6741\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 4.6278e-07 - val_loss: 0.6854 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6748\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.8500e-07 - val_loss: 0.6852 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6747\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.7521e-07 - val_loss: 0.6860 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6755\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.4177e-07 - val_loss: 0.6880 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6775\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.9118e-07 - val_loss: 0.6890 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6785\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.3315e-07 - val_loss: 0.6889 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6783\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.7901e-07 - val_loss: 0.6914 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6809\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0052 - sequential_2_loss: 0.0052 - sequential_3_loss: 3.8199e-07 - val_loss: 0.6902 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6797\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.0565e-07 - val_loss: 0.6910 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6805\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.4714e-07 - val_loss: 0.6937 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6832\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.7777e-07 - val_loss: 0.6946 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6841\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.2309e-07 - val_loss: 0.6977 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6872\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.9684e-07 - val_loss: 0.6957 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6852\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.4985e-07 - val_loss: 0.6980 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6875\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5996e-07 - val_loss: 0.6988 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6883\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.9574e-07 - val_loss: 0.6987 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6881\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.5883e-07 - val_loss: 0.7003 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6898\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.0040e-07 - val_loss: 0.7016 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.6911\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 4.3966e-07 - val_loss: 0.7029 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6924\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.4118e-07 - val_loss: 0.7051 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6946\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.6710e-07 - val_loss: 0.7056 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6951\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5133e-07 - val_loss: 0.7040 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6935\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5640e-07 - val_loss: 0.7066 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6961\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5972e-07 - val_loss: 0.7088 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6983\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.2190e-07 - val_loss: 0.7101 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6996\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.4388e-07 - val_loss: 0.7114 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7008\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.2671e-07 - val_loss: 0.7097 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.6992\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.5828e-07 - val_loss: 0.7149 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7043\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.2736e-07 - val_loss: 0.7142 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7036\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 3.5816e-07 - val_loss: 0.7157 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7052\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.0553e-07 - val_loss: 0.7178 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7072\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.0634e-07 - val_loss: 0.7178 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7073\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.3393e-07 - val_loss: 0.7185 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7079\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.1669e-07 - val_loss: 0.7199 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7093\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.9895e-07 - val_loss: 0.7200 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7094\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.1824e-07 - val_loss: 0.7218 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7113\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 2.2741e-07 - val_loss: 0.7235 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7130\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.9093e-07 - val_loss: 0.7244 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7138\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.7709e-07 - val_loss: 0.7254 - val_sequential_2_loss: 0.0106 - val_sequential_3_loss: 0.7148\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.9887e-07 - val_loss: 0.7269 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7164\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0051 - sequential_2_loss: 0.0051 - sequential_3_loss: 1.9520e-07 - val_loss: 0.7281 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7176\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0050 - sequential_2_loss: 0.0050 - sequential_3_loss: 1.9565e-07 - val_loss: 0.7290 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7185\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0050 - sequential_2_loss: 0.0050 - sequential_3_loss: 1.9544e-07 - val_loss: 0.7308 - val_sequential_2_loss: 0.0105 - val_sequential_3_loss: 0.7202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e2b239e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x=x_train,\n",
    "                y=[x_train,y_train],\n",
    "                epochs=num_epochs, batch_size=batch_size, shuffle=True,\n",
    "                validation_data=[x_test, [x_test, y_test]],\n",
    "                callbacks=[tensorboard, LearningRateScheduler(lr_scheduler)],\n",
    "                verbose=1,\n",
    "                initial_epoch= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:09.293337Z",
     "start_time": "2019-01-13T05:03:05.582422Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yag.send('mizutaninikkou@gmail.com', subject = \"Training Done\", contents='Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:11.575505Z",
     "start_time": "2019-01-13T05:03:09.294487Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autoencoder.save(F'{model_path}autoencoder{autoencoder_id}.h5')\n",
    "encoder.save(F'{model_path}encoder{autoencoder_id}.h5')\n",
    "decoder.save(F'{model_path}decoder{autoencoder_id}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:11.579272Z",
     "start_time": "2019-01-13T05:03:11.576490Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:16.912647Z",
     "start_time": "2019-01-13T05:03:11.580321Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicoroble/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/keras/models.py:252: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "autoencoder = load_model(F'{model_path}autoencoder{autoencoder_id}.h5')\n",
    "encoder = load_model(F'{model_path}encoder{autoencoder_id}.h5')\n",
    "decoder = load_model(F'{model_path}decoder{autoencoder_id}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:17.049711Z",
     "start_time": "2019-01-13T05:03:16.913638Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code = encoder.predict(x_test[0][None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:17.186307Z",
     "start_time": "2019-01-13T05:03:17.050747Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reco = decoder.predict(code)[0].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:17.264132Z",
     "start_time": "2019-01-13T05:03:17.187457Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8d84c51668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e1030dc18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:17.342833Z",
     "start_time": "2019-01-13T05:03:17.265340Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8d838b9b70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8d837db710>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
